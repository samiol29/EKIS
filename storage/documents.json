[
  {
    "id": "2716e296-c928-48c1-bad9-5de30969e640",
    "text": "Perspective\nSearch still matters: information retrieval in the era of \ngenerative AI\nWilliam Hersh \n , MD\n�\nDepartment of Medical Informatics & Clinical Epidemiology, School of Medicine, Oregon Health & Science University, Portland, OR 97239, \nUnited States\n�\nCorresponding author: William Hersh, MD, Department of Medical Informatics & Clinical Epidemiology, School of Medicine, Oregon Health & Science \nUniversity, BICC, 3181 SW Sam Jackson Park Rd., Portland, OR 97239, United States ( hersh@ohsu.edu )\nAbstract \nObjective: Information retrieval (IR, also known as search) systems are ubiquitous in modern times. How does the emergence of generative \nartificial intelligence (AI), based on large language models (LLMs), fit into the IR process?\nProcess: This perspective explores the use of generative AI in the context of the motivations, considerations, and outcomes of the IR process \nwith a focus on the academic use of such systems.\nConclusions: There are many information needs, from simple to complex, that motivate use of IR. Users of such systems, particularly academ -\nics, have concerns for authoritativeness, timeliness, and contextualization of search. While LLMs may provide functionality that aids the IR proc -\ness, the continued need for search systems, and research into their improvement, remains essential.\nKey words: information storage and retrieval; generative artificial intelligence; large language models; ChatGPT . \nIntroduction\nInformation retrieval (IR, also known as search) systems are \nwidely used tools for information seeking in biomedicine and \nhealth and just about all other aspects of our lives. Search sys -\ntems such as Google or Bing for general Web searching and \nPubMed for the biomedical literature put the world’s archival \nknowledge at our fingertips for general and biomedical topics \n(even if paywalls do not always allow immediate access).\nIR systems had been relatively mature applications until \nlate 2022, when any staidness of search systems was upended \nby the emergence of generally-available generative artificial \nintelligence (AI) chatbots, based on large language models \n(LLMs), initially with ChatGPT and soon others to follow. \nShortly thereafter came generative AI capabilities added \nto the two major Web search engines, Microsoft Bing and \nGoogle. All of a sudden, searching the Web was transformed \nin ways that many did not see coming.\nI contemplate a good deal about search systems not only \nbecause they are the focus of my research\n1\nbut also because I \nam an academic who does a great deal of teaching and writ -\ning. One of my main teaching activities is a widely-subscribed \nintroductory course in biomedical and health informatics, in \nwhich I aim to impart not only the big picture of the field, \nbut also to provide a broad array of references to document \nthe knowledge that makes up that big picture. I teach another \ncourse on evidence-based medicine, which leads me to think \nnot only about how we find the best evidence to support bio -\nmedical and health decisions, but also how we use search sys -\ntems to find and synthesize that evidence.\nMy work activities may make me different from the aver -\nage user of search systems, but they do give me a broad per -\nspective on their use and now about the role(s) that \ngenerative AI may play. Decades ago, information scientists \nelucidated the different types of information needs that users \nbring to search systems. We may search over different types \nof content now, and have to deal with challenges that were \nnot thought about in those earlier times, such as misinforma -\ntion and filtering out true content versus advertising or misin -\nformation. But we still search for information to help answer \ndirect questions or make decisions, or to use information for \nmore integrative tasks, such as writing and teaching.\nInformation needs inform use of LLMs\nThe types of information needs that users bring to IR systems \nhave been studied for decades. Lancaster and Warner defined \nsubject needs,\n2\nwhich fall into three categories:\n•\nHelp in solving a certain problem or making a decision. \n•\nBackground information on a topic. \n•\nKeep up with information in a given subject area. \nThey called the first two subject needs retrospective infor -\nmation needs, in that documents already published are \nsought, while the latter need is called a current awareness \nneed, which is met by filtering new documents to identify \nthose on a certain topic. Retrospective needs may also be clas -\nsified by the amount of information needed:\nReceived: November 29, 2023; Revised: December 19, 2023; Editorial Decision: January 14, 2024; Accepted: January 15, 2024 \n# The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.  \nFor permissions, please email: journals.permissions@oup.com  \nJournal of the American Medical Informatics Association, 2024, 31(9), 2159–2161 \nhttps://doi.org/10.1093/jamia/ocae014 \nAdvance access publication 29 January 2024 \nPerspective \n\n•\nA single fact. \n•\nOne or more documents but less than the entire literature \non the topic. \n•\nA comprehensive search of the literature. \nWilkinson and Fuller described four types of information \nneeds for document collections:\n3\n•\nFact-finding—locating a specific item of information. \n•\nLearning—developing an understanding of a topic. \n•\nGathering—finding material relevant to a new problem \nnot explicitly stated. \n•\nExploring—browsing material with a partially specified infor -\nmation need that can be modified as the content is viewed. \nAnother type of information need is known-item searching, \nwhere we know what specific information item we are seek -\ning, and now want to find it. Many academics do quite a bit \nof this latter task, for example wanting to retrieval a scientific \npaper or preprint when we only have its title or less-than- \ncomplete metadata about it.\nAll of these varied information needs are at odds with the \noutput of generative AI chatbots that provide no or few refer -\nences. Even when references are provided, they often do not \nprovide a direct citation for what is said. Furthermore, the \nuser has no idea what actual source text led the generative \nmodel to output the specific text.\nAn additional aspect to searching that is critical to academics \nand many others when seeking information is the authoritative -\nness of information items retrieved. This is certainly important \nin scientific areas, especially in biomedicine and health. When \nsearching on biomedical and health-related topics, one of the \nfirst things I look at when retrieving something is who wrote it \nand who published it. This process is not just limited to my aca -\ndemic searching. Even when I am searching on other topics, for \nexample current events or consumer products, I want to know \nthe author(s) and source(s) of the information retrieved. I also \noften find myself trying to trace statements and assertions back \nto their source. Even for consumer-oriented information, but \ncertainly for academic searching, I want to know what research \nsupports, for example a given claim that some diagnostic test \nshould be used or some treatment prescribed. Likewise, if some -\none makes a claim about a research method or results, I often \nwant to find the original research to make sure the claim is sup -\nported by someone’s interpretation of it.\nA final aspect of search that is important to many is timeli -\nness, in that we want to retrieve the most up-to-date informa -\ntion. This is especially critical in rapidly evolving fields, for \nexample the use of generative AI in biomedical and health \napplications. The large resource requirements of training \nLLM models only allow them to be updated intermittently, \nand they may not reflect the latest information about a topic.\nChallenges for LLMs in search\nIn the early days of the World Wide Web, there was much \nconcern about the “quality” of information on the Web and \nreturned by Web search engines.\n4\nAs the Web democratized \npublishing, that is anyone with access to a Web server could \npost information, IR researchers thought about ways to aid \nsearchers in identifying the veracity of what they retrieved. A \ncolleague and I, with countless others, thought about meth -\nods to automate detection of factors related to the quality on \nhealth information.\n5\nThis problem was never really solved, \nalthough the emergence of Google helped in one important \nregard by ranking search output by links to given pages, \nwhich was a good if imperfect proxy for quality. Nonetheless, \nthe information quality war has probably been lost, especially \nwith emergence of social media as well as methods for manip -\nulating the retrieval of disinformation.\n6 , 7\nIt is from this perspective that we can contextualize the \nemergence of generative AI, not only as standalone chatbots \nsuch as ChatGPT, but also embedded in search systems such \nas Bing and Google. Clearly there are times when a simple \nanswer, coming out of a search or a chatbot, is sufficient. But \nit is when my information need goes beyond a simple ques -\ntion that I find LLMs still to be wanting. Whether it is search -\ning for information to make a decision about my personal \nhealth or to synthesize an area of science for my teaching or \nresearch, I need more than the general commentary of an \nLLM or its provision of a short list of references or Web sites.\nA number of other IR researchers have thought about these \nissues. Shah and Bender discussed “situating search,” noting \nthe varied information-seeking tasks that lead us to use \nsearch, and now generative AI, systems.\n8\nThese systems must \naccount for various aspects of searching, including diverse \ninformation-seeking needs and strategies, types of searchers, \nand bias in search results. Shah has also noted some of the \nchallenges for LLM systems in the context of IR systems:\n9\n•\nOpacity and hallucinations—LLMs “don’t know when \nthey don’t know.” \n•\nStealing content and Web site traffic—LLMs “learn from \nother people’s content and may divert traffic from their \nWeb sites.” \n•\nTaking away learning and serendipity—“search is explor -\ning and we may learn new unrelated things.” \nOne additional concern in modern times about chatbots \nreplacing search is the increased energy consumption of the \nformer, especially in an era of concern over climate change. \nAI systems not only use a great deal of electricity to power \nservers for training large models, but they also use more \nenergy during user interactions. One recent study estimated a \nGoogle search using its generative AI capabilities consumed \n10 times more energy than a plain Google search.\n10\nFuture role of LLMs in search\nThere has been some early research to look at information \nseeking and search system use in the LLM era, but there is \nnot yet any sort of comprehensive picture of the role of chat -\nbots vs search for different information needs. A few studies \nhave noted that ChatGPT does not provide comprehensive \nreferences and may even confabulate them in areas such as \nlearning health systems,\n11\ncryptococcal meningitis,\n12\nand a \nvariety of other interdisciplinary topics.\n13\nIt may be possible that LLM systems can augment the \nsearch process, but the evidence so far is slim. One study \nlooked at the potential for generating Boolean queries for sys -\ntematic review search and found improved precision but at a \ncost to recall, which is critically important in such searches.\n14\nAnother study found that ChatGPT could be help in answer -\ning consumer-health questions but correctness reduced when \nprompting included supporting evidence in the form of docu -\nments).\n15\nThere may also be potential value for methods such \nas retrieval-augmented generation, where search engine \n2160                                                                                                    Journal of the American Medical Informatics Association, 2024, Vol. 31, No. 9 \noutput is used to prompt existing LLMs for more focused \nand up-to-date information.\n16\nAdditional value may emanate \nfrom improved methods of prompting\n17\nor the addition of \nknowledge graphs to augment the prompting process.\n18\nAt \nthis time, however, there is little experimental evidence for \nthe value of these methods.\nThese are still early days for LLMs and generative AI in \nsearch, and there may yet be development of the critical fea -\ntures we desire for IR systems that are noted above. ChatGPT \nand the generative AI in Bing and Google are fascinating to \nexplore, and require the attention of everyone who may \nemploy future iterations of them in health-related settings. \nBut for critical information needs of academics like myself, \nwhether seeking information about personal aspects of life or \nscholarly work, I still scroll past the generative AI at the top \nof Bing and Google to the search functions while heading to \nsites like PubMed to seek scientific literature. Whether I \nretrieve a news article, a commentary, or a scientific paper, I \nstill look for who wrote the article and in which venue it was \npublished. I then delve into the text looking for the informa -\ntion I need to decide whether this item is useful for the task(s) \nthat led me to retrieve it.\nAs such, I still head first to search engines over generative \nAI chatbots for information seeking. As I prepare lectures, \npapers, and other intellectual syntheses, who wrote the paper, \nreport, news story, etc., and where it was published are as \nimportant as the content itself. ChatGPT and other chatbots \nproduce interesting information, but I find it less valuable for \nmy work than its original source. This may change in the \nfuture as LLM systems become more powerful and trained to \nsearching use cases, although we also know that the history \nof biomedical and health informatics is littered with applica -\ntions that had great hype and never achieved the revolution -\nary use that was anticipated.\nAuthor contributions\nThe author is the sole contributor to this article.\nFunding\nThis research received no specific grant from any funding \nagency in the public, commercial or not-for-profit sectors.\nConflicts of interest\nThe author has no competing interests to declare.\nData availability\nThis article is a Perspective piece and has no associated data.\nReferences\n1.0 Hersh W. Information Retrieval: A Biomedical and Health Per -\nspective. 4th ed. Springer International Publishing; 2020.\n2.0 Lancaster FW, Warner AJ. Information Retrieval Today. Rev., \nretitled, and expanded ed. Information Resources Press; 1993.\n3.0 Wilkinson R, Fuller M. Integration of information retrieval and \nhypertext via structure. In: Agosti M, Smeaton A, eds. Information \nRetrieval and Hypertext. Springer; 1996:257–272.\n4.0 Silberg WM, Lundberg GD, Musacchio RA. Assessing, control -\nling, and assuring the quality of medical information on the Inter -\nnet: Caveant lector et viewor—let the reader and viewer beware. \nJAMA. 1997;277(15):1244-1245.\n5.0 Price SL, Hersh WR. Filtering web pages for quality indicators: \nan empirical approach to finding high quality consumer health \ninformation on the world wide web. Proc AMIA Symp. \n1999:911-915.\n6.0 Boyd D, Golebiewski M. Data Voids. Data & Society. Data & \nSociety Research Institute. 2019. Accessed April 22, 2022. https:// \ndatasociety.net/library/data-voids/\n7.0 Center for Countering Digital Hate [Internet]. 2021 The Disinfor -\nmation Dozen. Accessed September 28, 2021. https://www.coun -\nterhate.com/disinformationdozen\n8.0 Shah C, Bender EM. Situating search. In: Proceedings of the 2022 \nConference on Human Information Interaction and Retrieval \n[Internet]. New York, NY, USA: Association for Computing \nMachinery; 2022:221-232.\n9.0 Shah C. AI information retrieval: A search engine researcher \nexplains the promise and peril of letting ChatGPT and its cousins \nsearch the web for you. The Conversation. 2023. Accessed August \n22, 2023. http://theconversation.com/ai-information-retrieval-a- \nsearch-engine-researcher-explains-the-promise-and-peril-of-let -\nting-chatgpt-and-its-cousins-search-the-web-for-you-200875\n10. de Vries A. The growing energy footprint of artificial intelligence. \nJoule. 2023;7(10):2191-2194.\n11. Chen A, Chen DO. Accuracy of Chatbots in citing journal articles. \nJAMA Netw Open. 2023;6(8):e2327647.\n12. Schwartz IS, Link KE, Daneshjou R, Cort �es-Penfield N. Black box \nwarning: large language models and the future of infectious dis -\neases consultation. Clin Infect Dis off Publ Infect Dis Soc Am. \n2023. https://doi.org/10.1093/cid/ciad633\n13. Walters WH, Wilder EI. Fabrication and errors in the biblio -\ngraphic citations generated by ChatGPT. Sci Rep. 2023;13 \n(1):14045.\n14. Wang S, Scells H, Koopman B, Zuccon G. 2023. Can ChatGPT \nwrite a good boolean query for systematic review literature search? \narXiv; http://arxiv.org/abs/2302.03495, preprint: not peer \nreviewed.\n15. Zuccon G, Koopman B. 2023. Dr ChatGPT, tell me what I want to \nhear: how prompt knowledge impacts health answer correctness. \narXiv. http://arxiv.org/abs/2302.13793, preprint: not peer \nreviewed.\n16. Lewis P, Perez E, Piktus A, et al. Retrieval-augmented generation \nfor knowledge-intensive NLP tasks. In: Proceedings of the 34th \nInternational Conference on Neural Information Processing Sys -\ntems. Red Hook, NY, USA: Curran Associates Inc.; \n2020:9459-974.\n17. Nori H, Lee YT, Zhang S, et al. 2023. Can generalist foundation \nmodels outcompete special-purpose tuning? Case study in medi -\ncine. arXiv. http://arxiv.org/abs/2311.16452, preprint: not peer \nreviewed.\n18. Pan S, Luo L, Wang Y, Chen C, Wang J, Wu X. 2023. Unifying \nlarge language models and knowledge graphs: a roadmap. arXiv. \nhttp://arxiv.org/abs/2306.08302, preprint: not peer reviewed.\n# The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.  \nFor permissions, please email: journals.permissions@oup.com\nJournal of the American Medical Informatics Association, 2024, 31, 2159–2161\nhttps://doi.org/10.1093/jamia/ocae014\nPerspective\nJournal of the American Medical Informatics Association, 2024, Vol. 31, No. 9                                                                                                    2161",
    "metadata": {
      "filename": "google_inforetri_genai.pdf",
      "source": "uploads"
    }
  },
  {
    "id": "ee63a4a7-d50f-4b71-9e20-8061450d6e0c",
    "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni ∈ Rdmodel×dk , WK\ni ∈ Rdmodel×dk , WV\ni ∈ Rdmodel×dv\nand WO ∈ Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\n5\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ϵls\ntrain PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013.\n12\nAttention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15",
    "metadata": {
      "filename": "google_transformer_research_papear.pdf",
      "source": "uploads"
    }
  },
  {
    "id": "602b3b1f-51f2-4581-a725-9eafbcdd9e09",
    "text": "Data residency, \noperational \ntransparency,  \nand privacy for \nEuropean customers \non Google Cloud\nGoogle Cloud Whitepaper \nFebruary 2020\nThe information contained herein is intended to outline general product direction and should not be relied upon in making \npurchasing decisions nor shall it be used to trade in the securities of Alphabet Inc. The content is for informational purposes only \nand may not be incorporated into any contract. The information presented is not a commitment, promise, or legal obligation to \ndeliver any material, code or functionality. Any references to the development, release, and timing of any features or functionality \ndescribed for these services remains at Google’s sole discretion. Product capabilities, timeframes and features are subject to \nchange and should not be viewed as Google commitments.\n\n2\nIntroduction\nAt Google Cloud, the privacy and security of customer data underpins the design of all of the services \nthat we offer. We recognize that our global customers have specific concerns based on their regional and \nindustry-specific requirements. While this whitepaper is directed towards our European customers, many \nof the topics addressed are not unique to Europe. This whitepaper describes options customers have for \nconfiguring services to meet their privacy and security requirements when using Google Cloud. Cloud \nusers around the world have similar needs, and Google Cloud’s Trust Principles1 apply to all of our \ncustomers in every region. The content in this whitepaper is intended to be for informational purposes \nonly, and is not legal advice. If you require it, you should seek independent legal advice relating to your \nstatus and obligations as a Google Cloud customer in Europe. \nFor our customers in Europe, common concerns may include compliance with the General Data \nProtection Regulation (GDPR), as well as sector-specific regulatory compliance requirements, such as \nthe European Banking Authority (EBA) Guidelines. Customers may also have questions involving where \ntheir data is stored (data residency), how access to that data is controlled, and how we handle \ngovernment requests and the CLOUD Act. We understand that data residency, operational transparency, \nand privacy on Google Cloud are top of mind for our European customers, and we are committed to \noffering the tools to meet these critical needs and preferences.\n3\nData storage & data access\nGoogle Cloud provides you with the ability to control where your data \nis stored. In Europe, our compute and storage key services2 allow you \nto store customer data in regions3 in the UK, Belgium, Germany, \nFinland, Switzerland, and the Netherlands, with a new planned region \nin Poland, and several others forthcoming.4 \nWhen you choose to configure resources in these locations for our \ncompute and storage key services, Google will store that customer \ndata at rest only in the selected region, in accordance with our Service \nSpecific Terms5 and Terms of Service. \nTo assist our customers in enforcing these controls, Google Cloud \noffers Organization Policy6 constraints, which can be applied at the \norganization, folder, or project level. You can limit the physical \nlocation of a new resource with the Organization Policy Service \nresource locations constraint.7 When coupled with Cloud IAM \nconfiguration,8 which helps you to define fine-grained access policies \nand precisely control access to Google Cloud hosted data, you can \nprevent your employees from accidentally storing customer data in \nthe wrong Google Cloud region. \nGoogle Cloud also provides you with the ability to control the network \nlocations from which users can access data by using VPC Service \nControls.9 This product allows you to limit access to users by IP \naddress filtering, and Cloud Armor10 allows restricting your external \nload balancer ingress to a specific region. You can even use this \nconstraint if the user is authorized according to your Cloud IAM11 \npolicy. Using VPC Service Controls, you create a service perimeter12 \nwhich defines the virtual boundaries from which a service can be \naccessed, preventing customer data from being moved outside of \nthose boundaries. It also helps mitigate risks such as the \nmisconfiguration of employee access controls or attackers taking \nadvantage of compromised accounts. Identity-Aware Proxy (IAP)13 \nenables customers to control access to cloud applications and VMs \nbased on the user’s identity and the context of their request. We also \nallow the creation of a perimeter that permits limited external \naccess14 if desired.\n\n4\nEncryption key management\nFor many operations, you may want to transfer your data between GCP’s regions. Google Cloud enables \nencryption in transit15 by default to encrypt inter-region traffic that is outside the perimeter of Google’s \nfacilities. Whenever data is stored, Google Cloud applies encryption at rest16 by default. To gain more \ncontrol over how data is encrypted at rest, Google Cloud customers can use our Cloud Key Management \nService (Cloud KMS)17 to generate, use, rotate, and destroy encryption keys according to the customers’ \nown policies, a control we refer to as customer-managed encryption keys (CMEK). If you are using Cloud \nKMS,18 your cryptographic keys must be stored in the region where you deploy the resource. You can also \nchoose to store your keys in the region you choose with our Cloud Hardware Security Module (HSM)19 \nservice, which allows customers to host encryption keys and perform cryptographic operations in FIPS \n140-2 Level 3 certified HSMs.\nCustomers can also implement Customer Supplied Encryption Keys (CSEK)20 for supported services so \nthat GCP encrypts data with customer-supplied keys and purges the supplied keys from memory after \nthe customer requested operation is complete. \nWe also offer External Key Manager (Cloud EKM),21 which allows you to store and manage keys in a \nthird-party key management product deployed outside of Google’s infrastructure. Using a third-party \nproduct allows you to place a KMS key ring in a geographic location of your choice or in one of the \nregions22 recommended by your external key manager. You can use Cloud EKM in any specific (i.e. non-\nglobal) Google Cloud region supported for Cloud KMS.\n\n5\nCloud administrator access\nOn GCP, you can configure Cloud IAM permissions23 to limit access by your own \nadministrators, curating the right amount of access at the project, folder or \ndataset level. This includes an extensive list of permissions and the predefined \nroles24 that grant them. You can also create your own custom roles25 that contain \nexactly the permissions you specify. \nWe also allow you to control access by Google personnel. Access Approval26 \nallows you to require explicit approval before any personnel accesses your data \nor configurations on GCP, unless those accesses are necessary to resolve a \ncurrent service disruption, security incident, or legal requirement.* This \nfunctionality is available to Platinum or Enterprise (Role-based) support \ncustomers on GCP. Access Approval works by sending customers an email and/\nor Cloud Pub/Sub message with an access request that the customer is able to \napprove. Using the information in the message, customers can use the GCP \nConsole or the Access Approval API to approve the access. \nAccess Approval for GCP complements the visibility provided by Access \nTransparency,27 which generates near real-time logs when Google administrators \ninteract with your data, including the office location of the administrator and the \nreason for the access. Coming soon, you’ll be able to enforce specific attributes \nfor administrators who are allowed to access your data or configurations—\nincluding the geographic region from which they are operating and other \ncompliance-relevant attributes. \nKey Access Justifications,28 an upcoming feature that works with Cloud KMS and \nExternal Key Manager, provides a detailed justification each time one of your keys \nis requested to decrypt data, along with a mechanism for you to approve or deny \nthe key access, using an automated policy that you set. This product provides \nvisibility into every request for an encryption key that permits data to change \nstate from at-rest to in-use, with a justification for that request. It includes a \ncommitment from GCP to protect the integrity of our controls and the \njustifications. Using Key Access Justifications with External Key Manager (initially \nwith BigQuery and Google Compute Engine/Persistent Disk), you can deny \nGoogle the ability to decrypt your data for any reason.\n* The comprehensive list of Access Approval exclusions can be found at  \nhttps://cloud.google.com/access-approval/docs/overview\n6\nGovernment requests for customer data\nOne particular situation that is of interest to our customers and partners in Europe relates to \nrequests for data from government agencies and, more recently, the impact of the U.S. CLOUD \nAct29 and the U.S./U.K. Agreement30 on the privacy and security of our customers’ data. \nThe CLOUD Act and the recently announced U.S./U.K. Agreement31 do not change how Google \nhandles government requests to disclose enterprise customer data.32 Our team reviews and \nevaluates each and every one of the requests we receive for legal validity and appropriate scope, \nas well as for compliance with international human rights standards, our own policies, and \napplicable law. \nGenerally speaking, if customer data is sought during the course of a legitimate legal \ninvestigation, Google informs the government that it should request customer data directly from \nthe organization in question. This approach is in line with the U.S. Department of Justice’s policy33 \nthat prosecutors should go to customers directly. It is also in line with EU policy proposals.34\nHowever, if Google does receive a direct government data request regarding a customer account, \nwe have a team of lawyers and trained personnel dedicated to reviewing requests. Each data \nrequest is reviewed using the following guidelines; note that we follow the same process for \nCLOUD Act data requests.\n\n7\n➊ Respect for the privacy and security of the data \ncustomers store with Google. Each request is reviewed \nto make sure it satisfies international human rights \nstandards, our own policies, and the law. If we believe a \nrequest is overly broad, we’ll seek to narrow it. Google \nhas opposed indefinite non-disclosure orders 35 and has \nfought for the right to notify customers of government \nrequests for data. We do not provide “backdoor” direct \naccess to any government and we do not hesitate to \nprotect customer interests.\n➋ Customer notification. At a minimum, governments \nshould provide direct notification to customers 36 when \nthey seek to compel cloud service providers to disclose \ndata. Except in emergency situations involving a threat \nto life, it is our policy to notify the customer before \nany information is disclosed unless such notification \nis prohibited by law. We will provide delayed notice to \nusers after a legal prohibition is lifted, such as when a \nstatutory or court ordered disclosure prohibition period \nhas expired. This notification typically goes to the \ncustomer’s point of contact.\n➌ Consideration of customer objections. Google will, \nto the extent allowed by law and by the terms of the \nrequest, comply with a customer’s reasonable requests \nregarding its efforts to oppose a request, such as the \ncustomer filing an objection to the disclosure with the \nrelevant court and providing a copy of the objection \nto Google.\nFor U.S. government data requests, if Google notifies the \ncustomer of the request and the customer subsequently \nfiles an objection to disclosure with the court and provides \na copy of the objection to Google, Google will not provide \nthe data in response to the request if the objection is \nresolved in favor of the customer.\n\n8\nCompliance controls and support\nOur customers and regulators expect independent verification of security, privacy, \nand compliance controls. Google Cloud undergoes several independent third-\nparty audits on a regular basis to provide this assurance. Some of the key \ninternational and European standards we are audited against are:\n• ISO/IEC 27001 (Information \nSecurity Management)37\n• ISO/IEC 27017 (Cloud Security)38\n• ISO/IEC 27018 (Cloud Privacy)39\n• SOC 240 and SOC 341 reports\n• C5 (German Federal Office for \nInformation Security (BSI))42 \nGoogle also participates in sector and country-specific frameworks. For example, \nfor companies working in and with the French healthcare sector, it is important \nthat Google Cloud is HDS-certified43 Additionally, Google provides offerings such \nas the ISAE 3000 Type 2 Report,44 which verifies the effectiveness of Google’s \ninternal controls to support adherence to certain FINMA (the Swiss Financial \nMarket Supervisory Authority) requirements applicable to regulated financial \nservices customers. Where formal certifications or attestations may not be \nrequired or applied, we also provide resource documents and mappings to \nframeworks and laws, such as the EBA Outsourcing Guidelines45 and the GDPR.46 \nA complete list of our compliance offerings is available via our Compliance \nresource center.47 \nWe also understand that regulations such as GDPR place significant emphasis on \nenterprises knowing how their data is being processed, who has access to data, \nand how security incidents will be managed. It’s important to note that GDPR \ncompliance is a shared responsibility. Google Cloud generally acts as a data \nprocessor of customer data, and as a data processor we process that data only \nas instructed by you—our customers. In turn, you own your data, and Google \nCloud is committed to providing you with tools and resources that put you in \ncontrol of your data. Our data processing terms for G Suite48 and Google Cloud \nPlatform49 are designed to directly address GDPR requirements. These \ncontractual commitments clearly articulate our privacy commitments to \ncustomers, and are fundamental to GDPR compliance for both Google and our \nCloud customers. We provide GDPR-related documentation, white papers, videos, \nand other useful information for customers on our GDPR Resource Center,50 as \nwell as our GDPR overview page.51\n\n9\nConclusion\nAt Google Cloud, we work hard to earn and maintain your trust by giving you a \nclear and detailed understanding of our process and approach to security. The \ncapabilities outlined in this whitepaper create a solution that gives you control \nover the location of your data and access to that data. With these considerations \naddressed, our customers in Europe and around the globe can confidently build \nmission critical workloads on Google Cloud. Even so, we’re not done yet: we \ncontinue to invest in data privacy and security innovations to anticipate the future \nneeds of our customers so that they can move to Google Cloud today knowing \nthat they are fortified for the future.\nTo learn more about our capabilities, you can read our Trust whitepapers for \nGCP52 and G Suite,53 and visit our Trust & Security site.54\n\n10\nAppendix\nPage 2:\n1 Google Cloud Trust Principles: https://cloud.google.com/security/privacy\nPage 3:\n2 Google Cloud Platform Key Services: https://cloud.google.com/terms/key-services\n3,4 Google Cloud locations: https://cloud.google.com/about/locations\n5 Google Cloud Service Specific Terms: https://cloud.google.com/terms/service-terms\n6,7 Restricing Resource Locations: https://cloud.google.com/resource-manager/docs/organization-\npolicy/defining-locations\n8 Cloud IAM configuration: https://cloud.google.com/service-usage/docs/reference/rest/v1/ \nservices/enable\n9 Overview of VPC Service Controls: https://cloud.google.com/vpc-service-controls/docs/overview\n10 Cloud Armor: https://cloud.google.com/armor\n11 Cloud Identity and Access Management: https://cloud.google.com/iam\n12 VPC Service Controls: Creating a service perimeter: https://cloud.google.com/vpc-service-controls/\ndocs/create-service-perimeters\n13 Google Cloud Identity-Aware Proxy: https://cloud.google.com/iap\n14 VPC Service Controls: Enabling controlled access when creating a perimeter: https://cloud.google.\ncom/vpc-service-controls/docs/create-service-perimeters#external-access\nPage 4:\n15 Encryption in Transit in Google Cloud: https://cloud.google.com/security/encryption-in-transit\n16 Encryption at rest: https://cloud.google.com/security/encryption-at-rest\n17,18 Google Cloud Key Management Service: https://cloud.google.com/kms\n19 Google Cloud Hardware Security Module: https://cloud.google.com/hsm\n20 Customer-Supplied Encryption Keys: https://cloud.google.com/security/encryption-at-rest/\ncustomer-supplied-encryption-keys\n21 Google Cloud External Key Manager: https://cloud.google.com/ekm\n22 Google Cloud locations: https://cloud.google.com/about/locations\n11\nPage 5:\n23 Google Cloud IAM permissions reference: https://cloud.google.com/iam/docs/\npermissions-reference\n24 Cloud IAM: Understanding roles: https://cloud.google.com/iam/docs/understanding-roles\n25 Cloud IAM: Creating and managing custom roles: https://cloud.google.com/iam/docs/\ncreating-custom-roles\n26 Access Approval documentation: https://cloud.google.com/access-approval/docs\n27 Access Transparency: https://cloud.google.com/access-transparency\n28 Key Access Justifications: https://cloud.google.com/blog/products/identity-security/\ncontrol-access-to-gcp-data-with-key-access-justifications\nPage 6:\n29 U.S. Cloud Act: https://www.justice.gov/dag/page/file/1152896/download\n30,31 U.S./U.K. Agreement: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/\nattachment_data/file/836969/CS_USA_6.2019_Agreement_between_the_United_Kingdom_and_\nthe_USA_on_Access_to_Electronic_Data_for_the_Purpose_of_Countering_Serious_Crime.pdf\n32 Goverment requests for customer data: controlling access to your data in Google Cloud whitepaper: \nhttps://services.google.com/fh/files/blogs/government_access_technical_whitepaper.pdf\n33 Seeking enterprise customer data held by cloud service providers: https://www.justice.gov/criminal-\nccips/file/1017511/download\n34 EU policy proposal: https://eur-lex.europa.eu/resource.html?uri=cellar:639c80c9-4322-11e8-a9f4-\n01aa75ed71a1.0001.02/DOC_1&format=PDF\nPage 7:\n35 Advancing customer control in the cloud: https://cloud.google.com/blog/topics/inside-google-cloud/\nadvancing-customer-control-in-the-cloud\n36 Written testimony of Richard Salgado: https://www.judiciary.senate.gov/imo/media/doc/09-16-15%20\nSalgado%20Testimony.pdf\n12\nPage 8:\n37 Google Cloud Compliance Resource Center: ISO/IEC 27001: https://cloud.google.com/security/\ncompliance/iso-27001\n38 Google Cloud Compliance Resource Center: ISO/IEC 27017: https://cloud.google.com/security/\ncompliance/iso-27017\n39 Google Cloud Compliance Resource Center: ISO/IEC 27018: https://cloud.google.com/security/\ncompliance/iso-27018\n40 Google Cloud Compliance Resource Center: SOC 2: https://cloud.google.com/security/compliance/\nsoc-2\n41 Google Cloud Compliance Resource Center: SOC 3: https://cloud.google.com/security/compliance/\nsoc-3\n42 Google Cloud Compliance Resource Center: Cloud Computing Compliance Controls Catalogue (C5): \nhttps://cloud.google.com/security/compliance/bsi-c5\n43 Google Cloud Compliance Resource Center: HDS: https://cloud.google.com/security/compliance/hds\n44 Google Cloud Compliance Resource Center: ISAE 3000 Type 2 Report: https://cloud.google.com/\nsecurity/compliance/isae-3000-type-2\n45 Google Cloud Compliance Resource Center: EBA Outsourcing Guidelines: https://cloud.google.com/\nsecurity/compliance/eba-outsourcing-guidelines\n46,51 Google Cloud & the General Data Protection Regulation (GDPR): https://cloud.google.com/ \nsecurity/gdpr\n47 Google Cloud Compliance Resource Center: https://cloud.google.com/security/compliance\n48 Data Processing Amendment to G Suite and/or Complementary Product Agreement: https://gsuite.\ngoogle.com/terms/dpa_terms.html\n49 Google Cloud Data Processing and Security Terms: https://cloud.google.com/terms/\ndata-processing-terms\n50 GDPR Resource Center: https://cloud.google.com/security/gdpr/resource-center\nPage 9:\n52 Trusting your data with Google Cloud Platform whitepaper: https://cloud.google.com/files/ \ngcp-trust-whitepaper.pdf\n53 Trusting your data with G Suite whitepaper: https://cloud.google.com/files/ \ngsuite-trust-whitepaper.pdf\n54 Google Cloud Trust & security: https://cloud.google.com/security/",
    "metadata": {
      "filename": "googlecloud_european_commitments_whitepaper.pdf",
      "source": "uploads"
    }
  },
  {
    "id": "10596c01-2445-44ab-af4c-936ea4a189e8",
    "text": "National Strategy for Artificial Intelligence \n \n2 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis page has been intentionally left blank \n \n \n \n \n \n \n \n  \n \nNational Strategy for Artificial Intelligence \n \n3 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAcknowledgments \n \nIn writing this Report, Arnab Kumar , Punit Shukla, Aalekh Sharan and Tanay Mahindru of NITI Aayog \nhave made valuable contributions. \nInputs were also provided by Dr. Avik Sarkar, Dr. Ashish Nayan and Kartikeya Asthana of NITI Aayog. \nThe valuable contributions of Mr. P. Anan dan and his team from Wadhwani Institute for AI, Dr.  Manish \nGupta of Videoken, Prof. Ramesh Raskar of MIT Media Labs, nVIDIA, Intel, IBM, NASSCOM, McKinsey \nand Accenture are also thankfully acknowledged. Special contribution of Accenture in the production of \nthis report is also acknowledged. \n \n \nAnna Roy \nAdvisor (Industry) \nNITI Aayog \n  \n\n \nNational Strategy for Artificial Intelligence \n \n4 \n \n Contents  \nINTRODUCTION 5 \nEXECUTIVE SUMMARY 7 \nWHAT IS ARTIFICIAL INTELLIGENCE? 12 \nGLOBAL DEVELOPMENTS IN ARTIFICIAL INTELLIGENCE 16 \nARTIFICIAL INTELLIGENCE AND INDIA 18 \nFOCUS AREAS FOR AI INTERVENTION 24 \nHealthcare 24 \nAgriculture 30 \nEducation 35 \nSmart Cities and Infrastructure 39 \nSmart Mobility and Transportation 41 \nKEY CHALLENGES TO ADOPTION OF AI IN INDIA 46 \nWAY FORWARD TO HARNESS THE POWER OF AI 48 \nRESEARCH 50 \nSKILLING FOR THE AI AGE 64 \nACCELERATING ADOPTION 71 \nETHICS, PRIVACY, SECURITY AND ARTIFICIAL INTELLIGENCE 85 \nACTIONS FOR THE GOVERNMENT 91 \nAPPENDIX I: ARTIFICIAL INTELLIGENCE EXPLAINED 96 \nAPPENDIX II: GLOBAL COUNTRY STRATEGY REVIEW 100 \nAPPENDIX III: DATA ECOSYSTEM 110 \n  \n \nNational Strategy for Artificial Intelligence \n \n5 \n \nIntroduction  \n #AIforAll: Technology Leadership for Inclusive Growth  \n \nArtificial Intelligence (AI) is poised to disrupt our world. With intelligent machines enabling high -level \ncognitive processes like thinking, perceiving, learning, problem solving and decision making, coupled \nwith advances in data collection and aggregation, analytics and computer processing power, AI presents \nopportunities to complement and supplement human intelligence and enrich the way people live and \nwork. \nIndia, being the fastest growing economy with the second largest population in the world, has a significant \nstake in the AI revolution. Recognising AI’s potential to transform economies and the need for India to \nstrategise its approach, Hon’ble Finance Minister, in his budget speech  for 2018 – 2019, mandated NITI \nAayog to establish the National Program on AI , with a view to guiding the research and development in \nnew and emerging technologies. In pursuance of the above, NITI Aayog has ad opted a three -pronged \napproach – undertaking exploratory proof -of-concept AI projects in various areas, crafting a national \nstrategy for buil ding a vibrant AI ecosystem in India and collaborating with various experts and \nstakeholders. Since the start of this year, NITI Aayog has partnered with several leading AI technology \nplayers to implement AI projects in critical areas such as agriculture a nd health. Learnings from these \nprojects, under various stages of implementation, as well as our engagement with some of the leading \ninstitutions and experts have given a better perspective to our task of crafting the national strategy for AI, \nwhich is the focus of this discussion paper. \nThis strategy document is premised on the proposition that India, given its strengths and characteristics, \nhas the potential to position itself among leaders on the global AI map – with a unique brand of #AIforAll. \nThe approach in this paper focuses on how India can leverage the transformative technologies to ensure \nsocial and inclusive growth in line with the development philosophy of the government. In addition, India \nshould strive to replicate these solutions in other similarly placed developing countries.  \n#AIforAll will aim at enhancing and empowering human capabilities to address the challenges of access, \naffordability, shortage and inconsistency of skilled expertise; effective implementation of AI initiatives to \nevolve scalable solutions for emerging economies; and endeavors to tackle some of the global challenges \nfrom AI’s perspective, be it application, research, development,  technology, or responsible AI. #AIforAll \nwill focus on harnessing collaborations and partners hips, and aspires to ensure prosperity for all. Thus, \n#AIforAll means technology leadership in AI for achieving the greater good.  \nWhile evolving the national strategy for AI, the underlying thrust was to identify applications with maximum \nsocial impact, a willingness to learn from the best of the world when it comes to the recen t technology \nadvancements in AI, and leveraging approaches that democratize access to and further development of \nAI. \nFrom an applications perspective, the approach is to identify sec tors that may have the potential of \ngreatest externalities while adopting AI solutions, and hence require the government to play a leading \nrole in developing the implementation roadmap for AI. For example, the agriculture sector in India, which \nforms the bedrock of India’s economy, needs multi-layered technology infusion and coordination amongst \nseveral stakeholders. Efforts from private sector may neither be financially optimal nor efficient on a \nstandalone basis, and hence sustained government interventio n to tackle the existing challenges  and \nconstraints is needed. Hence, India’s approach to implementation of AI has to be guided by optimisation \nof social goods, rather than maximisation of topline growth. \n \nNational Strategy for Artificial Intelligence \n \n6 \n \nFrom a technology perspective, the strategy is to maximise the late-movers’ advantage. Acknowledging \nthat India is some distance away from consistently delivering home grown pioneering technology \nsolutions in AI, adapting and innovating the technology for India’s unique needs and opportunities would \nhelp it in leap frogging, while simultaneously building the foundational R&D capability aimed at ensuring \ncompetitiveness in the long run. \nSolving for India, given the complexity and multi -dimensional aspects of most of our economic and \nsocietal challenges, can easily be extended to the rest of the emerging and developing economies. An \nintegral part of India’s strategy for AI involves tackling common and complex global challenges that can \nbe solved through technology intervention, and India’s scale and opportunity landscape provides the ideal \ntest-bed to ensure sustainable and scalable solutions. \nThe purpose of this paper is to lay the ground work for evolv ing the National Strategy for Artificial \nIntelligence. While this paper includes several recommendations, so me of which may be deemed \ndisruptive, specifics (e.g. execution and financial implications) have been consciously avoided, since \nwider consultations and consensus building is needed to refine these recommendations. This document \nis intended to serve as an “essential pre-read” in building a truly transformative approach in pursuit of \n#AIforAll. \n \n \nAmitabh Kant \nCEO, NITI Aayog \n  \n\n \nNational Strategy for Artificial Intelligence \n \n7 \n \nExecutive Summary \n India’s Approach to L eadership in AI  \n \nAI refers to the ability of machines to perform cognitive tasks like thinking, perceiving, learning, problem \nsolving and decision making. Initially conceived as a technology that could mimic human intelligence, AI \nhas evolved in ways that far exceed its orig inal conception. With incredible advances made in data \ncollection, processing and computation power, intelligent systems can now be deployed to take over a \nvariety of tasks, enable connectivity and enhance productivity. As AI’s capabilities have dramatical ly \nexpanded, so have its utility in a growing number of fields. \nThe truly transformative nature of the technology, yet the nascent stage of its adoption worldwide, \nprovides India with an opportunity to define its own brand of AI leadership. #AIforAll - the brand proposed \nfor India implies inclusive technology leadership, where the full potential of AI is realised in pursuance of \nthe country’s unique needs and aspirations. The strategy should strive to leverage AI for economic \ngrowth, social development and inclusive growth, and finally as a “Garage” for emerging and developing \neconomies. \nWhile AI has the potential to provide large incremental value to a wide range of sectors, adoption till date \nhas been driven primarily from a commercial perspective. Technology disruptions like AI are once -in-a-\ngeneration phenomenon, and hence large-scale adoption strategies, especially national strategies, need \nto strike a balance between narrow definitions of financial impact and the greater good. NITI Aayog has \ndecided to focus on five sectors that are envisioned to benefit the most from AI in solving societal needs: \na) Healthcare: increased access and affordability of quality healthcare, \nb) Agriculture: enhanced farmers’ income, increased farm productivity and reduction of wastage, \nc) Education: improved access and quality of education, \nd) Smart Cities and Infrastructure: efficient and connectivity for the burgeoning urban population,  \nand \ne) Smart Mobility and Transportation: smarter and safer modes of transportation and better traffic \nand congestion problems. \nTo truly reap the benefits of deploying AI at scale, the report identifies the following barriers that need to \nbe addressed in order to achieve the goals of #AIforAll:  \na) Lack of broad based expertise in research and application of AI, \nb) Absence of enabling data ecosystems – access to intelligent data, \nc) High resource cost and low awareness for adoption of AI, \nd) Privacy and security, including a lack of formal regulations around anonymisation of data, and \ne) Absence of collaborative approach to adoption and application of AI. \nSuperior research capabilities have been the cornerstone of leadership aspirations in emerging \ntechnologies and effectively realis ing the growth potential requires expertise in both core and applied \nresearch. Despite indications of recent positive efforts in this aspect of technology, AI research in India \nis still in its infancy and requires large scale concerted and collaborative interventions .  \nThe paper proposes a two-tiered structure to address India’s AI research aspirations: \na) Centre of Research Excellence (CORE) focused on developing better understanding of existing \ncore research and pushing technology frontiers through creation of new knowledge;  \nb) International Centers of Transformational AI (ICTAI) with a mandate of developing and deploying \napplication-based research. Private sector collaboration is envisioned to be a key aspect of \nICTAIs. \n \nNational Strategy for Artificial Intelligence \n \n8 \n \nThe research capabilities are proposed to be complemented by an umbrella organisation responsible for \nproviding direction to research efforts through analysis of socio -economic indicators, studying global \nadvancements, and encouraging international collaboration. Pursuing “ moonshot research projects ” \nthrough speciali sed teams, development of a dedicated supranation al agency to channel research in \nsolving big, audacious problems of AI – “CERN for AI”, and developing common computing and other \nrelated infrastructure for AI are other key components research suggested.  \nAs technology increasingly disrupts the nature of jobs and shifts the benchmarks of technological \naptitude, skilling and reskilling of workforce forms an integral part of our approach to adopting AI. There \nis an emergent need for reskilling the existing workforce and developing future talent in accordance  with \nthe changing needs of the job market. This could be done via the adoption of decentrali sed teaching \nmechanisms working in collaboration with the private sector and educational institutions to prescribe \ncertification with value. Furthermore, promotion  of job creation in new areas, like data annotation needs \nto be identified and promoted, as these would have the potential of absorbing a large portion of the \nworkforce that may find itself redundant due to increasing automation.  \nAdoption of AI across the  value chain viz. startups, private sector, PSUs and government entities, will \ntruly unlock the potential by creating a virtuous cycle of supply and demand. The barriers to AI \ndevelopment and deployment can effectively be addressed by adopting the marketplace model – one \nthat enables market discovery of not only the price but also of different approaches that are best suited \nto achieve the desired results. A three -pronged, formal marketplace could be created focusing on data \ncollection and aggregation, data annotation and deployable models. There could be a common platform \ncalled the National AI Marketplace (NAIM). \nFurthermore, for accelerated adoption of a highly collaborative technology like AI, the government has to \nplay the critical role of a catalyst in  supporting partnerships, providing access to infrastructure, fostering \ninnovation through research and creating the demand by seeking solutions for addressing various \ngovernmental needs. \nAs AI -based solutions permeate the way we live and do business, questions on ethics , privacy and \nsecurity will also emerge . Most discussions on ethical considerations of AI are a derivation of the FAT \nframework (Fairness, Accountability and Transparency). A consortium of Ethics Councils at each Centre \nof Research Excellence can be set up and it would be expected that all COREs adhere to standard \npractice while developing AI technology and products. \nData is one of the primary drivers of AI solutions, and thus appropriate handling of data, ensuring privacy \nand security is of prime importance. Challenges include data usage without consent, risk of identification \nof individuals through data, data selection bias and the resulting discrimination of AI models, and \nasymmetry in data aggregation. The paper suggests establishing d ata protection frameworks and \nsectorial regulatory frameworks, and promotion of adoption of international standards.  \nIn order for India to ride the AI innovation wave, a robust intellectual property framework is required. \nDespite a number of government initiatives in strengthening the IP regime, challenges remain, especially \nin respect of applying stringent and narrowly focused patent laws to AI applications – given the unique \nnature of AI solution development. The importance of data to development of usef ul models is one such \nexample. To tackle these issues, establishment of IP facilitation centers to help bridge the gap between \npractitioners and AI developers, and adequate training of IP granting authorities, judiciary and tribunals \nis suggested. \nThe AI strategy is aimed at primarily guiding an inevitable wave of change for quicker and better impact. \nThe AI ecosystem is rapidly evolving and taking societies into uncharted territory. For now, we can begin \nto ask some of the big questions that each society m ust answer for itself: are we ready to manage data \nethically? How do we bridge the digital divide? Which innovations are worthy of public funds and \n \nNational Strategy for Artificial Intelligence \n \n9 \n \npartnerships? Bringing these questions into the open is the most important step in ensuring that AI \nadvances create a better society. \nThere has been tremendous activity concerning AI policy in different countries over the past couple of \nyears. Governments in USA, UK, France, Japan and China have released their policy and strategy papers \nrelating to AI. In order to establish a leadership role, it is important for India to take the plunge and start \nby releasing a Strategy Paper to initiate the roll out of an ambitious programme that would ensure for \nIndia its rightful place in this transformational era.  \n \nNational Strategy for Artificial Intelligence \n \n10 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis page has been intentionally left blank \n  \n \nNational Strategy for Artificial Intelligence \n \n12 \n \n  \n What is Artificial Intelligence? \n A technical primer  \n \nAI might just be the single largest technology revolution of our live times, with the potential to disrupt \nalmost all aspects of human existence. Andrew Ng, Co -founder of Coursera and formerly head of Baidu \nAI Group / Google Brain, compares the transformational impact of AI to that of electricity 100 years back. \nWith many industries aggressively investing in cognitive and AI solutions, global investments are forecast \nto achieve a compound annual growth rate (CAGR) of 50.1% to reach USD57.6 billion in 20211. \nAI is not a new phenomenon, with much of its theoretical and technological underpinning developed over \nthe past 70 years by computer scientists such as Alan Turing, Marvin Minsky and John McCarthy. AI has \nalready existed to some degree in many industries and governments. Now, thanks to virt ually unlimited \ncomputing power and the decreasing costs of data storage, we are on the cusp of the exponential age of \nAI as organisations learn to unlock the value trapped in vast volumes of data. \nAI is a constellation of technologies that enable machines  to act with higher levels of intelligence and \nemulate the human capabilities of sense, comprehend and act. Thus, comput er vision and audio \nprocessing can actively perceive the world around them by acquiring and processing images, sound and \nspeech. The natural language processing and inference engines can enable AI systems to analy se and \nunderstand the information collected. An AI system can also take action through technologies such as \nexpert systems and inference engines or  undertake actions in the physic al world . These human \ncapabilities are augmented by the ability to learn from experience and keep adapting over time. AI \nsystems are finding ever -wider application to supplement these capabilities across enterprises as they \ngrow in sophistication. \nIrrespective of the type of AI being used, however, every application begins with large amounts of training \ndata. In the past, this kind of performance was driven by rules-based data analytics programs, statistical \nregressions, and early “expert systems.” But the explosion of powerful de ep neural networks now gives \nAI something a mere program doesn’t have: the ability to do the unexpected.  \n  \n                                                      \n1 Worldwide Semi-annual Cognitive Artificial Intelligence Systems Spending Guide from International Data Corp. (IDC), 2017 \n \nNational Strategy for Artificial Intelligence \n \n13 \n \nFigure 1: What is Artificial Intelligence \n \n \nAI technology has experienced a checkered history of waves of optimism followed by disappointment and \nperiods of inertia, dubbed as “ AI winters”. Each previous breakthrough has only ever partly lived up to \nthe hype it generated, and none has managed to kick-start the technology into the mainstream. \nFigure 2: Evolution of AI \n \n \nThe big change today is that we are in an unprecedented period of technology innovation across so many \ndifferent fields tha t gives us the belief that the “ AI Spring” has not only arrived but is here to stay. Key \ndevelopments responsible for this optimism are: \nSource: \nAccenture \nSource: \nAccenture \n \nNational Strategy for Artificial Intelligence \n \n14 \n \na) Unlimited access to computing power : The worldwide public cloud services market is projected \nto grow 21.4% in 2018 to total USD186.4 billion, up from USD153.5 bi llion in 2017, according to \nGartner, Inc. The access is amplified by rapid increase in computational power.  \nb) Huge fall in cost of storing data: We are in an age where the hard drive cost per gigabyte of data \nhas been falling exponentially, to the extent that we are approaching near zero marginal cost for \nstoring data (down from USD500,000 a gigabyte in 1980 to 2 cents a gigabyte in 2017). \nc) Explosion in data that is digitised: As per IDC forecasts, by 2025, the global data sphere will grow \nto 163 zettabytes (that is a trillion gigabytes)2, or ten times the 16.1ZB of data generated in 2016. \nAs Barry Smyth, Professor of Computer science at University College Dublin, says: \"Data is to \nAI what food is to humans. \" So, in a more digital world, the exponential growth of data is \nconstantly feeding AI improvements. \nConsider, for example, the vastly increased processing power that comes from using Graphics \nProcessing Units (GPUs) in place of Central Processing Units (C PUs). Google, in May 2017 , \nannounced that its Tensor Processing Unit (TPU) delivered 30-80 times higher performance-per-watt \nthan contemporary CPUs and GPUs. When you add the decreasing cost of storage to the mix, plus \nthe exponential growth in data volume s, together with the emergence of open source platforms and \nframeworks, you have got a uniquely potent combination of technologies and capabilities. It all adds \nup to a very powerful foundation to give AI its critical mass for mainstream adoption . \nBox 1: Machine Learning and Deep Learning \nMachine Learning, a term coined by Artur Samuel in 1959, meant “the ability to learn without being \nexplicitly programmed.” Machine Learning involves the use of algorithms to parse data and learn from it, \nand making a determination or prediction as a result. Instead of hand coding software libraries with well-\ndefined specific instructions for a particular task, the machine gets “trained” using large amounts of data \nand algorithms, and in turn gains the capability to perform specific tasks. \n“Deep Learning is a technique for implementing Machine L earning. Deep Learning was inspired by the \nstructure and function of the brain, specifically the interconnecting of many neurons. Artificial Neural \nNetworks (ANNs) are algorithms that are based on the biological structure of  the brain. In ANNs, there \nare ‘neurons’ which have discrete layers and connections to other “neurons”. Each layer picks out a \nspecific feature to learn. It’s this layering that gives deep learning its name, dep th is created by using \nmultiple layers as opposed to a single layer3.” \n \nFigure 3: AI, ML and Deep Learning \n \nSource: nVIDIA \n                                                      \n2 Data Age 2025: “The Evolution of Data to Life-Critical whitepaper by International Data Corporation, 2017” \n3 Medium.com: “The Difference Between Artificial Intelligence, Machine Learning, and Deep Learning” \n\n \nNational Strategy for Artificial Intelligence \n \n15 \n \nFor a detailed analysis of major types of Machine Learning, algorithms and use cases, please refer to \nAppendix I: Artificial Intelligence Explained. \nAI gets categori sed in different ways and it may be useful to understand the various categories, their \nrationale and the implications. \na) Weak AI vs. Strong AI: Weak AI describes \"simulated\" thinking. That is, a system which appears \nto behave intelligently, but doesn't have any kind of consciousness about what it's doing. For \nexample, a chatbot might appear to hold a natural conversation, but it has no sense of who it is \nor why it's talking to you. Strong AI describes \"actual\" thinking.  That is, behaving intelligently, \nthinking as human does, with a conscious, subjective mind. For example, when two humans \nconverse, they most likely know exactly who they are, what they're doing, and why.  \nb) Narrow AI vs. General AI : Narrow AI describes an AI  that is limited to a single task or a set \nnumber of tasks. For example, the capabilities of IBM's Deep Blue, the chess playing computer \nthat beat world champion Gary Kasparov in 1997, were limited to playing chess. It wouldn't have \nbeen able to win a game  of tic-tac-toe - or even know how to play. General AI describes an AI \nwhich can be used to complete a wide range of tasks in a wide range of environments. As such, \nit's much closer to human intelligence. \nc) Superintelligence: The term \"superintelligence\" is often used to refer to general and strong AI at \nthe point at which it surpasses human intelligence, if it ever does. \nWhile big strides have been made in Artificial Narrow Intelligence – algorithms that can process \ndocuments, drive vehicles or beat champion  chess players, no one has yet claimed the first production \nor development of General AI. The weight of expert opinion is that we are a long way off the emergence \nof General AI. \nFigure 4: Narrow AI vs. General AI \n  \nSource: \nAccenture \n \nNational Strategy for Artificial Intelligence \n \n16 \n \n \n Global Developments in Artificial Intelligence \nBenchmarking s elect countries  \n \nCountries around the world are becoming increasingly aware of the potential economic and social \nbenefits of developing and applying AI. For example, China and U.K. estimate that 26% and 10% of their \nGDPs respectively in 2030 will be sourced from AI -related activities and businesses. There has been \ntremendous activity concerning AI policy positions and the development of an AI ecosystem in different \ncountries over the last 18 to 24 month s – the US published its AI report in December 2016; France \npublished the AI strategy in January 2017 followed by a detail ed policy document in March 2018; Japan \nreleased a document in March 2017; China published the AI strategy in July 2017; and U.K. rele ased its \nindustrial strategy in November 2017.  \nInfrastructural supply side interventions have been planned by various countries for creating a larger \necosystem of AI development. Creation of “data trusts”, rolling out of digital connectivity infrastructur e \nsuch as 5G / full fiber networks, common supercomputing facilities, fiscal incentives and creation of open \nsource software libraries are some of the focus areas of various governments as committed in their \nstrategy papers.  \nIn the area of core research in AI and related technologies, universities and research institutions from the \nUS, China and Japan have led the publication volume on AI research topics between 2010 and 2016. \nUniversities in USA, primarily Carnegie Mellon University, Massachusetts Institu te of Technology and \nStanford, took an early lead in AI research by offering new courses, establishing research facilities and \ninstituting industry partnerships. Off late, Chinese universities, especi ally Peking and Tsinghua \nUniversities have caught on to the race by utilising large scale public funding and extensive research \npartnerships with private companies.  \nFor building the future workforce for AI, countries are also significantly increasing the allocation of \nresources for Science, Technology, Enginee ring and M aths (STEM) talent development through \ninvestment in universities, mandating new courses (e.g., AI and law), and offering schemes to retrain \npeople. For instance, U.K. has planned to build over 1,000 government supported PhD researchers by \n2025 and set up a Turing fellowship to support an initial cohort of AI fellows while China has launched a \nfive-year university program to train at least 500 teachers and 5,000 students working on AI technologies.  \nGovernance structures for enabling all the above mandates vary across countries. Many countries have \ninstituted dedicated public offices such as Ministry of AI (UAE), and Office of AI and AI Council (U.K.) \nwhile China and Japan have allowed existing minist ries to take up AI implementation in their sectoral \nareas. Not just national governments, but even local city governments have become increasingly aware \nabout the importance and potential of AI and have committed public investments.  \nNational governments h ave significantly increased public funding for AI through commitments such as \nincreasing the R&D spend, setting up industrial and investment funds in AI startups, investing in network \nand infrastructure and AI -related public procurements. China, USA, Franc e and Japan have committed \nsignificant public spending for AI technology development and adoption.  \nThese countries are also leveraging different combinations of public -private-academia to develop and \npromote AI. Development of technology parks, and connec ting large corporations with start ups and \n \nNational Strategy for Artificial Intelligence \n \n17 \n \nforming “national teams” with large private players to undertake fundamental and applied research are \nsome of the public-private partnership approaches various national governments have espoused.  \nAI technology development and applications are evolving rapidly with major implications for economies \nand societies. A study by EY and NASCCOM found  that by 2022, around 46%  of the workforce will be \nengaged in entirely new jobs that do not exist today, or will be deployed in jobs that have radically \nchanged skillsets4. If some countries decide to wait for a few years to establish an AI strategy and put in \nplace the foundations for developing the AI ecosystem, it seems unlikely that they would be able to attain \nand match up to the current momentum in the rapidly changing socio-economic environment. Therefore, \nthe need of the hour is to develop a policy framework that will help set up a vibrant AI ecosystem in India.  \nA detailed study of various country strategies for AI is placed in the Appendix II: Global Country Strategy \nReview.   \n                                                      \n4 Future of Jobs in India: A 2022 Perspective, 2017 \n \nNational Strategy for Artificial Intelligence \n \n18 \n \n  \n Artificial Intelligence and India \nIdentifying priority areas for India’s efforts in Artificial Intelligence  \n \nA national AI strategy needs to be premised on a framework which is adapted to India’ s unique needs \nand aspirations, while at the same time, is capable of achieving the country’s full potential of leveraging \nAI developments. Such a framework could be seen as an aggregation of the following three distinct, yet \ninter-related components: \na) Opportunity: the economic impact of AI for India \nb) AI for Greater Good: social development and inclusive growth  \nc) AI Garage for 40% of the world : solution provider of choice for the emerging and developing \neconomies (ex-China) across the globe  \nOpportunity: the economic impact of Artificial Intelligence for India \nAI is emerging as a new factor of production, augmenting the traditional factors of production viz. labor, \ncapital and innovation and technological changes captured \nin total factor productivity. AI has the potential to overcome \nthe physical limitations of capital and labour, and open up \nnew sources of value and growth. From an economic impact \nperspective, AI has the potential to drive growth through \nenabling: (a) intelligent automation i.e. ability to au tomate \ncomplex physical world tasks that require adaptability and \nagility across industries, (b) labo ur and capital \naugmentation: enabling humans to focus on parts of their \nrole that add the most value, complementing human \ncapabilities and improving capita l efficiency, and (c) \ninnovation diffusion i.e. propelling innovations as it diffuses through the economy. AI innovations in one \nsector will have positive consequences in another, as industry sectors are interdependent based on value \nchain. Economic value is expected to be created from the new goods, services and innovations that AI \nwill enable. \nAccenture, in its recent AI research reports 5, provides a framework for evaluating the economic impact \nof AI for select G20 countries and estimates AI to boost Indi a’s annual growth rate by 1.3 percentage \npoints by 2035.  \n \n  \n                                                      \n5 Rewire for Growth: Accelerating India’s Economic Growth with Artificial Intelligence, Accenture \n\n \nNational Strategy for Artificial Intelligence \n \n19 \n \n \nFigure 5: Unlocking innovation through AI \n \nAI for Greater Good: social development and inclusive growth  \nBeyond just the headline numbers of economic impact, a disruptive technology such as AI needs to be \nseen from the perspective of the transformative impact it could have on the greater good – improving the \nquality of life and access of choice to a large section of the country. In that sense, the recent \nadvancements in AI seem to be custom -made for the unique opportunities and challenges that India \nfaces. Increased access to quality health facilities (including addressing the locational access barriers), \ninclusive financial growth for large sections of population that have hitherto been excluded  from formal \nfinancial products, providing real-time advisory to farmers and help address unforeseen factors towards \nincreasing productivity, building smart and efficient cities and infrastructure to meet the demands of \nrapidly urbanising population are some of the examples that can be most effectively solved through the \nnon-incremental advantages that a technology such as AI can provide. \nAI Garage for 40% of the world \nIn addition to providing unique opportuni ties, India provides a per fect “playground” for enterprises and \ninstitutions globally to develop scalable solutions which can be easily implemented in the rest of the \ndeveloping and emerging economies. Simply put, Solve for India  means solve for 40% or more of the \nworld. An advance d AI based solution for early diagnosis of tuberculosis (one of the top -10 causes of \ndeaths worldwide), for example, could easily be rolled out to countries in South East Asia or Africa, once \ndeveloped and refined in India. Beyond healthcare, AI technologi es in the other sectors including \nagriculture, education and mobility are set to transform the world. The commonality of issues with regard \nto the above sectors across developing countries provides the ideal use case of developing AI solutions \nthat could be adapted for multiple markets. Hence, AI technologies suited for the Indian agricultural sector \ncould easily be customised for other developing nations based on their local climatic conditions. \nEducation continues to be a major concern in almost all devel oping countries. AI technologies that are \ncapable of imparting quality education to India’s linguistically diverse population could prove very useful \nin other developing nations.  \nAnother aspect of India’s potential as a leader in AI is it proven track record in technology solution provider \nof choice. Solved in India (or more accurately, solved by Indian IT companies) could be the model going \nforward for Artificial Intelligence as a Service (AIaaS). Indian IT companies have been pioneers in bringing \ntechnology products and developments as solutions across the globe. As AI matures and generalised \napplications become common place, its advantage India when it comes to large scale implementation. \nFurthermore, India’s competence in IT combined with opportunities,  such as interoperability between \nSource: \nAccenture \n \nNational Strategy for Artificial Intelligence \n \n20 \n \nmultiple languages, provides the much needed impetus for finding scalable solutions for problems that \nhave global implications, such as NLP.  \nArtificial Intelligence has the potential to provide large incremental value to a wide range of sectors \nglobally, and is expected to be the key source of competitive advantage for firms.   \na) Healthcare: Application of AI in healthcare can help address issues of high barriers to access to \nhealthcare facilities, particularly in rural areas that suffer from poor connectivity and limited supply \nof healthcare professionals. This can be achieved through implementation of use cases such as \nAI driven diagnostics, personali sed treatment, early identification of potential pandemics, and \nimaging diagnostics, among others. \nb) Agriculture: AI holds the promise of driving a food revolution and meeting the increased demand \nfor food (global need to produce 50% more food and cater to an additional 2 billion people by 2050 \nas compared to today). It also has the potential to address challenges such as inadequate demand \nprediction, lack of assured irrigation, and overuse / misuse of pesticides and fertilisers. Some use \ncases include improvement in crop yield through real time advisory, advanced detection of pest \nattacks, and prediction of crop prices to inform sowing practices. \nc) Smart Mobility, including Transports and Logistics : Potential use cases in this domain include \nautonomous fleets for ride sharing, semi-autonomous features such as driver assist, and predictive \nengine monitoring and maintenance. Other areas that AI can impact include autonomous trucking \nand delivery, and improved traffic management.  \nd) Retail: The retail sector has been one of the early adopters of AI solutions, with applications such \nas improving user experience by providing personalised suggestions, preference-based browsing \nand image -based product se arch. Other use cases include customer demand anticipation, \nimproved inventory management, and efficient delivery management. \ne) Manufacturing: Man ufacturing industry is expected to be one of the biggest beneficiaries of AI \nbased solutions, thus enabling 'Factory of the Future' through flexible and adaptable technical \nsystems to automate processes and machinery to respond to unfamiliar or unexpected situations \nby making smart decisions. Impact areas include engineering (AI for R&D efforts), supply chain \nmanagement (demand forecasting), production (AI can achieve cost reduction and increase \nefficiency), maintenance (predictive mainte nance and increased  asset utilis ation), quality \nassurance (e.g. vision systems with machine learning algorithms to identify defects and deviations \nin product features), and in-plant logistics and warehousing. \nf) Energy: Potential use cases in the energy sector include energy system modelling and forecasting \nto decrease unpredictability and increase efficiency in power balancing and usage. In renewable \nenergy systems, AI can enable storage of energy through intelligent grids enabled by smart meters, \nand also improve the reliability and affordability of photovoltaic energy. Similar to the manufacturing \nsector, AI may also be deployed for predictive maintenance of grid infrastructure.  \ng) Smart Cities: Integration of AI in newly developed smart cities and infrastructure could also help \nmeet the demands of a rapidly urbanising population and providing them with enhanced quality of \nlife. Potential use cases include traffic control to reduce congestion and enhanced security through \nimproved  crowd management. \nh) Education and Skilling: AI can potentially solve for quality and access issues observed in the Indian \neducation sector. Potential use cases include augmenting and enhancing the learning experience \nthrough personalised learning, automating and expediting administrative tasks, and predicting the \nneed for student intervention to reduce dropouts or recommend vocational training . \n \nNational Strategy for Artificial Intelligence \n \n21 \n \nAdoption of AI by various sectors have been influenced by, among other factors, technical and regulatory \nchallenges, but commercial implications has been the biggest determinant. While technic al feasibility, \navailability of structured data, regulatory barriers, privacy considerations, ethic al issues, preference for \nhuman relationship have all played their roles in determining the readiness  of a sector for large scale AI \nadoption; compelling business use cases (e.g. improved efficiency, a ccuracy, speed, forecasting and \naccurate decision making) that lead to direct impact on revenue and profitability  have been the biggest \ndriver for companies to pursue accelerated adoption of AI. As illustrated in McKinsey Global Institute’s AI \nadoption and use survey, sectors leading the AI adoption today also intend to grow their investment in AI \nthe most, thus further reinforcing the varying degrees of AI adoption across sectors. \nFigure 6: Current AI adoption and future AI investments by sector \n \nIt comes as no surprise that Banking and Financial Services sector has been one of the leading sectors \nglobally when it comes to AI adoption, and India has also seen a steep increase in AI based \nimplementation in recent times. Existing and potential use of Artificial Intelligence in this sector include \nimproved customer interaction through personali sed engagement, virtual customer a ssistance, and \nchatbots; improved processes through deployment of intelligent automation in rule based back -office \noperations; development of credit scores through analysis of bank history or social media data; and fraud \nanalytics for proactive monitoring and prevention of various instances of fraud, money laundering, \nmalpractice, and the prediction of potential risks. AI in this sector has also been employed  in wealth \nmanagement viz. robo-advisory, algorithmic trading and automated transactions. \nSimilarly, manufacturing sector, primarily automotive and assembly, has been one of the first sectors to \nimplement advanced robotics at scale. The manufacturing sector in India hasn’t been far behind, as \nreflected in a recent study by BCG, where India was  ranked 3 rd in the world in AI implementation in \nSource: \nMcKinsey \nGlobal \nInstitute AI \nadoption and \nuse survey \n \nNational Strategy for Artificial Intelligence \n \n22 \n \nmanufacturing, ahead of nations such as Germany, with 19% of companies in the sector already using \nAI to a significant extent6. \nThese trends have also been reflected in the nature of investment in research in India, with private sector \ninitiatives such as the Robert Bosch Centre for Data Science and Artificial Intelligence (RBC -DSAI), \nchoosing to focus their efforts in applied research on sectors such as manufacturing analytics and \nfinancial analytics. \nFigure 6 also reveals that sectors like Healthcare and Education have quite a lot of ground to cover as \nfar as AI adoption is concerned. Healthcare, despite being one of the hottest areas of AI startup \ninvestments ( Appendix IV : What Do the Markets Say ), is tricky, especial ly in the Indian context. \nAgriculture doesn’t even feature in the analysis above. Another analysis by McKinsey Global Institute \nindicates that potential value of AI for agriculture was in the bottom tercile of 19 sectors evaluated 7, and \ncould be a possible  explanation for diminished private sector led AI adoption in agriculture. In sectors \nsuch as these, externalities from adoption of AI far outweigh the economic returns reali sed by private \ninitiatives, and hence the role of government becomes pivotal in ensuring large scale AI intervention.  \nNITI Aayog has evaluated various sectors that will be impacted by AI and has taken a conscious decision \nto focus on a select set of sectors where only private sector led initiatives will not lead to achieving desired \nsocietal outcomes. In addition to Healthcare and Agriculture, focus sectors include Education (preparing \ntomorrow’s generation to leverage the global AI revolution to India’s advantage), Smart Cities and \nInfrastructure (solving for India’s rapidly urbani sing population) and Smart Mobility and Transportation \n(solving for challenges congestion, pollution, high rates of road accidents leading to economic inefficiency \nand enormous human cost). \nAn unrelated but interesting paradigm for AI application is the “AI + X” approach. Despite its vast potential, \nthe capabilities of AI today are limited to tasks for which it has been specifically trained, and are still many \nyears from achieving human like consciousness. AI today should thus be regarded as an enhancement, \nor enabler of increased efficiency in previously existing processes, rather than capable of a complete \noverhaul of traditional tasks. Deployment can be viewed through the paradigm of “ take an existing \nprocess, and add AI”  or “AI + X”; where “X”  can range from tasks such as driving a car, where AI can \nprovide incremental value through improved routing and energy management, to act of sowing seeds, \nwhere AI can help inform decision making and improve productivity.  \nSimilar to the effects of electr icity, AI can increasingly be seen as an intelligent, additive utility that can \nbe deployed at will, but remain largely invisible to the tasks performer. This vision is perhaps best put by \nauthor Kevin Kelley:  \n“There is almost nothing we can think of that cannot be made new, different, or more valuable by infusing \nit with some extra IQ. In fact, the business plans for the next 10,000 startups are easy to forecast: Take \nX and add AI” . \nIn applying this paradigm to the development o f a national strategy, it is thus important to consider the \nchallenges faced by individual sectors , or various manifestations of “X”  to best identify the incremental \nvalue that AI can provide. The paradigm also cements the need for collaboration with secto rial \nstakeholders in the application of the technology. The paradigm provides a useful framework to analyse \nwhat is possible in terms of technology intervention today.\n                                                      \n6 BCG “AI in the Factory of the Future” \n7 McKinsey Global Institute “Notes From The AI Frontier: Insights From Hundreds Of Use Cases” \n  \n \nNational Strategy for Artificial Intelligence \n \n24 \n \n  \nFocus areas for AI intervention \n Sectoral deep dives  \n \nHealthcare  \nHealthcare is one of the most dynamic, yet challenging, sectors in India, and is expected to grow to \nUSD280 billion by 2020, at a CAGR of upwards of 16%, from the current ~USD100 billion 8.  \nYet, it faces major challenges of quality, accessibility and affordability for a large section of the population: \na) Shortage of qualified healthcare professionals and services like qualified doctors, nurses, \ntechnicians and infrastructure : as evidenced in 0.76 doctors and 2.09 nurses per 1,000 \npopulation (as compared to WH O recommendations of 1 doctor and 2.5 nurses per 1,000 \npopulation respectively) and 1.3 hospital beds per 1,000 population as compared to WHO \nrecommended 3.5 hospital beds per 1,000 population9. \nb) Non-uniform accessibility to healthcare across the country with physical access continuing to be \nthe major barrier to both preventive and curative health services, and glaring disparity between \nrural and urban India.  \n \nFigure 7: Accessibility of Healthcare across India \n \n                                                      \n8 FICCI-KPMG study \n9 WHO website, PwC analysis \nSource: \nPwC \nAnalysis, \nWorld Bank \ndata (2017) \n \nNational Strategy for Artificial Intelligence \n \n25 \n \nWith most of the private facilities concentrated in and around tier 1 and tier 2 cities, patients have to travel \nsubstantial distances for basic and advanced healthcare services. (Box 2: What TMH’s Cancer heat map \ntells us about the availability of healthcare in India?)  \nThe problem is further accentuated by lack of consistent quality in healthcare across India, most of the \nservices provided is individual driven rather than institution driven, and less than 2% of hospitals in India \nare accredited. \nBox 2: What TMH’s Cancer heat map tells us about the availability of \nhealthcare in India? \nTata Memorial Hospital, one of the leading cancer hospitals in India, registered more than 67,000 new \nregistrations for cancer treatment in 2015. While the hospital is loca ted in Mumbai, less than 23% of the \nnew patients were geographically based in Maharashtra, with a whopping 21.7% of patients traveling \nfrom the states of UP, Bihar, Jharkhand and West Bengal to TMH.  \nFigure 8: Geographic location of TMH cancer patients\n \nThat these patients had to travel more than 1,800 km , on an average,  to avail cancer treatment is an \nunfortunate tale of lack of access to quality healthcare. In addition to battling a potentially life threatening \ndisease, the patients are saddled by the stress and financial implications of traveling long way away from \nhome. While the data is not available to such an effect, it wouldn’t be surprising to find that most of these \npatients choose to travel to TMH when cancer has developed to an advanced stage, thus further reducing \nthe chances of successful cure and treatment. \nCredit: Tata Memorial Centre \n \n\n \nNational Strategy for Artificial Intelligence \n \n26 \n \nc) Affordability remains a problem with private expenditure accounting for ~70% of healthcare \nexpenses, of which ~62% is out-of-pocket expenditure, probably one of the highest in the world. \nSignificant portion of hospital costs in both rural (~47%) and urban India (~31%) are financed by \nloans and sale of assets.  Poor and marginalised are hit the most, and as per the Government \nestimates, a sizeable part of the population (~63 million) are faced with poverty every year \nbecause of their healthcare expenditure10. \nd) Reactive approach to essential healthcare  largely due to lack of awareness, access to services \nand behavioral factors implies that majority of patients approach a hospital / physician only when \na disease has reached an advanced stage, thus increasing the cost of care and reducing the \nchances of recovery.  \nThe Government of India  has been making a series of large scale interventions to address India’s \nhealthcare challenges, viz. transformation of 1.5 lakh Health and Wellness Centers, developing district \nhospitals to cater to long -term care for non -communicable diseases, Ayushman Bharat Mission, \npromoting e-Health etc. \nBox 3: Government of India’s push for Universal Healthcare Coverage \nThe Government of India , through its recent policy interventions, has shown a bold commitment to \nachieve Universal Health Coverage and increased access to comprehensive primary health care. \nThrough the Ayushman Bharat programme announced in Union Budget 2018, probably the world’s \nlargest government funded health care programme, the Government of India  has embarked on a path \nbreaking journey to ensure the affordability and accessibility of healthcare in India. The Ayushman Bharat \n– National Health Protection Mission (AB – NHPM) aims t o provide insurance cover of INR  5 lakh per \nfamily per year for secondary and tertiary care hospitalisation. Ayushman Bharat is targeted at more than \n10 crore families (approximately 50 crore beneficiaries / ~40% of India’s population) belonging to the poor \nand vulnerable sections based on the SECC database, and doesn’t impose any limitations on family size \nor age limit for the beneficiaries to avail benefits. The benefits package covers most medical and surgical \nconditions with minimal exclusions , covers pre and post hospitalis ation expenses, and covers all pre -\nexisting conditions from day one – thus simplifying availing requisite healthcare by the beneficiaries. The \nbenefits of the Mission will be avai lable at public hospitals as well as empaneled private health care \nfacilities.  \nThe Union Budget 2018 also included a commitment of ~INR1,200 crore for Health and Wellness Centres \n(HWC), which will lay the foundation for  India’s health system as envisioned in the National Health Policy \n2017. These HWCs, to be set up by transformi ng 1.5 lakh Health Sub Centres from  2018 to 2022, are \naimed at shifting primary healthcare from selective (reproductive and child health / few infectious \ndiseases) to comprehensive  (including screening and management of NCDs; screening and basic \nmanagement of mental health ailments; care for common ophthalmic and ENT problems; basic dental \nhealth care; geriatric and palliative health care, and trauma care and emergency care) . NCDs account \nfor ~60% of mortality in Indi a, 55% of which is premature. NCDs are predominantly chronic conditions \nand impact the poor most adversely, given the high costs of treatment involved. Prevention and early \ndetection are therefore of the essence in reducing the disease burden attributable to these conditions as \nwell as ensuring long-term follow-up and management of symptoms for patients. The HWCs, under the \nnew implementation plan, will provide 12 basic healthcare services, expanding from the current package \nof 6 services. Crucially, these centres will provide preventive services to improve healthy behaviours for \n                                                      \n10 National Health Policy 2015 Draft \n \nNational Strategy for Artificial Intelligence \n \n27 \n \nfamily health and control the incidence of communicable and non -communicable diseases among the \npopulation covered by HWCs.  A key component of HW Cs will be universal screening for NCDs. \nScreening for five NCDs and associated risk factors has been prioritised given the high burden of disease \nassociated with them. These include  hypertension, diabetes, as well as  three common cancer s - oral, \nbreast and cervical. Screening for othe r conditions such as Chronic Obstructive Disease will be added \nsubsequently. The HWCs will be operated by a mid -level health service provider,  auxiliary nurse \nmidwives, accredited social health activists and a male health w orker responsible for comprehensive \nprimary health care services for a population of about 5,000. \nFigure 9: Features of HWC \n \nThe NHPM and HWC, in unison, are aimed at holistically addressing the health needs of the population, \nincluding health promotion and disease prevention as well as the delivery of primary, secondary and \ntertiary services.  \nIn addition, the government aims at leveraging technology to improve healthcare facilities through the: \na) the National eHealth Authority (NeHA) which will strategise eHealth adoption, define  standards and a \nframework for the health sector, put in place  electronic health exchanges for interoperability, \nb) the Integrated Health Information Program  (IHIP) to provide EHR to all citizens of India and provide \ninteroperability to existing EHR/EMRs, \nc) the Electronic Health Record Standards for India \n \nDespite the obvious economic potential, the healthcare sector in India remains multi -layered and \ncomplex, and is ripe for disruption from emerging technologies at multiple levels. It is probably the most \nintuitive and obvious use case primed for interventi on by AI driven  solutions, as evidenced by the \nincreasing activity from large corporates and startups alike in developing AI focused healthcare solutions. \nAdoption of AI for healthcare applications is expected to see an exponential increase in next few yea rs. \nThe healthcare market globally driven by AI is expected to register an explosive CAGR of 40% through \nCare in pregnancy \nand child-birth\n1\n2\n3\n4\nNeonatal, infant health \ncare services\nChildhood and \nadolescent healthcare\nFamily planning / \nreproductive healthcare\nCommunicable diseases \n(TB, Malaria etc.)\n5\n6 Screening, prevention \nand control of NCDs\nCommon ophthalmic \nand ENT care\n7\n8\n9\n10\nOut-patient care for acute \nsimple illnesses /  ailments\nBasic oral healthcare\nManageable emergency \nmedical services\nScreening / management \nof mental health ailments\n11\n12 Elderly and palliative \nhealth care services\nComprehensive \nprimary health care \nthrough HWCs\nRobust IT \nsystem, MLHP \nand payment \nreforms\nExpansion to 12 \nbasic services, \nintegration with \nAYUSH, health \npromotion\nHub and spoke model for connecting \nHWCs with PHCs\nDrug storage and dispensation, \nwaiting area for 30+ people\nTelemedicine facilities and point of \ncare diagnostics\nEquipped with consulting spaces and \nwellness rooms\n \nNational Strategy for Artificial Intelligence \n \n28 \n \n2021, and what was a USD600 million market in 2014 is expected to reach USD6.6 billion by 202111. The \nincreased advances in technology, and interest and  activity from innovators, provides opportunity for \nIndia to solve some of its long existing challenges in providing appropriate healthcare to a large section \nof its population. AI combined with robotics and Internet of Medical Things (IoMT) could potentially be the \nnew nervous system for healthcare, presenting solutions to address healthcare problems and helping the \ngovernment in meeting the above objectives. \n \nFigure 10: Potential use cases of AI in Healthcare \n \n \nAI solutions can augment the scarce personnel and lab facilities; help overcome the barriers to access \nand solve the accessibility problem; through early detection, diagnostic, decision making and treatment, \ncater to a large part of India.  \nCancer screening and treatment is an area where AI provides tremendous scope for targeted large scale \ninterventions. India sees an incidence of more than 1 million new cases of cancer every year, and early \ndetection and management can be crucial in an optimum cancer trea tment regimen across the country. \nNITI Aayog is in an advanced stage for launching a programme to develop a national repository of \nannotated and curated pathology images. Another related project under discussions is an Imaging \nBiobank for Cancer. \nBox 4: AI for India’s cancer woes \nCancer screening and treatment is an area where AI provides tremendous scope for targeted large scale \ninterventions. India sees an incidence of more than 1 million new cases of cancer every year, a number \nthat is likely to increase  given the increasing age of Indian population and lifestyle changes. Early \ndetection and management can be crucial in an optimum cancer treatment regimen across the country. \nGood quality pathology service is the essential building block of cancer care, wh ich unfortunately is not \neasily available outside select Indian cities. For an annual incidence of more than 1 million new cancer \ndiagnosis every year, India has barely 2,000 pathologists experienced in oncology, and less than 500 \n                                                      \n11 Frost & Sullivan, “From $600 M to $6 Billion, Artificial Intelligence Systems Poised for Dramatic Market Expansion in \nHealthcare” \nSource: \nPWC, “No \nlonger \nscience \nfiction,  \nAI and \nrobotics are \ntransforming \nhealthcare” \n \nNational Strategy for Artificial Intelligence \n \n29 \n \npathologists who could be  considered an expert oncopathologist. Machine learning solutions aimed at \nassisting a general pathologist  in making quality diagnosis can very well plug this gap in providing \nessential healthcare. An essential pre -requisite in implementation of such a sol ution is availability of \nquality annotated pathology datasets. NITI Aayog is in an advanced stage for launching a programme to \ndevelop a national repository of annotated and curated pathology images. The components of such a \nrepository include a move towar ds “Digital Pathology”, which entails all glass slides generated being \nscanned at high resolution and magnification, followed by accurate, precise and comprehensive \nannotation of the scanned images using various data sources & levels of clinical & patholog ical (gross \npathology, histopathology and molecular) information available from day-to-day patient care.  \nAnother related project under discussions is an Imaging Biobank for Cancer. Human cancers exhibit \nstrong phenotypic differences that may be visualised noninvasively by expert radiologists (using imaging \nmodalities). Recent literature suggests that certain image based features may correlate to molecular and \nclinical features like known mutations (KRAS, EGFR, etc.), receptor status, prognostic power, intra-tumor \nheterogeneity, gene expression patterns, etc. Reports have shown an association between radiographic \nimaging phenotypes and tumor stage, metabolism, hypoxia, angiogenesis and the underlying gene \nand/or protein expression profiles. These correlations, if rigorously established, may have a huge clinical \nimpact as imaging is routinely used in clinical practice. Moreover, this provides an unprecedented \nopportunity to use artificial intelligence to improve decision -support in cancer treatment at low cost  \nespecially in countries like India. AI based Radiomics is an emerging field that refers to the \ncomprehensive quantification of tumor phenotypes by applying a large number of quantitative imaging \nfeatures. It has resulted in improvement to existing biomark er signature panels by adding imaging \nfeatures. \nCredit: Tata Memorial Centre for developing the concepts for Digital Pathology and Imaging Biobank \nNITI Aayog is working with Microsoft and Forus Health to roll out a technology for early  detection of \ndiabetic retinopathy as a pilot project. 3Nethra, developed by Forus Health, is a portable device that can \nscreen for common eye problem. Integrating AI capabilities to this device using Microsoft’s retinal imaging \nAPIs  enables operators of 3Nethra device to get AI-powered insights even when they are working at eye \ncheckup camps in remote areas with nil or intermittent connectivity to the cloud. The resultant technology \nsolution also solves for quality issues with image capture and systems che cks in place to evaluate the \nusability of the image captured.  \nAI based healthcare solutions can also help in making healthcare services more proactive – moving from \n“sick” care to true “health” care, with emphasis on preventive techniques. \n  \n \nNational Strategy for Artificial Intelligence \n \n30 \n \nAgriculture \nWhile India has come a long way from being categorised as purely an agrarian economy, agriculture and \nallied sector still accounts for 49 % of India’s workforce,  16% of the country’s gross domestic product \n(GDP)12, and ensures food security to roughly 1.3 billion people.  \nAgriculture and allied sector is critical to India’s growth story. To achieve and maintain an annual growth \nrate of 8 –10% for the Indian economy, agriculture sector must grow 4% or higher rate. The Government \nof India has recently prioritise d Doubling Farmers’ Income as a National Agenda ; putting considerable \nfocus on supply chain perspectives in agriculture and market development in addition to productivity \naugmentation. \nDespite making impressive progress and receiving government attention, the sector continues to be \ndependent on unpredictable variables, has weak supply chain and low productivity.  \nIndia has not been able to completely remove its exploitative dependence on  resource intensive \nagricultural practices. Degradation of land, reduction in soil fertility, increased dependence on inorganic \nfertilizers for higher production, rapidly dropping water tables and emerging pest resistance are some of \nthe several  manifestations of India’s unsustainable agricultural practices. As global climate becomes \nmore vulnerable and unpredictable, dependence on unsustainable and resource intensive agriculture will \nonly heighten the risks of food scarcity and agricultural distress. \nThe sector suffers from poor resource utilisation, with the production quantum and productivity still being \nquite low. For example, yield of cereals, comprising a major share of food grain production, in terms of \nmagnitude is significantly lower than that of C hina and the USA. Technology adoption and efficient \nresource usage in these two countries are far higher, thus resulting in higher yields. \nSimilarly, use of water in agriculture continues to be high and sub-optimal. The practice of growing water \nintensive crops, and inefficient water management, makes India a net exporter of water and puts India’s \nlong run agronomic sustainability in question. Despite having just one -third of the gross cropped area \nunder irrigation, agriculture consumes 89% of our extracted groundwater. \nFigure 11: Comparison of yield and water footprint \n \nAgrarian distress in India has increased over time due to a multiplicity of factors. Fluctuating agric ultural \ngrowth rate, globalised value chains leading to variability in commodity prices, unpredictable changes in \n                                                      \n12 Economic Survey 2018 \nSource: \nWorld Bank \nand \nUNSECO-\nIHE Report \non Water \nFootprint \n2010 \n \nNational Strategy for Artificial Intelligence \n \n31 \n \nmonsoon rainfall over years and structurally inefficient domestic agricultural markets are just some of the \nreasons for income variability of farmers. Various Na tional Sample Survey rounds have shown the \nreduction in proportion of value share of crops to overall agricultural value from 78% to 69% since the \nGreen Revolution13. Thus income disparity between a farmer and non-agricultural worker has increased \nover the years.14 \nFigure 12: Income disparity \n \nOn the market side, non -existent functional end -to-end agriculture value chains have caused the price \nrealisation for farmers to remain low. Access to , and timely availability of services , across agricultural \nvalue chain at the farmers’ end thus becomes a challenge. At present, there is no functional mechanism \nto track the capacity of storage facilities available to the farmer. Value chain is not integrated through its \nentire length – procurement to market, including ICT , and banking services. The following figure \neffectively captures the present scenario. \nFigure 13: Agri-Commodity value chain in India \n \n                                                      \n13 Doubling Farmers Income Committee Estimates \n14 Chand, R., R. Saxena and S. Rana; Estimates and analysis of farm income in India (2015) \nSource: \nDoubling \nFarmers \nIncome \nCommittee \nEstimates \n \nNational Strategy for Artificial Intelligence \n \n32 \n \nAI will have significant global impact on agricultural productivity at all levels of the value chain.  \nAn estimate by Markets and Markets Research valued AI in agriculture to be USD432 million in 2016 and \nexpects it to grow at the rate of 22.5% CAGR to be valued at USD2.6 billion by 202515. \nAccording to CB Insights, agricultural tech startups have raised o ver USD800million in the last 5 years. \nDeals for startups using robotics and machine learning to solve problems in agriculture started gaining \nmomentum in 2014, in line with the rising interest in AI across multiple industries like healthcare, finance, \nand commerce. From analysing millions of satellite images to finding healthy strains of plant microbiome, \nthese startups have raised over USD500 million to bring AI and robotics to agriculture. \nGlobally, digital and AI technologies are helping solve pressing issues across the agriculture value chain. \nThe relative role of each technology in creating impact is dependent on the nature of the work, and the \nissues at hand. India has ~30 million farmers who own smartphones, which is expected to grow 3 times \nby 2020 and 315 million rural Indians will be using internet by 202016. An Accenture study says – digital \nfarming and connected farm services can impact 70 million India n farmers in 2020, adding USD 9 billion \nto farmer incomes . These are not futuristic scenarios, they are in play today, enabled by a vast digital \necosystem which includes traditional Original Equipment Manufacturers (OEM), software and services \ncompanies, cloud providers, open source platforms, startups, R&D institutions and others. Future growth \nis interdependent on the close partnership among these players. \nFigure 14: Ecosystem crucial for benefits of Precision Agriculture \n \nIn 2016, approximately 50 Indian agricultural, technology based startups (‘AgTech’) raised USD313 \nmillion17. For the first time, this sector is seeing widespread participation by startups. Intello Labs, for \nexample, uses image -recognition software to monitor crops and predict farm yields. Aibono uses agri -\ndata science and AI to provide solutions to stabilise crop yields. Trithi Robotics uses drone technology to \nallow farmers to monitor crops in real time and provide precise analysis of their soil. SatSure, a startup \n                                                      \n15 MarketsAndMarkets \n16 Forbes.com “For India's Farmers It's Agtech Startups, Not Government, That Is Key” \n17 Agfunder.com \nSource: \nAccenture \nResearch \n \nNational Strategy for Artificial Intelligence \n \n33 \n \nwith roots in India, uses ML techniques to assess images of farms and predict economic value of t heir \nfuture yield.  \nUse of AI and related technologies have the potential to impact productivity and efficiency at all of the \nabove stages of the agricultural value chain. \n Soil health monitoring and restoration : Image recognition and deep learning models h ave enabled \ndistributed soil health monitoring without the need of laboratory testing infrastructure. AI solutions \nintegrated with data signals from remote satellites, as well as local image capture in the farm, have \nmade it possible for farmers to take immediate actions to restore soil health. \n \nBox 5: Application for soil care \nBerlin-based agricultural tech startup PEAT has developed a deep learning application called Plantix that \nreportedly identifies potential defects and nutrient deficiencies in the soil. The analysis is conducted by \nsoftware algorithms which correlate particular foliage patterns with certain soil defects, plant pests and \ndiseases. The image recognition app  identifies possible defects through images captured by the user’s \nsmartphone camera. Users are then provided with soil restoration techniques, tips and other possible \nsolutions. \n Crop health monitoring and providing real time action advisories to farmers : The Indian agriculture \nsector is vulnerable to climate change due to being rain dependent. Varying weather patterns such \nas increase in temperature, changes in precipitation levels, and ground water density, can affect \nfarmers especially in the rainfed areas of the country. AI can be used to predict advisories for sowing, \npest control, input control can help in ensuring increased income and providing stability for the \nagricultural community. For example, many agronomic factors (such as vegetation health and  soil \nmoisture) can be monitored up to the farm level through remote sensing. Using remote sensed data, \nhigh resolution weather data, AI technologies, and AI platform, it is possible to monitor crops \nholistically and provide additional insights to the extension workers/farmers for their farms as & when \nrequired. \n \n Increasing efficiency of farm mechani sation: Image classification tools combined with remote and \nlocal sensed data can bring a revolutionary change in utilisation and efficiency of farm machinery, in \nareas of weed removal, early disease identification, produce harvesting and grading. Horticultural \npractices require a lot of monitoring at all levels of plant growth and AI tools provide round the clock \nmonitoring of these high value products.  \n \nBox 6: AI sowing app \nMicrosoft in collaboration with ICRISAT, developed an AI Sowing App powered by Microsoft Cortana \nIntelligence Suite including Machine Learning and Power BI. The app sends sowing advisories to \nparticipating farmers on the optimal date to sow. The best part – the farmers don’t need to install any \nsensors in their fields or incur any capital expenditure. All they needed was a feature phone capable of  \nreceiving text messages. The advisories contain ed essential information including the optimal sowing \ndate, soil test based fertilizer application, farm yard manure application, seed treatment, optimum sowing \ndepth, and more. In tandem with the app, a personali sed village advisory dashboard provided important \n \nNational Strategy for Artificial Intelligence \n \n34 \n \ninsights into soil health, recommended fertilizer, and seven-day weather forecasts. In 2017, the program \nwas expanded to touch more than 3,000 farmers across the states of Andhra Pradesh and Karnataka \nduring the Kharif crop cycle (rainy season) for a host of crops including groundnut, ragi, maize, rice and \ncotton, among others. The increase in yield ranged from 10% to 30% across crops.  \n \nBox 7: AI for herbicide optimisation \nBlue River Technology has designed and integrated computer vision and machine learning technology \nthat enables farmers to reduce the use of herbicides by spraying only where weeds are present, \noptimising the use of inputs in farming – a key objective of precision agriculture. \n Increasing the share of price realisation to producers: Current low levels of price realisation to farmers \n(as low as 20% in fruits and  vegetables18) are primarily due to ineffective price discovery and \ndissemination mechanisms, supply chain intermediary inefficiency and local regulations. Predictive \nanalytics using AI tools can bring more accurate supply and demand information to farmers, thus \nreducing information asymmetry between farmers and intermediaries. As commodity prices are \ninterlinked globally, big data analysis becomes imperative. Data from e -NAM, Agricultural Census \n(with data on over 138 million operational holdings), AGMARKET and over 110 million Soil Health \nSamples provide the volumes required for any predictive modelling.  \n \nBox 8: AI for Precision Farming \nNITI Aayog and IBM have partnered to develop a crop yield prediction model using AI to provide real time \nadvisory to farmers. IBM’s AI model for predictive insights to improve crop productivity, soil yield, control \nagricultural inputs and early warning on pest/disease outbreak will use data from remote sensing (ISRO), \nsoil health cards, IMD’s weather prediction and soil moisture/temperature, crop phenology etc. to give \naccurate prescriptions to farmers. The project is being implemented in 10 Aspirational Districts across \nthe States of Assam, Bihar, Jharkhand, Madhya Pradesh, Maharashtra, Rajasthan and Uttar Pradesh.  \n  \n                                                      \n18 DFI Committee Report \n \nNational Strategy for Artificial Intelligence \n \n35 \n \nEducation  \nAn effective education sector  has the ability to  transform a country through development of human \nresources and increased productivity. In the context of emerging countries particularly, levels of education \nand literacy of the population play an important role in development and overall transition to an advanced \neconomy.  \nIn India, the importance of a developed education sec tor is amplified by a  large youth  population. \nEstimates indicate that currently over half the population of the country is below the age of 25. As the \nadoption of digital means of gathering data increases, it is important that these methods are effectively \nleveraged to deliver improved education and teaching. \nThe adoption of technology in education is improving, though not at the pace required. It is estimated that \nschools globally spent nearly USD160 billion on education technology, or ‘EdTech’, in 2016, and forecast \nspending to grow 17% annually through 2020. Private investment in educational technology, broadly \ndefined as the use of computers or other technology to enhance teaching, grew 32% annually from 2011 \nthrough 2015, rising to USD4.5 billion global ly. Adoption of new technologies is still lacking, however, \noften attributed to unwillingness of teachers and students to adopt technology.   \nSchool education in India has seen substantial progress in recent decades, with efforts at both the Central \nand State levels, and substantive gains in enrolment have been achieved – Gross Enrolment Ratio (GER) \nis 97% at elementary level and 80% at secondary level, as per recent figures. However, low retention \nrates and poor learning outcomes mar the impact of gains in enrolment. \na) Low retention rates: Enrolment of children is of little use if children are not retained in the schooling \nsystem. Retention rate of 70.7% at elementary level indicates that one-third of enrolled children drop \nout before completing Class 8. Rete ntion rate at secondary level is also poor at 57.4 %. Low quality \nof education is one of the causes of poor retention. \n \nb) Poor learning outcomes : There is increasing concern about the poor learning levels of children in \nschool, and a new National Achievement Survey (NAS) was recently conducted in November 2017. \nPrevious rounds of NAS results provide an insight into longitudinal performance over time – average \nperformance of States / UTs on previous rounds showed that over 60% of Class 5 students scored \nbelow 50% across subjects; and for majority of the 31 States / UTs tested, performance significantly \ndeteriorated in NAS Cycle-4 versus Cycle-3. Assessments from the perspective of basic foundational \nskills also indicate poor learning outcomes – in rural areas, only 47.8% of Class 5 children could read \nClass 2 level text and only 26% could do Class-5-level arithmetic. \n \nThe above scenario is a consequence of a complex interplay of factors that pose challenges to improving \nthe quality of education: \na) Multi-grade and m ulti-level classrooms : For a large proportion of schools, especially in small or \nremote villages, it is not viable to have separate classrooms and teachers for different grades  / \nclasses. Consequently, the teacher is faced with a heterogenous group of children in the same \nclassroom, with wide variations in their classes, ages, abilities and learning levels. This large variation \nposes a huge challenge to the teacher and is a common cause of poor teaching-learning, thus leading \nto poor learning outcomes. \nc) Lack of interactive pedagogy and ineffective remedial instruction : Teaching-learning processes in \nmost classrooms are highly rote-based and non-interactive. Remedial instruction, where conducted, \ntypically lacks customisation to the child’s learning level, abilities, and pace of learning.  \n \nNational Strategy for Artificial Intelligence \n \n36 \n \n \nd) Inadequate attention / action for likely drop -outs: Several children may be at risk of dropout due to \nvarious factors, such as inadequate school infrastructure, poor teachers, poor school readiness, \nlanguage barriers, large learning gaps with respect to grade level, family circumstances (e.g. migrant \nfamilies), poor nutritional or health status, etc.  \n \ne) Large teacher vacancies due to uneven distribution across locations : Large number of teacher \nvacancies are mostly not due to an overall shortage of teachers in a State – instead, they are due to \nuneven distribution across different geographical areas within the State. For instance, recent figures \nfor Uttar Pradesh revealed 1.74 lakh teacher vacancies at elementary school lev el, but a \nsimultaneous surplus of 0.66 lakh teachers across the state. \n \nf) Professional development courses / training do not cater to real needs and have poor coverage \nExisting teacher training is typically a generic kind of an exercise. It is not linked to the specific \nweaknesses / requirements of a teacher – for instance, a teacher with poor arithmetic understanding \nrequires corresponding training to clarify arithmetic concepts. Consequently, most teacher training \nexercises end up as wasted public expenditu re, with little or n o benefit to the teacher and her / his \nstudents. Similar issues exist with respect to trainin g of other staff such as school \nheadmasters/principals. The coverage of existing training programs is also extrem ely low, typically \nless than 20% annually. \ng) Low adoption of existing technologies : A recent survey found that level of adoption of technology in \nschools is lacking, and can be largely attributed to lack of teache r training, despite provision of the \nICT infrastructure. While 83% of the teachers surveyed use computers, the use is limited primarily to \naudio / visual display, or student practice . A meagre 41% and 27% use technology for tracking \nstudent data and participating in forums respectively. This trend is even more pronounced in the low \nfee school segment surveyed19. Another trend observed is that trained teachers are much more likely \nto use technology in the classroom. 88% o f trained teachers reported making  use of available \ncomputers as compared to only 53% of untrained teachers. Trained teachers were found to be nearly \ntwice as likely to report using technology for communication purposes and for online forum \nparticipation20. \n \nAccording to EdTechXGlobal, EdTech is becoming a global phenomenon, and as distribution and \nplatforms scale internationally, the market is projected to grow at 17.0% per annum, to USD252 billion by \n2020. India’s digital le arning market was valued at USD 2 billion in 2016 and is pr ojected to grow at a \nCAGR of 30%, reaching USD5.7 billion in 2020 as per estimates from Technopak. \nAs per Forbes, in  2017, across every market involved in EdTech, international f unding reached a new \nrecord of USD 9.52 billion, and 813 different EdTech companies received funding s last year. These \nEdTech investments mark a gain of 30% from 201 6. VC interest in the education space continues to \ngrow. For example, one of India’s leading Ed Tech startups Byju’s raised USD40 million from Tencent in \nJuly 2017, jus t four months after raising USD 30 million from Belgium -based Verlinvest. Among Byju’s \nother investors include Sequoia Capital and The Chan Zuckerberg Foundation.  \nAI has the potential to bring about changes in the sector by supplementing pedagogy and establishing \nsystems to inform and support decision making across stakeholders and administrative levels. However, \n                                                      \n19 Cross Square Foundation, Ed Tech Adoption Survey \n20 Economic Survey, 2016-17 \n \nNational Strategy for Artificial Intelligence \n \n37 \n \nimplementation of AI must be preceded by efforts to digiti se records of teacher performance, student \nperformance, and curriculum.  Several AI tools are being successfully used in other parts of the world, \nand they can be adapted to the Indian context to target specific challenges. \na) Adaptive learning tools for customi sed learning: While AI may not completely replace a teacher, it \nhas the potential to greatly assist teachers in efficiently and effectively managing multi -level / multi-\ngrade classrooms, by judging learning levels of individual students, and allowing automated \ndevelopment of customi sed educational content adapted to each child’s class and learning level. \nAssessing time spent by a student on each part  / page of the learning material, for example, would \nallow real-time feedback on student performance to help the teacher appropriately tailor her guidance \nto the child. This concept can be extended to automatic grading of tests, as well.  \n \nb) Intelligent and interactive tutoring systems: Intelligent Tutoring Systems can provide great benefit to \nstudents through delivery of learning materials adapted to the child’s proficiency level, learning style, \nand pace of learning. In -built pop-up questions tailored to students, for exam ple, can help increase \ninteractivity, and catch student’s attention and interest. It can also help in assessment of student’s \nlevel of attention or comprehension to appropriately design remedial instruction. GradeGuardian, for \nexample, uses predictive mode ls and visuali sations for student performance with an interactive \ndashboard showing anticipated effect of policy  changes. Submission includes 3 components \npackaged as a single web app – a Chatbot that inputs student information, an Adv isor Console that \nshows students at risk, and a prediction module for policymakers. \n \nBox 9: Creating ‘smart content’ for improved interactivity \nContent Technologies Inc. (CTI), an AI research and development company, develops AI that creates \ncustomised educational content. Usi ng deep learning to absorb and analy se existing course materials, \ntextbooks, and course curriculum, the technology creates custom learning materials, including textbooks, \nchapter summaries, and multiple-choice tests.  \nA recent hackathon conducted by NITI A ayog also featured ‘ReadEx’, an android application that does \nreal-time question generation using NLP, content recommendations, and flashcard creation.  \nc) Predictive tools to inform pre-emptive action for students predicted to drop out of school: Analysis of \ntest results and attendance records using AI can be used to predict probable student activities and \ninform pre -emptive action. For instance, in a recent preliminary experiment conducted in Andhra \nPradesh, AI applications processed data on all students based on parameters such as gender, socio-\neconomic factors, academic performance, school infrastructure, teacher skills, etc., with the objective \nof helping the government identify students likely to drop out. Test results could inform suggestions \nto enroll students in vocational studies. Additionally, redressal mechanisms could be put in place to \nidentify students whose performance can be improved by focus of existing schemes to their family. \n  \n \nNational Strategy for Artificial Intelligence \n \n38 \n \nBox 10: Microsoft is helping in predicting drop outs in Andhra \nPradesh \nThe AP government is making concerted efforts to bring down the school dropout rate in the state. It has \ntied up with Microsoft to address this complex challenge. Based on specific parameters, such as gender, \nsocio-economic demographics, academic performance, school infrastructure and teacher skills, an \napplication powered by Azure Machine Learning processes the data pert aining to all students to find \npredictive patterns. \nWith these data insights,  the district education officials can intervene and help students who are most \nlikely to drop out. A variety of programs and counselling sessions could be conducted for these students \nand their parents. \nThe Andhra Pradesh government, based on machine learning and analytics, has identified about 19,500 \nprobable dropouts from government schools in Visakhapatnam district for the next academic year (2018-\n19). \nd) Automated rationalisation of teachers: AI tools can be used to develop automated teacher posting \nand transfer systems, using analytics based on demand – supply gaps across schools in the State, \ncandidate’s prior postings, candidate preferences, etc. This would help in plugging of gaps in teacher \ndistribution more effectively. \ne) Customised professional development courses: To tackle issues of poorly designed professional \ndevelopment courses with poor coverage, adaptive AI tools can be used to design automated, \ncustomised professional development training content for the teacher based on their performance, \nidentification of their knowledge and skill gaps. This could then be continuously adapted as \nteacher’s skills and concepts improve. \n \nBox 11: WriteToLearn by Pearson \nPearson’s WriteToLe arn software uses natural language processing technology to give students \npersonalised feedback, hints, and tips to improve their writing skills. In describing his experience using \nWriteToLearn, one 7th -grade English language arts teacher said, “ I feel it’ s pretty accurate. … Is it \nperfect? No. But when I reach that 67th essay, I’m not [really] accurate, either. As a team, [WriteToLearn \nand I] are pretty good .” Essay grading technology cannot substitute for a teacher’s ability to provide \nfeedback and coaching on particular words and sentences: the software merely rates students’ essays \nin genera l areas — such as organis ation, idea development, and style —and then provides generic \nsuggestions for improvement in these areas. But when teachers use the software as a first pass at grading \nand then interject their detailed feedback to address the improvement areas identified by the software, \nessay grading becomes a much less time -consuming and laborious process. The net result is that \nteachers can spend less time gra ding and more time teaching, while also giving students more \nopportunities to receive customised feedback on their writing. \n \n \nNational Strategy for Artificial Intelligence \n \n39 \n \nSmart Cities and Infrastructure \nIndia is currently in the midst of a surge of urbanisation. While the percentage of the population  living in \nurban areas was estimated to be 31% in 201121, recent research on satellite data indicates that this figure \nis close 45% today22, and predicted to rise to up -to 60 percent by 205023. Though seen as an important \naspect of a country’s economic growth  and a major step in the overall development of the country, \nunplanned urbanisation presents challenges such as congestion, over pollution, high crime rates, poor \nliving standards, and can potentially put a huge burden on the infrastructure and administrative needs of \nexisting Indian cities. \nTo tackle these challenges, the Government of India has embarked on an ambitious initiative to set up \nSmart Cities across India, aimed at driving economic growth and improving the quality of life , by \nharnessing IT solutions. As part of the Smart Cities Mission, 99 cities have been selected with expected \ninvestment of INR 2.04 lakh crores. The strategic components of these Smart Cities include city \nimprovement (retrofitting), city renewal (redevelopment) and city extension  (greenfield development) in \naddition to a pan -city initiative in which smart solutions are applied covering large parts of the city. The \nAtal Mission for Rejuvenation and Urban Transformation (AMRUT) is another related initiative which \ntargets improving the infrastructure of existing cities.  \nSmart cities attempt to address the challenges of urbanisation through development of features based \non IT solutions, some of which are listed below.  \na) Poor urban planning24: Smart cities aim to solve challenges of inefficient land use, improper land use \ncategorisation, area based development and lack of open spaces such as parks, playgrounds, and \nrecreational spaces in order to enhance the quality of life of citizens, reduce t he urban heat effect, \nand generally promote improved ecological balance. \nb) Inefficient utility distribution: Through large scale deployment of smart meters in both electricity and \nwater, smart cities being developed are trying to solve challenges such as low  visibility on usage of \nutilities such as electricity, water, and waste management. This is also targeted to help address \nissues of leakages in electricity and water distribution, and improper disposal of waste, and have the \npotential to significantly reduce cost associate with administration and management.  \nc) Improved delivery of citizen services: In the domain of service delivery, smart cities aim to harness \ndata to solve issues in low accountability and transparency. By using digital channels, they can help \naddress challenges in administration of offices, and long wait times. Today, poor standards  of \ngrievance redressal form another pressing issue that may be addressed by increasing adoption of \ntechnology based solutions.  \nd) Improving public safety: Cities in India today are hotbeds for a range of crimes. Smart cities aim to \naddress the issues of inc rease in crime and increased risk of urban emergencies through improved \ncity design and surveillance analytics.  \nSome Smart Cities have already begun implementing these features through specific projects. Pune, for \nexample, has launched The Pune Street Lig ht Project to setup energy efficient street lights that can be \nremote controlled through a Supervisory Control and Data Acquisition (SCADA) systems. Surat has built \na network of more than 600 surveillance cameras which will be expanded to all major locatio ns in the \n                                                      \n21 Census 2011 \n22 LiveMint: “How much of India is actually urban?” \n23 LiveMint: “60% of India’s population to live in cities by 2050: government” \n24 Smartcities.gov.in \n \nNational Strategy for Artificial Intelligence \n \n40 \n \ncity, as well as collaborated with Microsoft to develop solutions for water management and urban \nplanning.  \nDue to the large amount of data they can create, smart cities are especially amenable to application of \nAI, which can make sense of the data being generated, and transform it into predictive intelligence – thus \ntransitioning from a smart city to an ‘intelligent city’.  \nHowever, the wide range of connected devices also gives rise to increased risks in cyber security, with \nharmful actors such as hackers now capable of affecting city scale infrastructure.  \nSome use cases of AI that can augment the features of a smart city are listed below.  \na) Smart Parks and public facilities : Public facilities such as parks and other spaces contribute \nsubstantially to a city’s liveability. Use of AI to monitor patronage and accordingly control associated \nsystems such as pavement lighting, park maintenance and other operational conditions could lead \nto cost savings while also improving safety and accessibility.  \nb) Smart Homes : Smart homes concept is creating buzz with AI technologies being developed to \noptimise human effort in performing daily activities. Extending this concept to other domestic \napplications such as smart rooftops, water saving applications optimising  domestic water utilisation \nfor different human activities etc.  \nc) AI driven service delivery : Implementation of AI to leverage data on service delivery could see \napplication such as predictive service delivery on the b asis of citizen data, rationalis ation o f \nadministrative personnel on the basis of predicted service demand and migration trend analysis, and \nAI based grievance redressal through chat-bots.   \nd) Crowd management: Use of AI in providing effective solutions in crowd management in recent times \nhave been in vogue and given fruitful results in averting city -scale challenges such as managing \nmega footfall events, emergency and disasters. Accenture worked with the Singapore Government \nduring their SG50 Celebrations (50 th anniversary of Singapore’ independe nce), and developed \nsolution aimed at predicting crowd behavior and potential responses to incidents. The solution \nresulted in 85% accuracy in high crowd activity, crowd size estimation and object detection. Closer \nhome, the “Kumbh Mela Experiment” is aime d at predicting crowd behavior and possibility of a \nstampede. Similar Big Data and AI solutions could help with advance prediction and response \nmanagement. \ne) Intelligent safety systems: AI technology could provide safety through smart command centres with \nsophisticated surveillance systems that could keep checks on people’s movement, potential crime \nincidents, and general security of the residents. Social media intelligence platforms can provide aid \nto public safety by gathering information from social media and predicting potential activities that \ncould disrupt public peace. In the city of Surat, the crime rate has declined by 27% after the \nimplementation of AI powered safety systems.  \nf) Cyber-attacks: Cyber -attacks seem to pose a great threat to our institutio ns and public systems, \ntoday. AI technologies possess the capability to detect vulnerabilities and take remedial measures to \nminimise exposure of secure online platforms containing highly sensitive data from being targeted by \nunscrupulous social elements. \n \n  \n \nNational Strategy for Artificial Intelligence \n \n41 \n \nSmart Mobility and Transportation \nMobility and transportation form the backbone of the modern economy due to their  linkages with other \nsectors and importance in both domestic and international trade s. Today’s society demands a high \ndegree of mobility of various kinds, so as to enable efficient and safe transportation of both people and \ngoods. As a major contributor to overall emissions, this sector must also be sensitive to ideas of \nenvironmental sustainability.  \nIn India, majority of both passenger and freight traffic is carried through roads and railways. As of 2007 – \n08, roads and railways accounted for almost 87% of total freight traffic in the country and almost 90% of \ntotal traffic as of 2011 -1225. As the economy grows, it is expected that this reliance on these modes of \ntransport shall continue unless there are major shifts in the policy initiatives in the area. The fact that \nthese modes of transport are particularly pollution intensive compared t o shipping and air transport, \nfurther increase the need to implement smart practices in their deployment.  \nEven apart from issues in poor modal mix, the Indian transportation sector faces a variety of issues.  \na) Congestion and road accidents: Despite having one of the most extensive transportation networks in \nthe world, various sub -sectors sector continues to be underdeveloped leading to economic \ninefficiency and enormous human cost. Congestion and its associated costs in India are co ntinually \non the rise. St atistics from Government of India  and a study conducted by IIT Madras suggest the \nfollowing growth patterns over the years, respectively. \n \nFigure 15: Total registered motor vehicles per 1,000 population \n \n \n \n \n \n \n                                                      \n25 NTDPC, India Transport Report: Moving India to 2032 \n\n \nNational Strategy for Artificial Intelligence \n \n42 \n \n \nFigure 16: Projected cost of congestion (USD million / year) \n \n \nb) High number of traffic deaths: According to a PIB release by the Ministry of Road Transport and \nHighways (MORTH) in Mar ch 2017, the total number of road accidents in the countr y during 2015 \nwas 501,423 which resulted to 146,133 fatalities. The National Highways (NHs) accounted for about \n29.1% share of total road accidents and 35.0% of total fatalities. Although the existing NHs comprise \napproximately 1.9 % of total road network, they carry about 40% of total road traffic. According to \nMORTH statistics, there has been a steady increase in the number of o n-road accidents over the \nyears. \n \nc) Lack of public transportation infrastruc ture: Public transport infrastructure development remains \nlaggard in the overall discourse of transport policy design, either at national and regional levels, with \nfocus directed towards promoting and improving private car and associated infrastructure. Th e \nfollowing statistics from MORTH indicates the modal share of public transport (buses) over the years \nwhich has seen, but minimal increase over the years.  \n \n \n \n \n \n \n \n \n \n \n \n \n \nSource: \nCost \nEstimates  \nfor Road \nCongestion \nin Delhi, IIT \n(Madras) \n \nNational Strategy for Artificial Intelligence \n \n43 \n \nFigure 17: Total numbers of public buses per 1,000 population \n \n \nd) Assisted vehicle technologies: Autonomy is not economically viable in India currently as driver costs \nper kilometre is too low. However, investing in the suite of autonomous vehicle technologies and \nexporting such vehicles represents a significant economic  opportunity for India and since the same \ntechnologies can play a large role in reducing fatalities and decreasing congestion, it would be wise \nfor Indian manufactures to invest in research and development of the broader suite of technologies \nthat are essential for assistive AI. These technologies can assist the driver by taking driving decisions \nwhich the system has a high degree of confidence in and alerting the driver in case it has a low \ndegree of confidence in any decision. A prime example of such prod uctised, assistive AI is the \nadvanced cruise control used in Tesla vehicles today. This can follow highway traffic and the curves \nin the road as well as start and stop in response to traffic. However, the moment the driver gives any \ninput, the system is taken over by the driver. This hybrid approach is much safer than an unassisted \nhuman driving, without the potential drawbacks of having a completely autonomous system. There \nis another reason why India should not completely ignore assistive vehicle technology research, and \nit has to do with the development of new public infrastructure. Since we have only recently begun \nbuilding a large share of the total requirement of greenfield infrastructure, we have the benefit of \nhindsight.  \n \ne) Need for sustainable transportation: The recent initiative of the Government of India for announcing \ndevelopment of 100 Smart Cities is aimed at addressing this anomaly and catalyse smart strategies \nfor urban planning which promote sustainable land use design and multimodal integration. While new \ninitiatives could take time to show realisable impact, the existing issues in urban mobility related to \ncongestion, efficient traffic flow, movement of goods e tc. can indeed be solved using AI technology. \nAI can power multimodal integration by assisting with scheduling public transportation systems, \nimproved accessibility to public transportation infrastructure based on users’ choice behaviours, while \nalso suggesting real -time travel mode adv isory based on predicted traffic situation. AI enabled \nmobility solutions can ameliorate several of the challenges being faced by the Indian automotive and \ntransportation sector. \n \nf) Efficiencies in design of greenfield infrastructure  – Autonomous-ready traff ic will have significant \nimpacts on greenfield road infrastructure design and consequently on greenfield city design. Lane \nsize, lesser traffic congestion and reduced costs in upgradation of highway infrastructure are some \nof the externalities which will benefit the sector of assisted vehicle adoption.   \nSource: \nMORTH \n \nNational Strategy for Artificial Intelligence \n \n44 \n \n \nListed below are some of the major applications of AI on the mobility front beyond autonomous cars:  \na) Autonomous trucking: Autonomous technology in trucking has the potential to transform the way we \nmove goods today. AI can help increase safety and hauling efficiency through intelligent platooning, \nwherein trucks form platoons giving drivers the liberty to rest while the platoon keeps moving. Such \na method also ensures optimal road-space utilisation, helping improve road infrastructure capacity.  \n \nb) Intelligent Transportation Systems : Through the use of an intelligent traffic management system \nincluding sensors, CCTV cameras, automatic number plate recognition cameras, speed detection \ncameras, signalised pedestr ian crossings and stop line violation detection systems and the use of \nAI, real time dynamic decisions on traffic flows such as lane monitoring, access to exits, toll pricing, \nallocating right of way to public transport vehicles, enforcing traffic regulations through smart ticketing \netc. can be made. Accident heat maps could be generated using accident data and driver behaviour \nat specific locations on the road network related to topology, road geometric design, speed limit etc. \nand suitable measures could be pre-emptively taken to prevent possible accidents. Also, AI could \nhelp to design sophisticated urban traffic control systems that can optimise signal timings at the \nintersection, zonal and network level, while also facilitating services such as automati c vehicle \ndetection for extension of red/green phase or providing intermittent priority.  \n \nc) Travel route/flow optimisation: With access to traffic data at the network level, AI can help make smart \npredictions for public transport journeys by optimising total journey time including access time, waiting \ntime and travel time. Considering factors such as accessibility to nearest mode of travel, most \nconvenient access path based on local conditions and one’s preferences, AI can revolutioni se first-\nlast mile trave l which could change the way we perceive public transport journeys, today. About \nprivate car usage, AI could utilise a range of traffic data sets and one’s own preferences to make \nhuman-like decisions on route selection. With information on dynamic tolls a nd traffic flows on links, \nthe dependency on overhead Variable Messaging Systems (VMS) could be minimised, reducing \nsubstantial infrastructure costs. On the systemic level, AI can help predict flow of traffic at the network \nlevel and suggest alternative fl ow strategies in order to contain congestion, alleviating cities of this \nmajor issue.  \n \nd) AI for Railways: According to official figures, more than 500 train accidents occurred between 2012 -\n2017, 53% of them due to derailment. Train operators can obtain situational intelligence through real-\ntime operational data and analyse them in three different dimensions: s patial, temporal and nodal. \nFleet management and asset maintenance including that of rolling stock are pertinent AI use cases. \nRecently, the Ministry of Railways, Govt. of India has decided to use AI to undertake remote condition \nmonitoring using non-intrusive sensors for monitoring signals, track circuits, axle counters and their \nsub-systems of interlocking, power supply systems including the voltage and current levels, relays, \ntimers.  \n \ne) Community Based Parking: The availability of parking is a major issue  for Indian cities. AI can help \noptimise parking, likely by minimising vehicle downtime and maximising driving time. With the advent \nof electric vehicles, AI will be needed to mediate the complex vehicle grid interactions(VGI)  as well \nas for charging optim isation. Parking guidance systems help drivers to find vacant parking spaces \nwhile they are using the road network and have approached close to their destination. Community \n \nNational Strategy for Artificial Intelligence \n \n45 \n \nbased parking using AI helps cars in traffic to collect data on vacant parking spac es, and allocates \ncars to spaces such that the demand is always met. \nBox 12: Effect on R&D \nGlobally, research on autonomous vehicle has spurred advances, especially in AI fields of computer \nvision and robotics. Due to the extremely high market potential, over the past two years, most of the large \ninvestments in AI have been made in the field of autonomous vehicle as it is widely tipped to be the first \nlarge scale commercial application of AI to be adopted. \nMoreover, due to the congestion and chaotic conditi ons of Indian traffic, AI algorithms trained on Indian \ndriving data have the potential to be very robust.  \nError rates of object classification have fallen from 28.5% to 2.5% since 2010 according to the Stanford \nAI index. Therefore, current techniques are mature enough to be used in Indian conditions.  Also, within \nAI, the core technologies used have high transference potential. The same template used to identify \nobjects on a road can be used to identify cancerous cells in a pathological image.   \n \nNational Strategy for Artificial Intelligence \n \n46 \n \n \nKey challenges to adoption of AI in India \n Commonality of problems mandate an integrated approach  \n \nThe preceding analysis of focus sectors – Healthcare, Agriculture, Education, Smart Cities and \nInfrastructure, and Smart Mobility and Transport, highlight the potentia l of AI tools and technologies in \ntransforming the sectors and state of Indian economy as a whole. The analysis, however, also detail a \nmultitude of challenges that India needs to overcome to realise the full potential of a disruptive technology \nlike AI. \nAdopting a narrow view and focusing on the challenges for a specific sector, the barriers to developing a \nrobust set of AI applications may seem contextual and limited to that sector. Taking Healthcare sector as \nan example, enabling large scale adoption would require at least the following factors to be addressed:  \na) absence of collaborative effort between various stakeholders: while India has adopted electronic \nhealth record (EHR) policy, sharing of data between various hospital chains still remains a work \nin progress, since different hospital chains have adopted different interpretations of ‘digiti sing \nrecords’; \nb) relevant data is unavailable and there is absence of robust open clinical data sets; and  \nc) concerns on privacy and security of data, including lack of formal regulation around \nanonymisation of data. \nHowever, analysing across the focus sectors, the challenges are concentrated across common themes \nof: \na) Lack of enabling data ecosystems  \nb) Low intensity of AI research \ni. Core research in fundamental technologies \nii. Transforming core research into market applications \nc) Inadequate availability of AI expertise, manpower and skilling opportunities \nd) High resource cost and low awareness for adopting AI in business processes  \ne) Unclear privacy, security and ethical regulations  \nf) Unattractive Intellectual Property regime to incentivise research and adoption of AI \nThese challenges, while by no means exhaustive, if addressed in an expeditious manner through \nconcerted collaborative efforts by relevant stakehol ders, with government playing a leading role, could \nlead to fundamental building blocks that form the core to India’s march towards leadership in AI. The next \nsection of the paper attempts to solve some of these challenge through specific interventions and  \nrecommendations. These recommendations have been formulated as fundamentally infrastructural in \nnature, and hence span across sectoral use cases.\n  \n \nNational Strategy for Artificial Intelligence \n \n48 \n \n \n Way Forward to Harness the Power of AI \n Recommendations  \n \nIndia’s unique challenges and aspirations, combined with the advancement in AI, and a desire to assume \nleadership in this nascent technology means India’s approach towards AI strategy has to be balanced for \nboth local needs and greater good. The way forward for India in AI has to factor in our cur rent strengths \nin AI, or a lack thereof, and thus requires large scale transformational interventions, primarily led by the \ngovernment, with private sector providing able support. \nThis section lays down a set of recommendation to address the biggest challenges and opportunities for \nIndia in the field of AI. The preceding analysis of focus sectors lead us to the assertion that the efforts \nneed to be concentrated across major the mes of research, data democratis ation, accelerating adoption \nand reskilling – with privacy, security, ethics and intellectual property rights permeating as common \ndenominators for all our recommended initiatives. These challenges, while by no means exhaustive, if \naddressed in an expeditious manner through concerted collaborative effort s by relevant stakeholders, \nwith government playing a catalytic role, could lead to fundamental building blocks that can form the core \nto India’s march towards achieving its goal of #AIforAll. \nIndia’s capabilities in AI research are rather limited, both in  quantity (distant 5th globally) and especially \nin quality (disappointing impact of research produced). The research community is rather confined to a \nhandful of academic institutes, and relies on individual brilliance rather than institutional competence.  \nAcerbating the problem is the fact that private sector’s contribution to AI research has remained meagre. \nDespite some encouraging recent developments, viz. Government of Karnataka’s intention to set up a \nCentre of Excellence in AI in partnership with NAS SCOM, a lot of ground needs to be covered. The first \nset of recommendations focus on turbocharging both core and applied research. In addition, two \nframeworks for solving some of AI’s biggest research challenges through collaborative, market oriented \napproach have been proposed. \nThe new age of AI and related frontier technologies would disrupt the nature of jobs of tomorrow and the \nskills required to reali se the true potential of these transformative technologies. The changes and \nchallenges anticipated for the workforce will come from both the demand and supply side: demand for \ncapabilities for jobs that don’t even exist today and diminished demand for some of the jobs that could \nbe automated, supply of newly minted STEM graduates, a large portion of whom ma y struggle to be \ngainfully employed. Given our strength in advanced IT sector and the strength of favorable demographics, \nIndia may seem more equipped for workforce disruption that AI will bring, however our large numbers \nmay soon turn from potential asset s to liabilities if right structures are not put in place. Our next set of \nrecommendations focus on reskilling of existing workforce and preparing students for developing applied \nset of skills for the changing world of technology. \nEarly adoption of AI – be it the research community building technology infrastructure, the startup \ncommunity developing applications and corporations deploying solutions for their business needs, would \nbe one of the key determinants in ensuring leadership in AI. Adoption of AI in  India has remained rather \nlimited, less than a quarter of firms in India are using AI in any form for their business processes and \nstartup ecosystem in AI is virtually non -existent. Among the several impediments towards large scale \n \nNational Strategy for Artificial Intelligence \n \n49 \n \nadoption of AI in India, the primary ones include difficulty in access to data (more specifically, structured \nand intelligent data), high cost and low availability of computing infrastructure, lack of collaborative \napproach to solving for AI combined with low awareness. Our reco mmendations to address these \nchallenges include developing large foundational annotated data sets to democrati se data and multi -\nstakeholder marketplaces across the AI value chain (data, annotated data and AI models).  \nOne of the key aspects of our ambition of #AIforAll includes responsible AI: ensuring adequate privacy, \nsecurity and IP related concerns and balancing ethical considerations with need for innovation. Our final \nset of recommendations lay down the challenges and suggestion for addressing some of these not so \nstraightforward implementational challenges of AI.  \nThe recommendations in the following chapters are aimed at initiating an informed conversation on India’s \nfuture roadmap for AI, and are descriptive rather than prescriptive by design. The pa per should be seen \nas providing framework for developing National Strategy for Artificial Intelligence, and as such, we have \nconsciously avoided providing specific funding targets and funding mechanisms, as these require broad \nbased stakeholder consultations. \n  \n \nNational Strategy for Artificial Intelligence \n \n50 \n \n  \n Research \n Incentivi sing Core and A pplied research in AI \n \nAdvanced research, both core and applied, pro vides the basis for commercialisation and utilis ation of \nany emerging technology, more so for technologies like AI. \nWhere does India stand in Artificial Intelligence research? \nIndia has the necessary building blocks to develop a thriving AI research and development ecosystem, \nviz. availability of highly educated talent pool, world class educational institutes and an illustrious list of \ntop notch IT companies dominating the global IT landscape. Despite these advantages India sees itself \nlagging considerably in producing world -class research and innovation in most technology fields, more \nso in AI. \nIndia produced a whopping 2.6 million 26 STEM graduates in 2016, second only to China and more than \n4 times the graduates produced by USA, thus producing the requisite talent pool to drive innovation in \nemerging technologies. Disappointingly though, an overwhelming majority of this talent pool is focused \non routine IT development and not so much on research and innovation. Exacerbating the problem \nfurther, a majority of the small population focused on research almost always prefers to pursue advance \ndegrees (Masters or PhD degrees) to subsequently apply their expertise abroad.  \nAn analysis of India’s competence in core research in AI paints a somber picture. As per the Global AI \nTalent Report 2018, which crawled LinkedIn for its analysis, India only has 386 of a total of 22,000 PhD -\neducated researchers worldwi de, and is ranked 10 th globally. The report also looks at leading AI \nconferences globally for presenters who could be considered influential experts in their respective field \nof AI. On this met ric, India was ranked 13 th globally, with just 44 top -notch presenters. While these two \napproaches have their limitations and inherent biases, anecdotal evidence based on discussions with top \nresearchers reveals that serious research work in India is limited to less than 50 researchers, \nconcentrated mostly at institutes like IITs, IIITs and IISc. \nIn terms of the citable documents published in the field of AI from 2010 - 2016, India ranks a distant 5 th, \nfar behind the likes of China and USA and just about edging ahead of Germany and France who have \nconsiderably smaller STEM population. \n \n \n \n \n                                                      \n26 World Economic Forum \n \nNational Strategy for Artificial Intelligence \n \n51 \n \nFigure 18: Citable documents in Artificial Intelligence (2010 - 2016) \n \nDiving deeper into these numbers, if we look at the country wise H -Index (a metric that quantifies a \ncountry’s scientific productivity and scientific impact),  India ranks a dismal 19 th globally. In other words, \nwhile India may be producing research pieces in numbers, their utility has been rather limited .  \nFigure 19: H Index for Artificial Intelligence (1996 - 2016) \n \nLooking at the research coming out of academic institutes, the numbers are heavily skewed in favour of \ntop-15 institutes, that in total have contributed more than 42% of all research publications from 2 001 – \n201627. IISc dominates the research publications, with 7.5% of all publications coming from this institute. \nFor a country that has more than 750 universities and close to 40,000 colleges, this concentration of \npublication is a worrying sign. \nThe Indian IT services companies, the likes of TCS, Wipro and Infosys, have been the flag bearers of \nIndia’s competence in implementation of cutting edge technology solutions, yet their contribution to \nresearch has been limited. Given that these IT giants have been working closely with businesses globally \nand anticipating the trends in emerging technologies, it wouldn’t be unreasonable to expect a sizeable \nvolume of research work coming out of these companies. Yet, looking at all the research publications \nfrom 2001 – 2016, only 14% of all publications have come from industry, with universities contributing \n86% of all publications. Even this limited research publication universe by industry is dominated by Indian \nsubsidiaries of international companies (~70%), with only one Indian company in top-10 (TCS)28.  \n \n                                                      \n27 Neel Shah: “Research trends of AI in India” \n28 Neel Shah: “Research trends of AI in India” \nSource: \nScimago \nJournal and \nCountry \nRank (SJR) \nSource: \nScimago \nJournal and \nCountry \nRank (SJR) \n \nNational Strategy for Artificial Intelligence \n \n52 \n \nBox 13: What India can learn from other economies in terms of AI \nResearch \nThe US Government is estimated to have spent USD1.2 billion in non-classified research in 2016-18 and \nthe Defence Advanced Research Projects Agency (DARPA) is seeking a budget of USD 3.44 billion in \nfiscal year 2019-20, an increase of 8.5% compared wit h its request for fiscal 2018 -19. However, US \nleadership in AI investment has largely been driven by the private sector. The world’s leading companies \nin AI research in 2016 were Microsoft, Google and IBM, all US companies. According to CB Insights, \nbased on 2017 figures, Amazon, Google and Microsoft dominate enterprise AI – again all US companies. \nIt is estimated that mo re than half the world’s unicorns are from the US. The digital eco systems around \nthe hubs of Silicon Valley, Seattle, Boston and New York, which bring together talent and research \ncapabilities from leading universities, private investment and cross -science / industry collaboration, can \nbe considered to have played an important role in developing the US’s AI capabilities.  \nWhile still behind the US in terms of overall investment, China has clear ambitions to be at the same level \nas the US by 2020 and the worl d leader in AI by 2030, supported by a ne w development plan to create \na USD150 billion domestic AI industry. Its plans to build a new AI industry include a national fund that \nsupports research, f rom the most basic research to critical AI projects. The top 9 universities have \nreceived government funding to each establish an AI school and the remaining 32 to include an AI \nprogramme as part of their curriculum. The Ministry of Industry and Information Techno logy is planning \nto put nearly USD950 million dollars per year into strategic AI projects for State Owned Enterprises and \nthe public sector. In addition to state investment, the government is expected (at the time of writing) to \npublish Next Generation AI Development Guidelines immanently. The guidelines are expected to include \na clear governance structure, with allocation of responsibility and plans for research, industry and \nlegislative action or each of 2020, 2025 and 2030. \nWhile China’s approach  is not necessarily replicable in other parts of the world, there are two key \nlearnings from its programme: \n• Public sector investment, particularly in R&D, helps drive private investment.  \n• It has a plan with a governance structure and clear milestones. Having a plan instils confidence in \ninward investors. Based o n interviews Accenture carried out with inward investors in the UK, there \nwas consensus that the governments’ public messaging had a significant impact on companies’ \nconfidence and therefore willingness to invest in a country. \nIn the UK, the universities o f Cambridge and Oxford are considered centres of AI innovation; having \nalready stimulated three startups that made major AI breakthroughs and later became prime acquisition \ntargets. Google in 2014 bought DeepMind, Apple in 2015 purchased VocalIQ, and Micro soft bought \nSwiftKey in 2016. This success is supported by funding from organisations like the Leverhulme Trust, \nwhich provides annual funding of GBP80 million for research. Other capabilities include The Alan Turing \nInstitute: the national institute for d ata science. The Institute was established in 2015 by five founding \nuniversities (Cambridge, Edinburgh, Oxford, UCL and Warwick) and the UK Engineering and Physical \nSciences Research Council. The Institute’s researchers work across disciplines and look at theoretical \ndevelopment and application to real world problems. It was announced as the national centre for AI in \nNovember 2017 and six new universities will join the institute 2018. \nThe German Research Centre for Artificial Intelligence (DFKI – Deutsches Forschungszentrum für \nKünstliche Intelligenz) is one of the world’s largest AI research institutes. It has facilities in the German \ncities of Kaiserslautern, Saarbrücken, Bremen and Berlin and is partnering with companies on application \noriented basic research to develop product functions, prototypes and patentable solutions. \n \nNational Strategy for Artificial Intelligence \n \n53 \n \nThe EU’s Robotics Public Private Partnership, launched in 20 13, has seen the allocation of EUR 700 \nmillion for research to 2020. This is coupled with private invest ment for an overall b acking of EUR 2.8 \nbillion.  \nIt is believed to be the biggest civilian research programme in this area in the world and could be \nconsidered instrumental in the strong presence of Europe among service robot manufacturers. Clearly, \npublic-sector investment has paid-off. \nSource: Realising the economic and societal potential of responsible AI, Accenture \nThe research ecosystem in India has seen some green shoots in recent years. Encouraging is the fact \nthat the number of papers published has jumped 10 fold in last 10 years, from 331 papers in 2006 to \n3,301 papers in 2016 29. IISc, almost all the IITs, some of the IIITs and central / state universities have \nincreased their research efforts in various foundational and applied fields of AI. IIT Bombay and IIT Patna \nhave entered into a joint research collaboration with industry to focus on the applied aspects of AI. The \nresearch, focused on IT services and social good, will aim to provide powerful AI insights and \nrecommendations for improved productivity. It also incl udes software analytics  – building, testing, \nmanaging and modernisation of applications, solving real -life social issues such as malnutrition, human \ntrafficking and climate change through prediction and recommendation models using AI.  \nAnother research group at IISc is working on the theory and application of reinforcement learning (RL), \nan aspect of machine learning used in optimis ation problems. They are particularly interested in traffic \nhandling—both the vehicular kind on our roads, as well as the digital kind in our wireless networks. \nFurthermore, there have been some encouraging efforts from both government and private sector in \nfacilitating top quality research in recent times. Government of Karnataka is setting up a Centre of \nExcellence for Data Scien ce and Artificial Intelligence in partnership with NASSCOM. Wadhwani \nFoundation has set up India’s first research institute dedicated to developing AI solutions for social good \nin Mumbai in Feb 2018. \nHowever, the research ecosystem still has several obviou s gaps. The Detailed Project  Report of  \nInter-Ministerial National Mission on Interdisciplinary Cyber Physical  Systems has highlighted some of \nthese as:  \na) Lack of collaborative / interdisciplinary approach:  research is mostly focused in silos in academic \ninstitutions \nb) Lack of scale for experimental validation:  due to various practical and financial reasons, university \nresearch is largely restricted to theoretical or laboratory scale. This needs to be augmented with pilot \nprojects / large scale test beds / laboratories \nc) Lack of facilities to support large scale experimental test beds:  Large scale experimental test -beds \nare difficult to construct, maintain and operate, solely by academic institutions  \nd) Lack of connect with stakeholders and practitioners to convert outputs to outcomes: The views of the \nstakeholders in terms of what application problems to focus on will be of great importance to ensure \npractical applicability of the research. At the same time, this should be facilitated in a way which does \nnot constraint/suffocate the academic researchers in order for them to make foundational advances. \nInvolving “technology translators” at an early stage, i.e. entrepreneurs/agencies/companies which \ncan convert the research technologies to commercial products is needed. \n                                                      \n29 Scimago Journal and Country Rank (SJR) \n \nNational Strategy for Artificial Intelligence \n \n54 \n \ne) Lack of large scale mission mode project management capabilities: Academic researchers usually \nwork best individually (with a small team of students and research project staff). Current approaches \nto research and related facilities may not be suited for large scale experimental projects. \nWhile the numbers point to a small yet encouraging base, a concerted effort is needed to build a \ncomprehensive research focused AI strategy for India, one that will position India towards global \nleadership in this emerging area of technology. What is evident though is that incremental changes would \nnot suffice and there is need for transformational changes to boost research major push coming from the \ngovernment. \nFramework for promoting Artificial Intelligence Research in India \nThe Detailed Project Report of Inter -Ministerial National Mission on Interdisciplinary  Cyber Physical \nSystems (IM-ICPS) has suggested the following four -tier framework for promoting research focused on \nall aspects of technology life-cycle: research, technology deployment, translation and management:  \na) ICON (International Centres of New Knowledge):  focusing on creation of new knowledge through \nbasic research,  \nb) CROSS (Centre for Research On Sub -Systems): focusing on developing and integrating core \ntechnologies developed at ICON layer and any other sources \nc) CASTLE (Center for Advanced Studies, Translational research and Leadership): focusing on \ndevelopment and deployment of application based research and  \nd) CETIT (Centre of Excellence in Technology Innovation and Transfer): focusing on commercialisation \nof technologies developed \nWhile the above structure has significant advantages, a far more simplified and agile approach is required \nto ensure seamless, target ed and accountable framework for  promoting research. Hence the following \ntwo-tier integrated approach to boost both core and applied research in AI is proposed:  \na) COREs (Centres of Research Excellence in Artificial Intelligence): COREs will focus on core research \nof AI, and will take on the mantle of executing the responsibilities of both ICON and CROSS as per \nthe IM -ICPS framework. Thus, CO REs will speciali se in creating new knowledge through basic \nresearch and will source for fundamental knowledge / technologies that will be needed to keep India \nprepared for the next generation of technologies. Furthermore, CO REs will also emphasi se on \ndevelopment infrastructure tools for direct application of basic research, including development of \nnew areas of AI architecture / platforms. \nb) ICTAI (International Centre for Transformational Artificial Intelligence):  ICTAIs will provide the eco -\nsystem for application based technology development and deployment, and will take on the mantle \nof executing the responsibilities of both CASTLE and CETIT as per the IM-ICPS framework. This will \nbe an industry -led initiative and expec ted to take on the top -level challenges identified or inter -\nministerial projects calling for AI based solutions. Furthermore, ICTAIs will also be responsible for \ndelivering commercial technology, and taking ideas  / concepts or prototypes and turning them i nto \nmarketable products by way of proactive coordination, communication and interfacing for technology \ntransfer to the industry. \n \n \n \nNational Strategy for Artificial Intelligence \n \n55 \n \nFigure 20: Proposed integration of COREs and ICTAIs \n \nIn summary, COREs will be focused on core research, in ev olving and new areas of AI, and will act as \nthe technology feeders for ICTAIs which will be focused on creating AI based applications for, and \naccelerating early adoption in, domains of societal importance.  \nIn addition, an umbrella organis ation should be established to address issues relating  to access to \nfinance, social sustainability and the global competitiveness of the technologies  developed. This body, \nwhich shall be recognised as the Centre for Studies on  Technological Sustainability (CSTS) , could be \nestablished on the lines of the Campus  for Research Excellence and Technological Enterprise \n(CREATE), Singapore program  or Innovate UK. The major responsibilities of CSTS could be on the \nfollowing lines: \na) to monitor the impact of the AI technologies developed at the consumer level through social indices \nand recommend necessary modifications for a better market penetration  \nb) to study the financial viability of the AI technologies developed such that it caters to the target \nconsumer base, while proposing improved pricing models for a pan India reach \nc) to study best practices on pricing models and social penetration of AI technologies across the world \nand recommend strategies to foster globally competitive technological development  \nd) to catalyse international collaborations for COREs and ICTAIs \ne) to study the AI landscape in other nations and design strategies for customisation and deployment \nof developed AI technologies as per their specifications for a global impact  \nf) to provide a knowledge management platform for  AI technologies by organising international \nworkshops and conferences, promoting the confluence of thought leaders, practitioners and \nauthorities \n \nCentres of Excellence for Artificial Intelligence \nCOREs for AI will focus on core research, in evolving and new areas of AI.  \n Physical AI\nCognitive AI\nSensory AI\nGeneral AI\n38%\nICTAIs in priority domains of \nAgriculture, Health, Education, Smart \nCities and Infrastructure and Smart \nMobility and Transportation\nInternational Centres for \nTransformational AI\nCentresof Research Excellence\n \nNational Strategy for Artificial Intelligence \n \n56 \n \nTo start with, COREs could be established at IISc, ISI and top  IITs and IIITs. Given that the research in \nAI needs to be multi -disciplinary, linkages need to be established with premier  institutions in other \ndisciplines viz. AIIMS for hea lthcare, TISS for arts and social science etc.  Furthermore, these COREs \nshould also act as a guide and mentor for other institutes  researching in AI, in a hub -and-spoke model, \nto enable broad based development of AI research capabilities across India. \nThe COREs would have to build on both the short -term and the long-term capabilities of these research \ncenters. In the short -term, given the paucity of quality faculty in India in AI, appropriate incentivisation \nmechanism (which could be a com bination of promis e of topnotch infrastructural facilities and \nremunerations matching in ternational standards) to bring top-tier international faculty, especially the \nIndian diaspora, needs to be developed. Furthermore, the top Indian PhD aspirants, who would otherwise \npursue their studies from  top universities abroad, will need to be retained in India, again through \nappropriate incentivisation mechanism. One possible way could be to institute National AI Fellowships.  \nIn the long-term, successive PhD classes of these COREs can increase the faculty pool and work towards \na sustainable operational model for COREs. \nPossible focus area for the COREs for AI could be: \na) Sensory AI (Computer Vision, IoT etc.), \nb) Physical AI (Robotics, Industrial Automation etc.), \nc) Cognitive AI (NLP, worker training etc.), \nd) General AI, \ne) High precision learning from small data sets, \nf) Research on new algorithms (e.g. advance cryptography, security), data sets etc. , and \ng) Explainable AI \nAn application based model may be followed to selected COREs wherein an applicant institute would \nhave to demonstrate a viability plan, in terms of faculty and other capabilities, before being anointed an \nCORE. A CORE can choose to work on one or multiple focus areas. The COREs will be encouraged to \npursue projects across other COREs, to promote linkages and cross -functional technologies. The \nfinancial component of COREs for AI, which could be to the tune of INR 50 crore – INR 100 crore per \nCORE, should also include large scale funding for specific projects.  \nInternational Centers for Transformational Artificial Intelligence \nInternational Centers for Transformational Artificial Intellig ence (ICTAIs) are envisioned as institutions \nfocused on creating AI based  applications for, and accelerating early adoptions in, domains for societal \nimportance. These applied research and development institutions should be set up with the elements of \ndomain or industry “pull” baked into their structure and operation. While core (or fundamental AI) research \nand teaching may not be seen as their priority areas, some flexibility might be built in the structure.  \nFrom both funding and operational perspective, the ICTAIs are envisioned to be a truly public private \npartnership. A professionally managed society / s ection 8 company (“ ICTAI Inc.”) should be set up and \ntrusted with the initial contri bution from the government. The management team for ICTAI Inc. should \ncomprise of suitable representation from  the government, but should hav e a majority of independent \nprivate sector representation. The  mandate for ICTAI Inc. would be to select the ICTAIs to fund and \noversee their progress. In  addition, ICTAIs may commission “ moonshot projects” that may span across \nmultiple ICTAIs or may need limited term independent project teams to be set up.  \nICTAIs are expected to leverage the strength of AI technologies to solve for application in a specific focus \nsector areas. Suggested focus sector areas to begin with, as identified in this  report, are H ealthcare, \n \nNational Strategy for Artificial Intelligence \n \n57 \n \nAgriculture, Education, Smart Mobility  and Transportation , Smart Cities and  Infrastructure. ICTAI Inc. \nhowever should have the autonomy to decide on sectors to focus on and sequencing of sector selection. \nICTAI Inc. should work proactively with private sector institutions to seek partnerships to set up the ICTAIs \nin priority domains. \nFurthermore, ICTAIs should have well -defined linkages with the COREs. As a technology feeder for \nICTAIs, COREs should be incentivised to commercialise their research at ICTAIs.  \nThe key to success for the ICTAIs would be their leadersh ip teams. The decision on which ICTAIs to \ninvest in could involve believing in capability and  the vision of the proposed CEO and rest of the \nmanagement team who have appli ed for ICTAI fun ding. Alternatively, ICTAI Inc. may also search and \nselect a CEO and entrust her with running a particular ICTAI.  \nThe initial funding for a ICTAI would constitute the seed funding (in the range of INR 200 crore – INR 500 \ncrore per ICTAI) for first 5 years to cover the major operational expenses of the ICTAI, in  addition to the \nphysical infrastructure and technology / computing infrastructure. The seed  funding should be a \ncombination of funding from the government and private participation,  preferably in equity/grant sharing \nmechanism. Corporates should be free to take a  predetermined amount of ownership. One corporate \nshould be partnered with for each ICTAI based on a “challenge method”. The incentive for corporates to \nparticipate would arise from the following points that the government should endeavour to ensure: \na) Access to high quality training data \nb) Computational and physical infrastructure \nc) A chance for staff at corporates to be part of a national mission and work on challenging \nproblems with a higher gestation period than traditional commercial problems \nd) Ability to count expenditure incurred towards CSR \ne) Boost in visibility received by working on AI for social good directly with top government \ninstitutions \nThe ICTAIs should raise additional f unding beyond this base funding, through philanthropic / private \ncontributions, preferably in equity sharing mechanism as well. ICTAI Inc. may also award further \ncontributions to ICTAIs based on measures of success. \nEach ICTAI should have a Governance Board, comprising of industry leaders, academic luminaries and \nglobal thought leaders. The CEO and / or the industry partner (if one is there) may suggest the \nGovernance Board, which may be vetted and agreed by ICTAI Inc. An industry partner with sizeable \ncontribution may have a reasonable say in selection of the Governance Board. The responsibilities of \nGovernance Board will be akin to a Board of Directors / Board of Governors of any suitably large \nsuccessful corporation i.e. provide overall guidance and direc tion, and oversee the output for the ICTAI. \nWhile ICTAI Inc. may lay down the success criteria for ICTAIs in general, specific targets for each ICTAI \nshould be decided between the Governance Board and leadership team of individual ICTAIs . \nThe ICTAIs should  ideally be located close to top engineering institutes / close to large cities, so that \nattracting the best talent from across the world is plausible. ICTAIs should also seek to leverage the \navailability of students / research scholars from the engineerin g institutes close to them for specific \nprojects etc. (through internships etc.) to build the talent pool for future. ICTAIs should aim to involve the \nsuccessful AI researchers / practitioners of Indian origin from across the world in either full time or \nadvisory capacity. If finances are a constraint for ICTAIs to attract the best talent, ICTAI Inc. may consider \nNational AI Fellowships for the benefit of ICTAIs. \n \nNational Strategy for Artificial Intelligence \n \n58 \n \nPRAIRIE Institute, rece ntly established by the French g overnment in collaboration with academia  and \nindustry, is a good potential implementational model for CTAIs. \nBox 14: The PRAIRIE Institute \nThe PRAIRIE Institute (PaRis  Artificial Intelligence Research Institute)  is a collaboration of industry and \nacademia supported by the French government to create an institution which becomes an international \nbenchmark in AI. \nThe partners (CNRS, Inria and PSL University, together with  Amazon, Criteo, Facebook, Faurecia, \nGoogle, Microsoft, NAVER LABS, Nokia Bell Labs, PSA Group, SUEZ and Valeo) in PRAIRIE institute \nare pursuing three goals:  \n• to drive progress in fundamental knowledge creation in AI (AI) freely distributed among the \ninternational scientific community; \n• to take part in solving concrete problems with a major application-related impact; \n• to contribute to training in the field of AI. \nThe five-year objective is to bring together AI scientific and industrial leaders and mak e the PRAIRIE \nInstitute a world leader in AI.  \nThe aim of the PRAIRIE Institute is to act as a catalyst for exchanges between the academic and industrial \nworlds, to train new generations of researchers in AI and to play a role in leading and coordinating t he \ncommunity. Transfer and innovation will be among its duties, along with scientific progress. The work \ndone will highlight an integrated approach to the two traditional branches of research:  \n• upstream research, calling on partner facilities of excellence in France and abroad; \n• research focusing on companies and applications, drawing on industrial partners, who are often also \nworld leaders in their fields. \nIntegration between research topics will facilitate synergy between the two branches of the PRAIRIE  \nInstitute and will enable researchers to make the transition easily from one to the other.  \nAt an international level, the PRAIRIE Institute will draw on a network of partnerships with centres of AI \nexcellence to promote exchanges and leverage impact. Coll aborative agreements have already been \nsigned with the Center for Data Science  at NYU, the AI laboratory of UC Berkeley (BAIR), the  Robotics \ninstitute at Carnegie -Mellon University in Pittsburgh,  MILA in Montréal, the  Max Planck institute  in \nTübingen, the CIIRC (Czech Institute of Informatics, Robotics and Cybernetics) and the Turing Institute in \nLondon. \nCommon Compute Platform for CoEs / CTAIs \nFor COREs and ICTAIs to discharge their duties effectively, pooled cloud infrastructure for AI applications \nshould be made available. All the COR Es and ICTAIs should be connected to the National Knowledge \nNetwork (NKN) and from there via a very high-speed link to a pooled cloud computing environment. This \napproach will reduce infrastructural requirements due to pooling efficiencies and reduce operational and \nmaintenance costs while keeping national data secure (Box 15: AIRAWAT) \n \nNational Strategy for Artificial Intelligence \n \n59 \n \nBox 15: AIRAWAT (AI Research, Analytics and knoWledge \nAssimilation plaTform) \nScope  \nAIRAWAT will be a cloud platform for Big Data Analytics and Assimilation, with a large, power-optimised \nAI Computing infrastructure using advanced AI processing. The proposed Infrastructure will be equipped \nwith facilities for world’s leading machine learning including deep learning, high performance high \nthroughput supercomputing, infrastructure to store, process, simulate and analyse big data sets like \nimages, video, text, sound, speech.  AIRAWAT will support advancement of AI -based developments in \nimage recognition, speech recognition, natural language processing for research, development and \ncreation of varieties of new applications for the support of advancements in the fields of Agriculture & \nHealthcare.  \nInfrastructure Requirements \n1. AI Infrastructure: High throughput processing supercomputing systems to train the machines learn \nthe data sets using AI deep learning.  We propose to set up a 100 -AI petaflop computing \nInfrastructure, like the one as JAPAN’s ABCI AI Supercomputing facility.  \n2. Multi-tenant multi-user computing support through resource partitioning and provisioning, dynamic \ncomputing environment deployment, etc. \n3. Energy-saving, high Teraflops per Watt per U rack space designed computing systems with low TCO \n4. Deep Learning Software stack – Training and Inferencing development kit, frameworks, libraries, \ncloud management software and portal for data labelling, data analytics, data transfer & data model \nexchanges \n5. Low latency High bandwidth network  \n6. Mass Storage System to collect, store and share multi-petabytes of big data \n \nUtilisation \nBig data Labelling, Annotating, Anonymisation, Analytics, Skills development, job creation, support R&D, \nAcademia, startups, entrepreneurs and end  users for agriculture & healthcare advancements. \nintelligence. \nCredit: nVIDIA \n \nPursuing excellence in research in Artificial Intelligence \nTo achieve technology leadership in AI, India also needs to pursue “moonshot” projects – ambitious \nexplorations that aim to push the technology frontier and that would req uire the pursuit of world class \ntechnology development and leadership in ap plying AI technologies to solve some of the biggest \nchallenges. A potential project could be teste d on a twin criteria of wheth er it is (a) a new technology or \nscientific area that has emerged  or gained traction and has the capability of solving, often in new ways, \npractical problems of  importance, and (b) whether it addresses emerging user needs that existing and \navailable technology solutions cannot address. An example could be development of advanced na tural \nlanguage processing (NLP) infrastructure for Indian languages, with features of sent iment and semantic \nanalysis, or imaging biobank for early cancer detection. \n \nNational Strategy for Artificial Intelligence \n \n60 \n \nSome experts believe that such ambitious projects should be driven by the industry, since they have the \nrequisite resources, technical expertise and are attu ned to practical use cases; and should rarely be \npushed forward by government initiative s. Google’s ad vances in NLP, as underlined by Duplex, is one \nsuch example of private sector  led innovation that pushes the realisable boundaries of technology. \nHowever, the inherent risk and ambition of such projects does not necessarily lay themselves well to the \nfunctioning and methods of most corporations. \nICTAI Inc., the overall governing body for ICTAIs, coul d be the DARPA (see Box 16) equivalent of AI \nresearch in India. ICTAI Inc. should have  the full autonomy on selecting the projects of importance and \nexecuting these projects, and should be trusted with a separate budget for pursuing such projects. Some \nof the key elements of execution include: \na) Projects with pre -defined durations and tenures : Although ICTAIs could adopt the moonshot \napproach to select projects, th e projects shall have pre -defined durations for efficient management, \ntimely reali sation of research deliverables and financial sustainability. The type of project could \ngovern the timeline, for example a project that requires integration of existing AI technologies to solve \na specific problem could have a shorter duration, whereas projects which would require a unique \ntechnological development could be allocated a timeline suitably.  \nb) Teams of contractors : Specialised teams of contractors who have proven cr edentials in advanced \ntechnological development shall be chosen to undertake the projects. Focus could be laid more on \nthe qualitative impact of past research activity, and less on the size or global recognition.  \nc) Special emphasis on selecting the right project leaders: Project leaders with the right skill sets to lead \nprojects of national importance shall be key to drive the research output of projects undertaken at \nICTAIs. They shall be people with proven capabilities in applied research, and the acumen to deliver \ntechnology in the form most suitable for India and the developing world.  \nd) Independence: Independence to select projects, design timelines and deliverables and allocate \nbudgets shall fall within the ambit of the ICTAIs with no intervention of any government agency. \nSelection of the project teams based on credentials while collaborating with premier research \ninstitutions with full independence shall ensure a robust technological development that creates \nimpact. \ne) Fail fast model : ICTAIs shall be g iven the autonomy to shut down projects and shift resources to \nother operational projects.  With the moonshot approach for project selection, it is plausible that \ncertain research projects do not provide expected results in terms of efficiency, scalability  or cost. In \nsuch cases, diverting resources to other operational projects with higher probability of research \noutput delivery would ensure financial efficacy. \n \nProjects could be housed at one ICTAI, but could involve working across multiple ICTAIs and COREs.  \nThe algorithms and data used in an AI powered application are key elements in ensuring operational \nsuccess. Therefore, it is imperative that the Intellectual Property Regime in the context of AI be robust \nand enforceable for innovators to have the confidence that they will be able to make profits from and take \ncredit for their work. This is essential for the promotion of innovation, entrepreneurship and core and \napplied research in the field of AI. \n \nNational Strategy for Artificial Intelligence \n \n61 \n \nHowever, IP in the context of AI has some key differen ces with IP in the context of generic computer \nprograms or other content due to the way such algorithms are designed and trained with large datasets. \nThe data fed to an AI algorithm during the training step is key to its success.  Keeping this and other \nissues in mind, the government may set up a task force, comprising jointly of Ministry of Corporate Affairs \nand DIPP to examine and issue appropriate modifications to the IP regulatory regime pertaining to AI.  \nBox 16: Defense Advanced Research Projects Agency \nPentagon’s Defense Advanced Research Projects Agency (DARPA) was created in 1958 as response \nto increased US concerns that Soviet Union may have achieved technological superiority following its \nsuccessful launch of Spuntnik. DARPA’s founding mission was simple and nimble, “to prevent and create \nstrategic surprise”. \nFor the past 60 years, DARPA has been the beacon of scientific breakthroughs and radical innovations. \nAmongst its success stories include the internet, RISC computing; global positioning satell ites; stealth \ntechnology; unmanned aerial vehicles, or “drones. While originally created for US military, the agency \nhas played a pivotal role in giving genesis to several multi-billion dollar industries. \nWhat is even more impressive is DARPA’s agility and  swiftness despite a small team and modest \nbudgetary allocations. DARPA’s total support staff is only 120 strong, and its annual budget is only USD3 \nbillion. DARPA accomplishes most of its programs through a “speciali sed project” model – assembling \nproject specific teams of experts from universities, industry, government and non-profit, with well-defined \ntargets and tenures. \nThe three essential elements of DARPA are: \na) ambitious goals: the projects pursued by DARPA are designed to solve real-world problems or create \nnew opportunities. The problems are such that they can’t be solved without pursuing new frontiers or \ncatalysing new developments. Urgent need for application underlines the problems.  \nb) temporary project teams: teams of world-class experts from industry and academia are assembled \nto work on well-defined projects of relatively short duration, led by accomplished technical managers. \nGiven the intensity, sharp focus and reputation attached, these projects attract h igh caliber talent \nwho achieve extraordinary levels of collaboration. \nc) independence: DARPA has complete autonomy in selecting and running projects.  \n \nSource: Harvard Business Review article “Special Forces” Innovation: How DARPA Attacks Problems published October 2013 \nCERN for AI \nGary Marcus, Professor of Psychology at NYU, mooted the idea of a  CERN for AI at the #AIforGood \nSummit in Geneva in June 2017. In an OpEd for the New York Times, Gary further elaborated on the \nidea: \n“I look with envy at my peers in high -energy physics, and in particular at CERN, the European \nOrganisation for Nuclear Research, a huge, international collaboration, with thousands of scientists and \nbillions of dollars of funding. They pursue ambitious, tightly defined projects (like using the Large Hadron \nCollider to discover the Higgs boson) and share their results with the world, rather than restricting them \nto a single country or corporation.” \n \nNational Strategy for Artificial Intelligence \n \n62 \n \nBox 17: European Organisation for Nuclear Research aka CERN \nCERN, established in 1954 by 1 2 founding European nations, had its origins in establishing a truly \ninternational scientific collaboration to pursue world class research in particle physics. Among the biggest \nof the several achievements to come out of this remarkable collaboration, home  to the world’s biggest \natom smasher (the Large Hadron Collider) include the development of World Wide Web and the discovery \nof the Higgs boson (the ‘God Particle’), W and Z bosons, Antimatter etc. \nCERN’s current 21 member states each contribute to the ove rall CERN budget (CHF1,240 million in \n2013). Special contributions are made to specific projects, like Large Hardon Collider, by interested Host \nStates and non-member states. The biggest chunk of CERN’s budget is spent on the construction of its \nenormous scientific installations, most of which are too expensive for a single country to afford.  \nThe current state of AI, which is focused on solutions to narrow applications, and grappling with questions \naround next phases of development, ethics, security and pri vacy may benefit from a global public AI \nresearch institution advancing the field for the good of humanity, the “People’s AI” 30.  \nTo achieve #AIforAll, which gives the mandate for inclusive AI for the world, the Government of India \nshould take the lead in bringing together the relevant parties to create People’s AI, the CERN for AI. India \nhas a proven track record for leading projects with ambitions of greater good. India is already playing a \nleading role in climate leadership, with Hon’ble Prime Minister Na rendra Modi vowing to go “above and \nbeyond” India’s commitment on Paris Agreement on climate change. Similarly, India has been a pioneer \nin a sustained push for clean energy revolution by leading the International Solar Alliance, and setting an \nambitious t arget of 100GW of installed solar energy capacity by 2022. With 20GW of installed solar \ncapacity, India is well and truly on its way to achieving this target. With the Government  of India’s focus \non inclusive growth which saw several transformational reforms in the last few years, India is poised and \nmost suited to wear the mantle of leading the #AIforAll movement. Furthermore, India offers the best \npossible test bed and a plethora of use cases for building AI solutions fulfilling the inclusive AI criteria, be \nit in healthcare, education or agriculture. \nWhile the modalities of funding and mandate for this #AIforAll should be the subject of further \ndeliberations, the proposed centre should ideally be funded by a mix of government funding and \ncontributions fro m large companies pursuing AI (GAFAM, BATX etc.). Where should this centre be \nlocated? Nowhere and everywhere. While the CERN had the requirements of physical facilities such as \nthe Large Hadron Collider, #AIforAll could be distributed across different reg ions and countries. The \nGovernment of India, through NITI Aayog, can be the coordinating agency for initial funding and setting \nup the requisite mandates and human and computing resources.  \nThe mandates of the #AIforAll should include challenges of AI that  are common to other countries and \nlarge corporations and startups alike, and the foundational components that could truly make AI inclusive \nand good for all. Suggestive technology topics could include: \na) General AI; \nb) Opening the Black Box / Explainable AI; \nc) Advanced anonymisation protocols for data security and privacy; \n                                                      \n30 Medium.com “The People’s AI: democratizing AI research and development” \n \nNational Strategy for Artificial Intelligence \n \n63 \n \nd) Ethics in AI; and \ne) AI approach to solving world’s biggest problems in healthcare, education, urbanisation, agriculture \netc. \nA start has been made by OpenAI, set up by the likes of Elon Musk and  Sam Altman, with a mission to \ndiscover and enact the path to safe artificial general intelligence. Similarly, Declaration on Cooperation \nin Artificial Intelligence was signed by 25 states of European Union on 10 th April 2018. What is needed \nthough is a commitment and collaboration of truly international standards to develop and ensure #AIforAll. \n  \n \nNational Strategy for Artificial Intelligence \n \n64 \n \n \n Skilling for the AI age \n Getting India ready for the AI wave  \n \nHistory suggests how technology has disrupted the nature of jobs and the skills required to perform them, \nrequiring the global workforce to continuously adapt. Advent of AI has accelerated this disruption to a \npace that has not previously been seen, due to the wide range of capabilities it offers and speed at which \nit is developing. \nNASCCOM predicts that by 2022, a startling 46% of the Indian workforce will be engaged in entirely new \njobs that do not exist today or jobs that have radically changed skill sets31. Some other sources estimate \nthat demand for AI and machine learning specialists in India  is expected to see a 60% rise by 201832. In \nthe data domain as well, an independent study estimated that India will face a demand -supply gap of \n2,00,000 data analytics professionals by 2020. \nIn the IT-BPM sector, traditional software developer roles are set to transition to roles such as computer \nvision engineers, Robotic Process Automation (RPA) engineers and cloud architects, among others. At \nthe same time, completely new job roles such as language processing specialists and 3D modelling \nengineers are s et to arise as the technologies are increasingly adopted and deployed 33. With AI, such \ntransition would move beyond the IT sector and affect sectors such as education, health, agriculture, \nfinance etc, requiring the underlying skill sets.  \n \n \n \n \n \n \n \n \n \n \n \n \n                                                      \n31 Future of Jobs in India: A 2022 Perspective, 2017  \n32 KellyOCG, India \n33 Future of Jobs in India: A 2022 Perspective, 2017  \n \nNational Strategy for Artificial Intelligence \n \n65 \n \nFigure 21: Transition in jobs and skill sets \n \nThe demand for new-age jobs is accelerating in India, and can be attributed to three major factors34, viz. \nincreased adoption of technology; shift in market demographics and de-acceleration of globalisation.  \nFigure 22: Impact of primary forces on jobs in 2022 \n \nAs technology adoption increases across sectors, so will the demand for skills required to implement  \nthem. India’s growing middle class and millennial demographic form a major part of the market for \ntechnology enabled products. As the size of these demographics increases, so will the demand for these \nproducts, and thus the workforce that can enable their  creation. Increased globalisation played a major \npart in the creation of jobs in the services sector. However, as the permeation of automation increases, \n                                                      \n34 Future of Jobs in India: A 2022 Perspective, 2017  \nSource: \nReworking \nthe \nRevolution, \nAccenture \nSource: \nFuture of \nJobs in \nIndia: A \n2022 \nPerspective, \n2017 \n \nNational Strategy for Artificial Intelligence \n \n66 \n \nand protectionism among previous markets for these services becomes predominant, it is important to \nrecognise that these jobs will see a large overhaul.  \nIndia may appear to be relatively well positioned to take advantage of the AI disruption by virtue of its \nadvanced IT sector and large youth demographic potential to establish itself as the future hub fo r AI \nrelated activities. However, given the poor availability of qualified faculty and researchers, this advantage \ncould fast transform into a liability without urgent government interventions towards promoting access to \nsuch skills. This is a critical component of AI development, and should be a national priority.  \nChanging nature of the global service sector, uncertain impact of automation in manufacturing, and poor \ninfrastructure can pose a challenge to the task of enabling up-skilling or re-skilling.  \nShifts in the outsourcing needs of developed economies have the potential to impact India dramatically. \nAs multinational companies move toward ‘in -shoring’ or ‘no-shoring’ due to development of automation \ntechnology and increasingly protectionist measures g lobally, it is of critical importance that the \noutsourcing industry (which cur rently accounts for roughly 15%  of India’s total labour force) adapts to \nthese changing needs accordingly.  \nFigure 23: India IT-BPM sector \n \nAs indicated below , automation could affect even traditional sectors in a variety of ways. Agriculture is \npredicted to see a net decrease in jobs due to automation, while the job opportunities in the construction \nsector are predicted to improve significantly. These large scale shifts in employment depend largely on \nthe nature of technology being developed as well as deployed, and are very hard to predict for any \ninformed policy intervention.  \n  \nSource: \nNASSCOM, \nICRIER, \nMcKinsey \nanalysis \n \nNational Strategy for Artificial Intelligence \n \n67 \n \nFigure 24: Impact of automation \n \nMultiple studies have underlined the lack of employment readiness of STEM graduates, thus highlighting \nthe poor standard of education in engineering colleges in India. A s per some est imates, almost 80% of \nengineering graduates are unemployable on graduation . Low availability of speciali sed faculty; lack of \nflexibility in curriculum revisions in engineering and computer science courses to keep up with rapid \nadvancements in technology; a nd low levels of interdisciplinary research in AI related fields to facilitate \nan AI education for non-computer science engineering (and vice versa), are just some of the factors that \nhave led to this scenario.  \nAt the school l evel as well, poor outcomes i n Maths and reading are particularly troubling , since these \nsubjects form the foundation of knowledge required to move to an AI related education, and later jobs in \nthe domain. \nRecommendations \nFor addressing issues relating to skilling, a two-pronged approach is warranted, one set of interventions \naimed at the workforce and the second for the students.  \n1. Workforce \nRe-skilling of the current workforce will require integration with relevant existing skilling initiatives, building \nof new platforms that can enable improved learning, and novel methods of allowing large scale \nemployment generation through promotion of AI.  \na) Incentivising creation of jobs that could constitute the new service industry : To tackle the challenge \nof shifts  in the services industry, it is important to identify and promote creation of jobs that may \nreplace traditional IT -BPM sector jobs in the future. These jobs would ideally be a part of the AI \nsolution development value chain, but require a relatively low l evel of expertise so as to create \nemployment at scale . Tasks such as data annotation, for example, have the potential to employ a \nlarge quantum of human resources, and serve countries all over the world in otherwise capital \nSource: \nMcKinsey, \nJobs Lost, \nJobs \nGained: \nWorkforce \nTransitions \nin a Time of \nAutomation \n \nNational Strategy for Artificial Intelligence \n \n68 \n \nintensive projects. Tasks such a s image classification or speech transcription require low levels of \nexpertise and present an opportunity to exploit labour cost arbitrage to serve companies globally. \nSpecific policy interventions could be considered like tax holidays, inclusion under CSR  activities, \netc. to help solve the dual problem of workforce job displacement and creation of expertise in \nfundamental sections of the solution development value chain. \n \nBox 18: Impact sourcing through Samasource \nSamasource is a pioneer in impact sourcing of technology services, the practice of hiring people from the \nbottom of the income pyramid and directly raising them out of poverty by providing digital work for \ncompanies like Google, Walmart, eBay, and many startups.  \nAbout two-thirds of their work is in managed services for image capture and annotation. They need a \nlarge pool of skilled, detail -oriented workers to identify the appropriate images or video frames and \nmanually tag the scene, specific regions (a forest), or objects (a wolf) with the specif ic keywords the \ncustomer requires for their application. \nSamasource quotes its top objective as ‘creating technology jobs for the impoverished while delivering \nthe highest quality work in a cost -effective manner’. This  guided Samasource to focus on E ast Africa, \nwhere the majority of the population has completed secondary or high school , and has a good \nunderstanding of basic English language. They have a dedicated training organi sation that works within \nthe local communities to identify qualified men, women and youth in need and assess them for key skills. \nFor those with strong visual acuity, there is general computer and business skills training and a 3 -week \ndedicated machine learning and image annotation training track with example projects and ongoing \nassessments that lead to graduation \nAs workers get moved to projects, there is another set of qualification tests and project -specific training. \nOnce they pass those, they are moved into production for the project and their work is reviewed daily by \nthe QA team to correct mistakes and resolve interpretation or requirements issues (do you want to tag a \ntraffic light facing away from the camera?). These reviews are done both locally and in the US, allowing \nreal-time feedback with many checkpoints throughout the p roject lifecycle, reinforcing Samasource’s \ntightly managed approach to quality. \nSamasource reportedly provide jobs and digital skills training to people below the poverty line in Kenya, \nUganda, Haiti, and India. Since the company started in 2008, Samasourc e has been able to hire nearly \n8,000 people. As a result, they’ve could transition themselves and their dependents out of poverty and \nprovide living wages, transforming over 30,000 lives. \nb) Recognition and standardi sation of informal training institutions : The increasing demand for AI or \ndata related job positions has not gone unnoticed by the Indian workforce, with a large percentage \nof them opting for training institutions to bridge their knowledge gaps. In technology hubs such as \nBengaluru, this has led to  many traditional IT training institutions establishing courses in new age \ntechnologies. However, their standard of education is hard to assess for companies looking to hire. \nImplementation of recogni sed certificate courses through higher education institu tions could be a \nmajor boost to recognising resources spent on re-skilling, and holding these institutions to standards \nin delivery of knowledge. International School of Engineering (INSOFE), for example, provides \ncertification recognised by the Language Technologies Institute of Carnegie Mellon University (CMU) \nfor a post-graduate program in data analytics and optimisation. Jigsaw Academy’s data science post-\ngraduate program gets its students certified from the University of Chicago Graham School . \n \nNational Strategy for Artificial Intelligence \n \n69 \n \nIntegration and application of existing standards such as laid out by the National Skill Qualification \nFramework (NSQF) should also be explored. Given that standards in areas such as big data exist \nbut are not used by institutions for certification also highlights  the need for them to be designed in \ncloser collaboration with the private sector. \n \nc) Creation of open platforms for learning: Initiatives such as the NASSCOM Future Skills Platform will \nplay an instrumental role in large scale dissemination of requisite skills to some major sections of the \nemployed workforce. Online and self -learning platforms, such as Coursera and edX, are able to \nconnect learners to the best universities and institutions from around the world, and can play a crucial \nrole in this scenario. There is need to bring out guidelines for promoting these while ensuring \nuniformity, standards and usability. As in the promotion of MOOCs, large scale deployment and \nadoption of these platforms requires stringent measuring of quality, and recognition of t heir \ncertification. \n \nd) Creating financial incentives for reskilling of employees : Initiatives in reskilling of employees or \nallowing employees to undergo reskilling initiatives have a high opportunity cost for private \ncompanies, and may affect their willingness to let their employees engage in the process at scale. It \nis thus suggested that co -funded models between the government and companies be explored, in \nthe IT sector particularly. Financial incentives for private companies could include payroll taxes which \nare dedicated to subsidising training opportunities, income tax deductions for companies participating \nin reskilling initiatives, special taxes to be paid if a minimum training budget is not disbursed, as well \nas public grants for subsidi sing training especially for smaller sized firms.  Considering also the time \nrequired for reskilling, and the cost it entails to employers, financial incentives may also be tied to \nmandatory allocation of time for reskilling activities by companies for their workforce. However, in the \nabsence of standardi sation of training modules and institutions , such initiatives could be prone to \nmisuse. \n \n2. Students \nThe education sector needs to be re -aligned in order to effectively harness the potential of AI in a \nsustainable manner.  In primary and secondary schools, there is a need for transition to skill based \neducation in subjects relevant to AI. Often critici sed for being overly knowledge intensive, Indian \neducation is in urgent need of transition particularly in subjects relevant  to STEM, or computer based \neducation. As jobs based on technology become prominent, so will the need to develop applied skills in \na continuously changing environment.  \nIncreased amount of project related work across education levels, promoting schemes lik e the \nestablishment of ATLs (Atal Tinkering Labs) in schools, necessary change in curricula in schools, are \nsome of the steps that need to be considered to promote early adoption of technology organically.  \nIn higher education institutions there is need for increased collaboration between industry and academia \nthrough creation of channels of communication between faculty and industry to promote exchange of \nideas and expertise. Various avenues of collaboration need to be explored, including workshops, \nincentives for guest lectures by professionals and institutional arrangements for regular re -design of \ncourses in collaboration with the private sector.  \nLack of qualified faculty that poses a serious problem in the present scenario can be addressed through \ninnovative initiatives like credit-bearing MOOCs  (Massive Open Online Courses). Acceptability and \nadoption of these decentralised teaching mechanisms can be ensured through prescribed certification in \ncollaboration with the private sector and educational instit utions. Initiatives such as the SWAYAM \n \nNational Strategy for Artificial Intelligence \n \n70 \n \nplatform35, are in the right direction but these need to be further reinforced through additional investment \nand collaboration with the private sector and educational institutions in order to meet the market \ndemand.      \nCountries like the USA , that have a thriving student and research community engaged in AI, have \neffective bridge courses, generally at post graduate levels for non-related fields. Thus at a post-graduate \nlevel, the focus shifts to creating incentives to conduct research in domestic institutions and allow cross-\ndisciplinary collaboration and education . Similar initiatives are required in Indian educational space with \nbridge courses in AI for post-graduates in non-computer science or data science domains t o encourage \ncross-domain research and the ‘AI + X’ paradigm.  One year courses could be explored that will enable \nstudents of a range of subjects to build foundations of knowledge in the AI space, and effectively apply \ndomain expertise to solve pressing problems.  \nThis is an ever evolving area and needs to go beyond the one time solutions with an institutional \nframework in place that is responsive to changing scenarios. Such a framework would enable a more \nsustainable and customised solution. A standing committee or task force comprising of all stakeholders  \nmay be constituted by the government with the objective of examining and reporting the changes in \nemployment caused by AI in India. This taskforce would consider not only IT related activities, but the job \necosystem as a whole. As AI continues to evolve, this will facilitate evidence based d ecisions in funding \nof educational institutions, promotion of specific sectors, and channelling of human resources to be most \nefficiently utilised.  \n  \n                                                      \n35 developed by the All India Council for Technical Education (AICTE) and the Ministry for Human Resource Development \n(MHRD) \n \nNational Strategy for Artificial Intelligence \n \n71 \n \n \n Accelerating Adoption \n AI across the value  chain  \n \nGlobal Context  \nAdoption of AI globally is still in its nascent stages, but growing rapidly. A 2017 survey by  Statista finds \nthat 78% of firms globally are either using AI extensively, or have plans  for use in near future. Firms in \nChina and the US especially, are proactively engaging with  their research communities to enable early \nadoption and position themselves competitively.  \nFigure 25: Survey of global firms on adoption of emerging technologies in 2017 (In which of the \nfollowing new and emerging technologies is your organisation making investments over the next year?) \n \nA McKinsey report on “AI: The next digital frontier” concludes that AI adoption outside of the tech sector \nis at an early and often experimental stage and deployment at scale has been rare. In the survey of 3,000 \nAI-aware top management executives, across 10 countries and 14 sectors, only 20% use any AI related \ntechnology at scale or in a core part of their businesses.  \nAs the field is rapidly evolving, investments made in core AI research and product development is another  \nindicator of adoption. The McKinsey report estimates that organisations invested around USD26 – USD39 \nbillion in AI while USD6 – USD9 billion was invested in startups in the year 2016.  \nAdoption of AI in India has been slow and remains limited. Estimates indicate that only 22% of the firms \nin India use AI in any business process 36. Indian startups have been able to raise just USD87 million in \n2017, as against over USD28 billion raised by the Chinese startups in 201737.  \nLow adoption of AI technologies in India is particularly troubling , given the country’s  prominence in \nthe global IT industry that could have given it the natural first mover’s advantage  in AI. However, the IT \nindustry in India has remained content in delivering traditional IT services and has been slow to adapt to \nnew digital technologies compared to its counterparts in China and the US.   \n                                                      \n36 Intel and IDC Survey Report, dated \n37 XinhuaNet “AI sector sees big investment, financing in 2017” \nSource: \nAccenture \n \nNational Strategy for Artificial Intelligence \n \n72 \n \nDespite its sectoral leadership and programming talent,  India’s IT industry with over USD 160 billion \nannual revenue, has yet to build pioneering AI  / ML capabilities commensurate with its potential. \nNevertheless, the top 5 IT service companies have begun to use AI to cut costs and automate business \nprocesses. Wipro has built Holmes, an AI platform that deploys “bots” to carry out repetitive and mundane \ntasks. TCS has created its own AI platform, Ignio, and Infosys has built Nia, improving upon its earlier \nMano platform. While promising, these early efforts remain far from revolutionary.  \nThe limited success of Indian technology players to effectively adapt and carry forward the AI revolution \nsuggests the need for government intervention to promote AI adoption, lest India lose the chance to \nsecure a prominent position on the global AI map. While acknowledging the ne ed to promote AI, \ngovernments at different levels, along with their various instrumentalities, should adopt proactive \nmeasures to accelerate AI adoption in various processes.  \nThe major market segments for the increased AI adoption are:  \n(a) Private enterprises: mostly driven by market and enterprise considerations, \n(b) Public Sector Undertakings: imperative to drive up the operational efficiency of PSUs, and \n(c) Government: improve process efficiency, reduce human discretion, eliminate middlemen, advance \nprediction, pro-active and predictive service delivery to citizens. \nThe steps taken by the g overnment, in terms of incentives or in providing infrastructural platforms, will \nhave varying control and influence on all these segments – least on private enterpris es to most on \ngovernment agencies. \nAI adoption in India will face the following challenges:  \n(d) Lack of adequate talent to build and deploy AI systems at scale . An estimate claims that only 4% of \nAI professionals in India have worked on emerging technologies such as deep learning and neural \nnetworks. There is also a significant gap of PhD research scholars in the field. \n(e) Difficulty in access to industry specific data required to build customi sed platforms and solutions \ncurrently concentrated in the hands of a f ew major players . It is difficult for new entrants to deliver \ntailor made services that can compete with data rich incumbents such as Facebook or Google. This \nphenomenon results in the creation of a virtuous cycle which reinforces the hegemony of the big few, \ncreating a huge entry barrier for startups. \n(f) High cost and low availability of computing infrastructure required for development, training and \ndeployment of AI based services. Cloud infrastructure, though growing rapidly, has limited capability. \nAlso lacking are AI -as-a-service models of cloud platforms. Lack of infrastructure has led to many \nIndian AI startups to incorporate their business outside the country, which makes AI outside the reach \nof Indian researchers in government labs and many industries.  Initiatives like GI Cloud (MeghRaj), \nare in the right direction.  \n(g) Low awareness of AI  for resolving business problems in most public enterprises and government \nagencies, especially given the scarcity of AI professionals, is obstructing adoption.  \n \n \n \nNational Strategy for Artificial Intelligence \n \n73 \n \nFollowing specific initiatives are recommended for promoting AI adoption in the country:  \nRecommendation 1: Creating a multi-stakeholder Marketplace \nTo encourage the development of sustainable AI solutions at an appropriate price point for sectors such \nas health, education, and agriculture, it is necessary that a level playing field be ensured and a supportive \nenvironment be created for all players in the value chain. The development of any working AI -based \nproduct is a long process with very different specialised activities that are necessary for final delivery, just \nlike any other product or service value chain. \nFigure 26: AI Value Chain \n \nIt is exceedingly difficult for a small or medium business / startup to vertically integrate all these processes \nand have the internal capacity to deploy them simultaneously before even entering the market. Therefore, \nthis acts as a barrier to entry for new players. On the other hand, for different firms to take up different \nactivities and still deliver useful products, a strong , stable price discovery mechanism has to exist for \nincentives to align and sustainable business models to come up for the different parts of the value chain. \nAnother well documented and substantial barrier to entry is the difficulty of acquiring the raw m aterial \n(data) in the first place since as noted above, most usable data is in the hands of a few players.  \nTowards providing a level playing field, addressing information asymmetry, and incentivi sing and \nsimplifying collaboration between the various stakeholders in the AI ecosystem, a marketplace may prove \nto be a potent tool provided it enables the following:  \n(a) Discovery of required AI component : ability to reference assets , be it  data or ML models, and \nservices, such as annotation, and enable curation and rating of these assets. \n(b) Platform to execute transactions: with mechanisms to exchange value of a specific good and service \nfor currency or subscription models for APIs. \n(c) Verification of transactions: ability to verify the occurrence of transactions and the receipt of goods or \nservices in exchange for currency.  \nSuch a marketplace model would benefit by: \na) Reducing asymmetry of information: With a reasonable volume of transactions, aberrations caused \ndue to data oligopoly will reduce, thus reducing the spread i n pricing and effectively incentivi sing \nboth data owners and AI model creators. \nb) Encouraging specialisation in different niches by firms and creation of novel business models: Easy \navailability of raw components required at different stages of the AI solution development cycle \nwould allow firms to focus on specific problems, rather than attempt to build capability across the \nvalue chain. For example, a new segment of startups could entirely focus on the problem of curating \nraw data to be put on the marketplace, rather than attempting to do all the above at once. \n\n \nNational Strategy for Artificial Intelligence \n \n74 \n \nConsequently, such startups will see a better market for VC funding as they will be able to \nsubstantially mitigate their business risks, if an easily accessible, formal market for raw data exists.  \nc) Unlocking new sources of data and enabling more efficient use of computational and human \nresources: Estimates suggest that only 1% of data today is analysed due to its existence in various \nunconnected siloes. For example, medical imaging / diagnostic centres today are collecting valuable \ndata. It is currently not used to its potential as there is no way for diagnostic centres to build predictive \ndisease models without hiring AI e xperts, renting computational expenditure, and indeed shifting \ngears completely to become an AI company. In the presence of a formal market with a fair \nmechanism of price discovery, such diagnostic centres would have an incentive to collect this data, \nhave it curated, and place this data in the market with appropriate permissions and safeguards. \nConcurrently, machine learning experts in AI firms would be more productive as they could focus on \nthe problem of creating the model rather than that of sourcing the data and curating it.  \nd) Address ethical concerns regarding data sharing : Today, transactions and buying of data occur in \nan informal marketplace. However, there is no mechanism currently to ensure that appropriate \npermissions are taken from the actual dat a owners before data custodians share the data. Going \nback to the example of diagnostic centres (which are data custodians in this case), it would become \nnecessary for firms to take consent of individuals getting imaged to aggregate and sell their data. \nThis could create mechanisms through which individuals are ultimately compensated. In this way, \nthe informal market for data could be nudged towards entering the formal econom y. \nRecommendation 2: Facilitating creation of large foundational annotated data sets \nIn India, lack of annotated data in the domestic context has emerged as a major impediment in \ndevelopment of AI solutions for both startups and core research alike. Availability of general data corpora \nwhich can be applied across product functions can serve to provide a ready source of data (in ‘plugin’ \nmode) to startups and enable solutions customi sed to the Indian context. There is evidenc e to suggest \nthat ready availability of large corpora can spur research and innovation in the field of machine learning. \n \nFigure 27: Breakthroughs in AI \n \nBy its very nature, this task is laborious and despite innovations in automatic annotation, such as the \ndevelopment of GoogleNet object classification, human annotation and training of data sets are not \n\n \nNational Strategy for Artificial Intelligence \n \n75 \n \nexpected to be replaced in the near future. This is b est exemplified by the thriving investments and \nstartups in this area. As lately as 2017, Alegion, Scale, CloudFactory, Mighty AI, and CrowdFlower, all \ncompanies that enable human intervention to data a nnotation, have received about USD 50 million in \ninvestment funding.  \nGiven the nature of this task, it is necessary for the government to explore assistance in building of large \ncorpora of data across domains, as a means of laying the foundation for startups and enterprises to build \napplications and services which are tailor-made to the Indian context, and in the process lowering entry \nbarriers for startups and academia while also encouraging international expertise to focus on problems \nin the Indian context.  \nIn areas such as native language NLP (Natural Lang uage Processing) for diverse Indian languages, for \nexample, the funding for creation of these data sets can add incremental value to existing services across \nmany domains, ranging from e-commerce to agricultural advisory. Co-funding by the government would \nalso enable enforcing of standards across development of data sets, and allow interoperability at a large \nscale. A collaborative approach w here the g overnment acts as a catalyst is recommended as a way \nforward.  \nRecommendation 3: Partnerships and collaboration \nAI is a highly collaborative domain, and any framework aimed at promoting AI needs to be aligned \naccordingly. A multi -pronged approach, involving various stakeholders and promoting a collaborative \napproach is required for promoting development of AI tools as well as adoption of AI in different fields of \nactivity. \nPerkmann and Walsh (2007) studied different degrees  of partnership in research and categorised them \nin three categories, each having different characteristics. \nFigure 28: Categories of partnerships in Research \n \nIn India, Academic Entrepreneurship is being taken up through mechanisms such as the development of \ndedicated e-cells in institutions such as IIT Bombay, IIT Madras and IIM Ahmedabad, among others. Low \ndegrees of partnership , such as direct transfer of knowledge , is being taken up by MNCs and startups \nthat make use of scientific publications and informal interaction to develop. Research partnerships are \n\n \nNational Strategy for Artificial Intelligence \n \n76 \n \nalso being explored, but in a very limited capacity. Other partnership mechani sms, however, are all but \nabsent.  \n(h) Collaboration between Research Organi sations is required to promote the ‘AI+X’ paradigm where \nthe AI researcher works in close collaboration with the researcher in other domains like healthcare, \nmanufacturing, agriculture , etc. Several successful US universities have co -located departments \nwhich enabled close collaboration. DARPA in the USA has close linkages with research facilities \nhoused in academic institutions, and has taken up major initiatives for promoting research  and \ninnovation in collaboration with private enterprises and startups alike. In India these domains are not \nnaturally connected, primarily due to the structural nature of both academia and research labs.  To \npromote AI in the country, and fully realise th e ‘AI+X’ paradigm, it is also necessary to enable such \ncross-sectoral collaboration, e.g. between a medical doctor and a computer scientist, to lead research \nfor application of latest technology in solving medical problems and also making real data availab le \nin the process. \n(i) Industry-Research Collaboration  is required to help continuously scale and improve the initial \nresearch output based on user feedback from the market. For example, in the domain of search \nengines, several search datasets like TREC, SMART, etc. were made available by the US universities \nduring the 1970-80s to develop the technology of information retrieval and search. This in turn led to \ndevelopment of several search engines such as Yahoo! and Alta Vista during this period. Over time, \nthis led to improved smarter systems until Google became the market leader in the late 1990s. \n(j) Collaboration with trade bodies and Venture Capitals is essential for successful functioning of a \nprofitable business which involves various other elements , like deriving the optimal business model, \nmanaging human resources, advertisement / marketing and many varied functions depending upon \nthe type of technology and underlying business. Trade bodies and associations are very important \nfor collaborating with other people in similar businesses , as they can share information about the \ncommon problems, possible solutions and access to learning from the ground. Such bodies help in \ncollectively identifying new international markets for such products  / technologies and in \nlobbying/negotiating for favourable trade norms with the national and international bodies.  \nRecommendation 4: Spreading awareness on the advantages AI offers \nAnother major hurdle in adoptio n is low visibility for  the work being done across the count ry. Unless \nknown through networks, work is often duplicated without knowledge of previous work done in the area \nand consultation with experts. There is a need for an AI Database on an online portal for registered people \nto access and find this information. This database, primarily managed by the government, could serve as \na single source of truth for experts and projects being implemented. The resources can include details \nlike researchers stating their expertise , and professionals with hands -on experience on developing AI \nsolutions showcasing their credentials. This knowledge base can also become a forum for sharing various \ndiscussion related to research collaboration, and finding relevant collaborators and finding professionals \nto deliver on AI projects. \nThere is need to make senior level officials among government agencies, public sector undertakings, and \nother domestic firms aware of the numerous advantages that AI offers, by organising workshops and live \ndemonstrations of possible AI applications and how  its implementation can help augment the human \nworkforce, rather than displace it. The constitution of an annual ‘AI Readiness Index’ may also be \nconsidered, as highlighted in the DIPP Task Force report on Artificial Intelligence. This may also be further \nexpanded to highlight best practices across states, to be shared in various fora. NITI Aayog, given its \nmandate of competitive and cooperative federalism, could undertake this task. It has already collaborated \n \nNational Strategy for Artificial Intelligence \n \n77 \n \nwith organisations such as Google to organise capacity building training and workshops for government \nfunctionaries.  \nRecommendation 5: Supporting startups \nStartups and smaller firms are the engine for growth in a dynamic evolving economy like India, and are \nconstrained in the AI space, thus requiring targeted government interventions.   \n(a) Incubation hubs specifically for AI startups in collaboration with State Governments and private sector \nstakeholders need to be set up to provide space, and other infrastructure facilities for new startups \nto incubate along with interacting with other startups at various levels of maturity in order to interact \nand provide advice.  \n(b) Establishment of fund to provide grant funding to startups to facilitate their operation and business. \nThis should be aimed at assisting startups to sustain the initial years of business when they are \nunable to generate venture capital funds or have to sacrifice a large share of the business for early \nstage seed funds.  \nProposed modules of the National AI Marketplace (NAIM) \nIt is proposed t hat the marketplace , called here as National AI Marketplace (NAIM),  be divided into 3 \ndistinct modules, at least in early stages of development. These are as follows:  \na) Data marketplace \nb) Data annotation marketplace \nc) Deployable model marketplace / Solutions marketplace \nThe AI development value chain, discussed above, is supported by these three modular marketplaces in \nterms of easing collaboration, reducing time and cost of collecting & annotating data, and bringing multiple \nsolutions deployment at one place for scale and network effect.  \nFigure 29: Enablers for AI value chain \n \nThe marketplace mechanism being proposed here will be aimed at easing the adoption efforts of all \nparticipants – private enterprises, PSUs, governments, startups and academia. A common platform which \nbrings together enterprises and AI solution builders will trigger off the initial collaboration towards building \nAI solutions and adopting them at scale. \n \n\n \nNational Strategy for Artificial Intelligence \n \n78 \n \nFigure 30: Stakeholders for the proposed NAIM \n \nData marketplace \nToday, incumbents continue to enjoy an oligopoly in building sustainable business models i n AI for two \nmain reasons: (a) t hey can successfully buy data in the informal market setting due to availability of \nresources and reach to negotiate one -time contracts continually , and (b) they have speciali sed \ndepartments to work on different facets of the development value chain. In the process of informal data \nacquisition as well, there is no proper price discovery mechanism in place. Compounded with the issue \nof a huge asymmetry of information in the sector between data providers and buyers, data providers are \nlikely to be substantially underpaid.  \nThis further compounds the problem of data access and thwarts the creation of a healthy marketplace as \nwell as incentives. In a formal marketplace being proposed here, new entrants in the AI model creation / \ntraining space will find it easier to raise resources for buying data. This will be due to VCs having enough \ninformation to verify the funding demands and a lower risk resulting from a more level playing field and \nequitable access to new data sources. \nWhile data has been compared to oil ad nauseam, there are a few differences which must be highlighted: \na) Reusability: Copied data is as valuable as the ‘original ’. Therefore, traceability of ownership is a \nchallenge for a viable marketplace model where the originators of the data should get compensated \nfrom resale of the data. \nb) Compounding value : Unlike oil, the value of a dataset increases not just with quantity but also \nstructural complexity.  For example, the value of a health dataset of a million individuals and an \neducation dataset of the same individuals is more than the sum of its parts because of the value \nadded by the connections between the different elements. Therefore, it is difficult to quantify the value \nof new datasets as it is dependent on the existing datasets one has and how one plans to connect \nthe new data with the existing data. \n\n \nNational Strategy for Artificial Intelligence \n \n79 \n \n \nc) Permanence: Unlike the oil, the inventory of data continually increases with time. However, this raises \nthe complexity in transporting larger and larger datasets. \n \nd) Ubiquity: Since data is ubiquitous and continually increasing in quantity, it follows that a way to \nautomatically curate new datasets is needed so that any market does not overwhelm users wi th \noutdated or irrelevant data. \n \nThe fundamental properties of data and its collection outlined above create challenges which make the \ncreation of a centrali sed data marketplace difficult. The primary assumption of a data marketplace in \noperation is that data custodians will share the data on the marketplace. Organisations may be reluctant \nto do so. Further, the operations of the data exchange, financially sustained by transaction fees and rigid \npricing models, also add to inefficiencies and discourage data sharing.  \nFeatures of a viable data marketplace – how it can be implemented? \nTo incentivise a larger supply of AI training data sets and services, it is necessary to ensure availability \nof data, an audit trail mechanism to curb reselling of the same data, and ways to address security and \nprivacy concerns. One solution is to have a centrali sed, trusted party to host the data on behalf of data \nproviders and enforce the rules of the game. However, this is not practical for a variety of reasons, not \nthe least because most data providers will not agree to let a replicable resource such as data be stor ed \nsomewhere else, but also because of cost, control and trust concerns. \nA more effective way to address these concerns is a decentrali sed data marketplace that is based on \nblockchain technology.  \nThe exchange platform should have the following features for data providers to share data: \na) Traceability, \nb) Access Controls,  \nc) Compliance with local and international regulations, and \nd) Robust price discovery mechanism for data \n \nThe proposed data exchange marketplace will attract data providers and model builders / trainers to build \nAI products. The process of exchange, with enforced provisions of privacy and anonymi sation, brings a \nmarket determined value to data and thus forces the existing informal data exchange economy, without \nany privacy protection, to move towards a formal economy.  \nThe g overnment can establish a committee of experts, researchers, AI developers and regulators to \ncreate the standards the data marketplace will adhere by and explore how can it be put in implementation.  \nThe unique approach that one such initiative, i.e., Ocean Protocol has taken is to create what they call a \nProofed Curation Market with two subcomponents: (a) Data exchange validation on blockchain, (b) Data \ncuration market based on a derivative token. The former lays out a mechanis m for data and value \nexchange, the latter enables the market to decide which datasets are important and creates an incentive \nfor even free data to be put on the platform. Essentially, data consumers can bet on the viability of even \nfree data and thereby earn money. \nSee Appendix III for more on data ecosystems as enablers. \nData annotation marketplace \nOf the three marketplace modules defined, the data annotation marketplace is perhaps the most mature.  \n \nNational Strategy for Artificial Intelligence \n \n80 \n \nGiven the large costs associated with the annotation exercise, a popular mechanism that has emerged \nis that of crowdsourcing using un -trained or non-expert anonymous annotators on the internet. Though \nthis gives rise to its own set of challenges, it may be considered a solution to the problem of cheaply and \nquickly acquiring annotations for the purposes of constructing a variety of models. To understand the \npotential of crowdsourcing, consider the observation of a research paper in 2004 38: a crowd of 5,000 \npeople playing an appropriately designed computer game  24 hours a day, could be made to label all \nimages on Google (425,000,000 images in 2005) in a matter of just 31 days.  \nHowever, some challenges that emerge in the annotation crowdsourcing process are as follows.  \na) Creation of verification mechanisms: Dependence on untrained professionals gives rise to issues in \nensuring high accuracy in annotation and thus potentially higher accuracy of the trained algorithm. \nVerification mechanisms such as multiple annotation, verification of samples by trained professionals, \netc. are some of the ways to solve this problem. \n \nb) Establishing verified traceability: Mechanisms must thus be put in place to identify poor annotators, \nand create mechanisms to reduce the effect of error generated. \n \nFeatures of a data annotation marketplace – how it can be implemented? \nPlatforms such as Amazon’s MTurks, Figure -Eight (previously known as CrowdFlower) or ClickWorker \nare perhaps the most popular crowd -sourced annotation platforms. Each allows for building of \nmechanisms of verification such as multiple annotations, creation of test cases, sample verification by \nprofessionals, etc. to increase accuracy of annotations.  \nAnalysis indicates that most annotation tasks can be categori sed on the basis of sensitivity to expertise \nof annotators. This is defined as the degree to which annotator expertise can affect the accuracy of \nannotation. The figure below considers how approaches to the annotation process could differ with \nchanging nature of the task.  \nFigure 31: Approaches to annotation \nSensitivity to \ndegree of expertise \nExample of task \n \nMotivation \n \nPotential \nannotators \nVerification mechanism \n \nLow \nClassification of \nobject in \npresented image \nGWAP, ML, \nALTR \nGeneral \npopulation Multiple annotations \nHigh Classification of \nmedical scans ML College students, \nprofessionals,  \nMultiple annotations, \nVerification by professionals \nGWAP = Games with a purpose, ML = Mechanised labor for financial incentives, ALTR = Altruistic motivation \nSimple object classification or regional language translation are examples of tasks that have low barriers \nto annotator expertise. However, tasks such as classification of medical images, requires robust \nmechanisms to verify annotations, and also ensure verified traceability to identify poor annotators, and \ncreate mechanisms to reduce the effect of error generated.  \nIn India, vast public datasets are available at organi sations such as ISRO (India Space Research \nOrganisation), ICAR (Indian Council of Agricul tural Research), All India Radio (AIR) and NIC (National \n                                                      \n38 Luis von Ahn and Laura Dabbish, 2004, Labeling images with a computer game \n \nNational Strategy for Artificial Intelligence \n \n81 \n \nInformatics Center) among others, but are unfortunately largely unused due to lack of proper annotation, \nprohibitive cost associated with the process, and  lack of proper mechanisms for acquisition. I t is \nenvisaged that these  datasets be used to  create initial corpora of data sets, and encourage enterprise \naction in the area. A crowdsourced platform for annotation, on the lines of the highly popular government’s \ncitizen participation platform myGov.in, can be established by the government (to be operated by a private \nplayer) where seed data for annotation can be provided by the organisations mentioned above.  \nDeployable Model Marketplace \nGiven the level of market for AI tools, AI -as-a-Service business model, and a plethora of standardi sed \nproblems faced by enterprises at scale, a deployable model segment in the marketplace, which brings \ntogether the buyers and suppliers of AI solutions, can greatly ease and expedite the adoption of AI. It \nwould give vis ibility to the existing solutions, address information asymmetry and generate awareness \namongst the relevant constituents. This AI -as-a-Service model segment would further build upon and \nsynergise with the previous two layers of the marketplace model viz. data and annotation. \nMany objectives aimed to be achieved by AI algorithms can now be categori sed as standardi sed \nproblems. The following are some of the standardi sed and ubiquitous  problems most of organi sations \nface in their activity streams: \na) object detection in images or video streams; \nb) conversational smart chatbots (text and speech); \nc) speech-to-text and text-to-speech; \nd) assistive diagnostic solutions; \ne) language recognition and transcription; \nf) contextual data mining to discover complex patterns; \ng) price optimisation; \nh) data collection, curation and annotation for specific business use; \ni) supply and demand forecasting; and  \nj) server, app and web uptime/downtime prediction. \n \nHere models refer to the weights of the AI model trained on the data, which contains the predictive power. \nThe inferencing of the model will require integration with other IT stacks and will be a customised process. \nHence, this module of the marketplace should focus on defined deployable models that can be bought or \nsold. These models could either be bought and sold as data, or more likely, as a service or an API layer \nthat can be charged per use. \nThe model marketplace will attract enterprises which can post problems which they are seeking to be \nsolved using AI tools. Governments can be another party  to announce specific problems which they \nintend to be solved. This demand side will be supported by the community of AI solution developers \n(consisting of startups, enterprises, individual researchers etc.) who will either showcase their existing \ndeveloped and tested product suites, or customise their products as per the demand placed by \nenterprises. Academic researchers will be another major beneficiary of the marketplace as they will get \nvaluable insights about problems faced by the industry and will get access to real industry data.  \n \n \n \n \n \nNational Strategy for Artificial Intelligence \n \n82 \n \nFigure 32: Stakeholders of NAIM \n \nChallenges to adoption of a marketplace model for AI models are as follows:  \na) Ensuring quality : Marketplaces require continuous monitoring and maintenance of quality. For \nexample, Amazon enforces this on vendors, and this goes beyond user ratings. Continuous scrutiny \nof products, sellers and buyers will be required. \nb) Responsive product development required: The correct set of features of this platform will have to be \ndiscovered through iteration and adaptation, since there isn’t precedent of such AI platforms to draw \nfrom. The platform should allow for trial based access to models to allow for iterative product \nevaluation by buyers. \nc) Sophistication of PSUs for data sharing and evaluation against specs: If data is to be provided over \nthe platform, solution -seekers will need to know how to provide clean datasets, balance data to \nsufficiently represent infrequent occurrences, hide test data for evaluation of solutions while sharing \ntraining data, define appropriate specs, decide on appropriate evaluation methods, etc. The platform \nshould be able to prescribe some basic standards for data sharing which will allow non -ML savvy \nPSUs to post problems and data. \nd) Incentives for seeding network effect : The marketplace needs a certain critical mass of solution \nseekers and providers for gaining traction, to subsequently trigger network effects. This is true of all \nmarketplaces. Incentives to attract both seekers and providers initially will need to be fi gured out. \nPSUs and government agencies may act as initial seeders/users of the platform with right incentives \nand mandates.  \ne) Potentially rich feature set needed for adoption: Such platforms can work well when there are systems \nlike a credible rating syste m, payment escrow, search and sort to aid discovery, recommendation \nengines, etc. These will need to be established at NAIM also.  \nHow can the stakeholders engage in NAIM? \nNAIM, as a marketplace for enterprises, will encourage all business entities, govern ment agencies, \nstartups, AI researchers, system integrators and academic research institutions to sign up on the \nplatform. Following may look like a snapshot of the marketplace where all the stakeholders engage in \ntheir respective activities on the platform: \n\n \nNational Strategy for Artificial Intelligence \n \n83 \n \nPrivate / public sector enterprises \na) Hospitals: assistive diagnostics for physicians and radiologists. \nb) Distribution companies: predict peak load, identify default prone customers . \nc) Travel and tourism: predict peak traffic routes, pricing optimi sation cust omers, offer alternative \nroutes/modes for traffic optimization. \nd) Large industrial or real estate establishments: smart power management, utilities management, real-\ntime failure detection and analysis. \ne) Cinema: automated subtitles in regional languages (speech-to-text and translation). \nGovernment authorities \na) Traffic authorities and police: real -time object detection in CCTV video feeds, face detection and \nobject tracking, number plate detection. \nb) Citizen engagement: conversational chatbots (speech and text) in Indic languages for grievance \nredressal and query management. \nc) Document analysis and management: text recognition and sentiment analysis from physical \ndocuments, analysis and summary. \nd) All India Radio and Prasar Bharti: speech-to-text services for transcription and analysis. \ne) Road and rail transport: face and eye movement detection (at the edge) for analysing driver fatigue \nand alarm signal, freight route optimization. \nf) Agriculture: crop health and soil health analysis through remote sensing and IoT, farmer advisory \nservices for input control and market demand prediction. \nAI Solution Providers  \na. India AI startups: solutions for natural language processing and Indic languages, diagnostic \nassistants, image recognition/object detection. \nb. Industrial Internet-of-Things: capturing data from machines and providing insights in areas of energy \nmanagement, retail and inventory management. \nc. On Edge AI devices: CCTV video feed monitoring and warning signals. \nd. Academic Research Institutions. \ne. Centers of Excellence: access to industry data for various use cases, model training and evaluation. \nBox 19: Why the focus on PSUs \nPrivate enterprises run on the basis of market conditions and business drivers of revenues and profits. \nDue to either absence of markets or failure of efficient markets, many sectors of operation of PSUs are \nfor achieving socio-economic goals of equality and distribution of wealth. There is not much incentive for \nPSUs for improving operational efficiency; they are further constrained by regulations imposed on publicly \nfunded entities in activities of procurement, manpower hiring etc.  \nTechnology adoption, especially AI, can significantly improve operational efficiency and drive down \noperational costs incurred by PSUs operating in financially unsustainable but socially critical areas. This \n \nNational Strategy for Artificial Intelligence \n \n84 \n \nimprovement in operational costs will make them financially more viable. The NAIM allows the public -\nsector firms to directly scout for solutions which they can use in their business operations by significantly \neasing their information search for solutions. Additionally, PSUs and government agencies can also be \nthe first driver of the network of the NAIM where initial pool of problems and industry data is shared by \nthem which will attract several solution developers, especially startups and researchers to sign up for the \nplatform. \nMechanisms to realise marketplace model \nIt is proposed that a decentralised data marketplace based on distributed ledger technology be set up. In \nthe future, such a platform can enable advanced privacy preserv ing AI techniques such as multi -party \ncomputation directly on individual pieces of data. \nInstead of attempting to build such marketplaces, which it is not equipped to do  so, the government \nshould come out with enabling regulations, such that these marketplaces can be set up by private players. \nThese would include regulations and standard permissions for any personal data being sold, standards \nfor anonymi sation, standards for ensuring annotation accuracy, an d cybersecurity standards for the \nmodule dealing with deployable models.  \nAn ini tial effort may be made by the g overnment to build the NAIM platform where all relevant \nstakeholders can sign up. However, open market competition will allow other marketplace operators to \ncome in with innovative services and up-to-date data on suppliers and buyers on the marketplace. NAIM \nwill lead to a more engaging collaboration, quicker and easier data access and accelerated adoption of \nAI among enterprises and public authorities in the country.   \n \nNational Strategy for Artificial Intelligence \n \n85 \n \n  \nEthics, Privacy, Security and Artificial \nIntelligence \n Towards a “Responsible AI”  \n \nAI is going to be the tipping point in technological evolution of mankind , with human dependence on \nmachines and algorithms for decision making never been such deep. Thus , any strategy document on \npromoting AI, necessarily needs to be conscious of the probable factors of the AI ecosystem that may \nundermine ethical conduct, impin ge on one’s privacy and undermine the security protocol. Appropriate \nsteps to mitigate these risks need to be an integral part of any such strategy.  \nWhile the issue of ethics would concern the biases that an AI system can propagate, the privacy concerns \nare largely on collection and inappropriate use of data for personal discrimination. Issue of security arises \nfrom the implications and the consequent accountability of any AI system. \nWhile addressing the above issues, one needs to be conscious of the poten tial vulnerabilities of our \nextant regulatory and societal structures which are dependent on human judgment and control, and thus \nsubject to inherent biases and discrimination. Thus , to say that extant decision making systems – \nindividual, societal, regula tory or even judicial – are entirely devoid of these shortcomings would be a \nfallacy as these are dependent upon human limitations of knowledge, precedent, rationale and bias \n(explicit or subconscious). Delegation of some aspects of that decision making to algorithms, which may \nwell be able to ingest and process many more parameters as compared to a human, may likely result in \nsystems with reduced bias, discrimination and improved privacy protection. However, even if a \ntechnological intervention helps us delegate that responsibility to an algorithm with improved outcomes, \nit is extremely important that we set much higher standards for privacy and protection in case of AI tools. \nEthics and AI \nFairness / tackling the biases AI \nBased on the premise that a large set of well-diversified data may be an accurate description of the world, \nmost of the developer community takes a technocratic attitude that data -driven decision making is good \nand algorithms are neutral. However, this argument does not recogni se the fact that the existing data \nmay have biases, which may have got reinforced over time. The issue of fairness is at the forefront of \ndiscussion in academic, research and policy fora, and definitely merits a combined dialogue and \nsustained research to come to an acceptable resolution. One possible way to approach this would be to \nidentify the in-built biases and assess their impact, and in turn find ways to reduce the bias. This reactive \napproach, use-case based, may help till the time we find techniques to bring neutrality to data feeding AI \nsolutions, or build AI solutions that ensure neutrality despite inherent biases.  \nTransparency / opening the “Black Box” \nPresently, most AI solutions suffer from what is commonly known as the “Black Box Phenomenon”, with \nvery little or no understanding of what happens in between and only the input data and results being the \nknown factors. This is due to the reliance in most current AI systems to incrementally improve the \nperformance as defined by a narrow set of parameters, with developer’s emphasis being less on how the \n \nNational Strategy for Artificial Intelligence \n \n86 \n \nalgorithms are achieving the requisite success. However, calls for explaining the decision -making \nprocess will gain momentum as AI systems are increasingly relied upon for decision making that has \nsignificant consequences for a large section of population.  \nOpening the Black Box, assuming it is possible and useful at this stage (there is considerable debate on \nthat as well), should not aim towards opening of code or technical disclosure – few clients of AI solutions \nwould be sophisticated AI experts - but should rather aim at “ explainability”. With extended disclosure \nthough, what needs to be balanced is whether the algorithm’s parameter may induce the individuals and \ncompanies to change their behavior and in turn game the system. Clearly, more collaborative research \nis required in this area.  \nBox 19: Decoding Explainable AI \nExplainable Artificial Intelligence (XAI) is an evolving area of research which has received a lot of attention \nfrom the research community and  the broader society alike. XAI project by DARPA is one such project \nwhich has shown substantial progress in explaining how and why machine learning algorithms work in a  \ncertain way. The aim of the XAI project, as described by DARPA is: \nThe Explainable AI (XAI) program aims to create a suite of machine learning techniques that:  \n• Produce more explainable models, while maintaining a high level of learning performance; and \n• Enable human users to understand, appropriately trust, and effectively manage the eme rging \ngeneration of artificially intelligent partners. \nThe machine learning algorithms of tomorrow should have the built -in capability to explain their logic, \nenumerate their strengths and weaknesses and specify an understanding of their future behavior.  \nFigure 33: XAI Concept \n \nXAI is one of a handful of current DARPA programs expected to enable “third -wave AI systems”, where \nmachines understand the context and environment in which they operate, and over time build underlying \nexplanatory models that allow them to characterise real world phenomena.  \nIn May 2018, XAI researchers are expected to demonstrate initial implementations of their explainable \nlearning systems and conduct pilot studies of their Phase 1 evaluations. Full Phase 1 system evaluations \nare expected in November 2018. \nCredit: DARPA Explainable Artificial Intelligence (XAI) \n\n \nNational Strategy for Artificial Intelligence \n \n87 \n \nPrivacy and AI \nAI models, solutions and their application depend on generation, collection and processing of large \namounts of data on individual, entity and community behavi our. Data collection without proper consent, \nprivacy of personal data, inherent selection biases and resultant risk of profiling and discrimination, and \nnon-transparent nature of AI solutions are some of the issues requiring deliberation and proper recourse.  \nHowever, the current debate on data usage have two distinct aspects. Firstly, there are concerns that \ncompanies are harvesting significant amounts of consumer data and using it inappropriately to gain \ninsights about consumers. Key here is that the cons umer may not have access to these insights or the \nability to derive value from them. Beyond compliance, companies can consider how to create awareness \nof how they use consumer information and the value they provide in return, which can build trust in their  \nbrand and services. \nSecondly, there are concerns that companies are amassing large data sets and thereby building an unfair \ncompetitive advantage. Datasets themselves have little intrinsic value without the ability to extract \nmeaning from them. The datase t is a necessary, but not the only, component of delivering meaningful \ninsights from data. Having the tools to analyse it and the experience to understand its meaning are the \nothers. While those that have access to large datasets and by the nature of their  business models have \ndata network effects, which enables them in turn to build a first mover advantage when it comes to \nperfecting their algorithms and driving business value, this does not necessarily negatively impact the \nconsumer. \nDealing with privacy issues \na. Establish a data protection framework with legal backing : The work being done by Justice \nSrikrishna Committee on data protection law is very opportune and timely. The 7-core principles \nof data protection and privacy – informed consent, technology agnosticism, data controller \naccountability, data minimisation, holistic application, deterrent penalties and structured \nenforcement – are quite comprehensive and should provide a strong privacy protection regime \nin the country once enacted.  \nb. Establish sectoral regulatory frameworks: Apart from having a central privacy protection law, due \nto diverse and fast changing nature of the technology, sectoral regulatory frameworks may also \nact as additional protection to user privacy a nd security. Japan and Germany have developed \nnew frameworks applicable to specific AI issues such as regulating next generation robots and \nself-driving cars respectively.  \nc. Benchmark national data protection and privacy laws with international standards : European \nUnion’s General Data Protection Regulation (GDPR) guidelines, which have been enforced in \nMay 2018, encourage design of less -privacy invasive systems. French laws give a right to \nexplanation for administrative algorithmic decisions, making it much more comprehensive than \nGDPR on administrative decisions. India’s privacy protection regime will have to be continually \nupdated to reflect understanding of new risks and their impact.  \nd. Encourage AI developers to adhere to international standards : Leaders and practitioners from \nacross the world have come together to frame standards for safe and privacy preserving AI. The \nGlobal Initiative on Ethics of Autonomous and Intelligent Systems of the IEEE has a chapter on \n‘Personal Data and Individual Access Control in Ethically Aligned Design’. Indian enterprises and \ndevelopers need to build these standards into AI design itself. \ne. Encourage self-regulation: Data Privacy Impact Assessment Tools can be used by AI developers \nand enterprises adopting AI solutions to manage privacy risks.  \n \nNational Strategy for Artificial Intelligence \n \n88 \n \nf. Invest and collaborate in privacy preserving AI research : New mathematical models for \npreserving privacy are being researched upon where risks of data exploitation and personal \nidentification (from an anonymised dataset) can be reduced by limiting information one can gain \nfrom released data, irrespective of amount of side information available otherwise. India should \ncollaborate on areas of research like Differential Privacy, Privacy by Design, Safety -Critical AI \nand Multi-Party Computations which enable protection of privacy despite data sharing at a wide \nscale.  \ng. Spread awareness: Privacy has been termed as a fundamental right by the Supreme Court of \nIndia. The protection of this right with its multiple facets in a fast -changing technolog ical \nenvironment will not just depend on State enforcement b ut by also making the citizens aware of \ntheir rights and how they can protect them. People often unknowingly give consent to sharing \ntheir data which they would not have ordinarily done had they known the purpose their data were \nbeing put to. There is an urgent need to spread awareness among the individuals about the \nimportance of consent, ethics and privacy while dealing with technology. A pan -India campaign \nin multiple languages, and inclusion of privacy rights in school and college curriculum can serve \nas effective mass outreach mediums to spread awareness. \nBox 20: Differential Privacy \nA concept developed by Cynthia Dwork in 2006, Differential Privacy aims at preserving identifiable user \ninformation irrespective of any outside information the aggregator agency holds. The dilemma of giving \npersonalised service to users based on their individuali sed preferences while at the same time ensuring \nthat user is not uniquely identifiable, using the data co llected or any other public data, is not solved by \ntraditional privacy preserving methods of cryptography. Differential privacy describes a promise, made \nby a data holder, or curator, to a data subject: “You will not be affec ted, adversely or otherwise, by  \nallowing your data to be used in any study or analysis, no matter what other studies, data sets, or \ninformation sources, are available.” It addresses the paradox of learning nothing about an individual while \nlearning useful information about a population. \nSecurity in AI \nThe accountability debate on AI, which in most of the cases today is aimed at ascertaining the liability, \nneeds to be shifted to objectively identifying the component that failed and how to prevent that in the \nfuture. An analogy can be drawn to how the airlines have become a relatively safe industry today. Every \naccident has been elaborately investigated, and future course of action has been determined. Something \nsimilar is needed to ensure safe AI. \nOne possible framework that can be mooted involves the following components:  \na. Negligence test for damages caused by AI software, as opposed to strict liability. This involves \nself-regulation by the stakeholders by conducting damage impact a ssessment at every stage of \ndevelopment of an AI model.  \nb. As an extension of the negligence test, safe harbo urs need to be formulated to insulate or limit \nliability so long as appropriate steps to design, test, monitor, and improve the AI product have \nbeen taken. \nc. Framework for apportionment of damages need to be developed so that the involved parties bear \nproportionate liability, rather than joint and several liability, for harm caused by products in which \n \nNational Strategy for Artificial Intelligence \n \n89 \n \nthe AI is embedded, especially where the use of AI was unexpected, prohibited, or inconsistent \nwith permitted use cases. \nd. Actual harm requirements policy may be followed, so that a lawsuit cannot proceed based only \non a speculative damage or a fear of future damages. \nIndia can also take a leaf out of UK’s playbook, where GBP9 million is being invested to establish a new \nCentre for Data Ethics and Innovation, aimed at enabling and ensuring ethical, safe and innovative uses \nof data, including AI. This will include engaging with industry to explore the possibilities of establishing \ndata trusts to facilitate easy and secure sharing of data. A consortium of Ethics Councils at each Centre \nof Excellence may be set up to define the standard practice (on the lines of OpenAI charter). It would be \nexpected that all Centres of Excellence adhere to standard practices while developing AI technology and \nproducts. \n  \n \n \n  \n \nNational Strategy for Artificial Intelligence \n \n91 \n \n \n Actions for the Government \nAchieving the goal of #AIforAll requires long term and engaged institutional collaboration between all \nthe stakeholders including the citizens. However, while playing the primary role in ensuring that this \ncollaborative strategy succeeds, the government needs to be mindful of not crowding out the private \nsector. Role of the government thus needs to be one of a facilitator, an active promoter and wherever \nrequired, of an owner.  \nThis section summarises the key recommendations, and the role of the government. \nFigure 34: Government’s role \nArea Recommendation Government role \nResearch and Application \nCore Research Setting up Centre of \nResearch Excellence for \nAI (COREs) \nIdentify academic institutions, provide fiscal \nsupport to establish COREs focusing on core \ntechnology research in AI. \n PhD Scholarships Institute National AI Fellowships to retain \noutgoing PhD students and attract researchers \nfrom foreign universities with attractive \nincentives and challenging projects. \n Inter-academia \ncollaboration  \nIncentivise research collaboration between \npremier academic institutions through special \ngrants while facilitating the formation of a global \nexpert pool for core AI research. \n Faculty Fellowships Provide Faculty Fellowships or Chairs in \nacademic institutes to promote research in AI. \nApplied Research  Setting up of \nInternational Centres for \nTransformational AI \n(ICTAIs) \nInvite Expression of Interests (EoIs) from \nindustry players to lead ICTAIs in various \nsectors (health, education, agriculture, smart \nmobility and smart cities), in collaboration with \nthe government and academia. Build  \ngovernance structure, provide fiscal support, \nformulate an IP model for ICTAIs and set up the \nICTAIs under a PPP model through “challenge \nmethod”. \n Setting up ICTAI Inc., \noverarching entity for \nICTAIs \nEstablish “ICTAI Inc.” as either society / section \n8 company, with initial contribution from \ngovernment and private sector representation, \nto select and fund ICTAIs. \nCommon Compute \nPlatform \nSetting up AI Research, \nAnalytics and \nknoWledge Assimilation \nplatform (AIRAWAT) \nSet up a common cloud platform for Big Data \nAnalytics and Assimilation with a large, power-\noptimised AI Computing infrastructure \nconnecting all COREs, ICTAIs and other \nacademic institutions with National Knowledge \nNetwork. \nIntellectual Property Building an attractive IP \nregime for AI innovation \nSet up a task force, comprising jointly of Ministry \nof Corporate Affairs and DIPP, to examine and \n \nNational Strategy for Artificial Intelligence \n \n92 \n \nissue appropriate modifications to the IP \nregulatory regime pertaining to AI. \nSupra-national \ncollaboration \nSetting up CERN for AI Take the lead in bringing together the relevant \nparties to create People’s AI, the CERN for AI – \nnational governments, industry, academia and \ninternational community of researchers. \nReskilling and Training \nWorkforce Promote formation of \nfuture service sector \njobs \nIncentivise creation of service sector jobs of the \nfuture such as data annotation through tax \nholidays or inclusion in CSR activities. \n Recognition and \nstandardisation of \ninformal training \nSet up AI / Data Science training standards, as \nper National Skills Qualification Framework, and \nprovide certifications to training institutes. \n Promote employee \nreskilling \nIncentivise investment in training of employees \nthrough tax breaks and grants for employers. \nColleges Expansion of quality \neducation in data \nscience and AI \nIncentivise colleges / universities to adopt \ncredit-bearing MOOCs in their curriculum. \n Promote cross-\ndisciplinary AI education \nIntroduce Bridge Courses in AI for post-\ngraduates in non-computer science or data \nscience domains. \nSchools Introducing AI / ML in \nschools \nIntroduce AI modules in Atal Tinkering Labs. \nOverall Continuously assess the \nchanging nature of jobs \nConstitute a standing committee or taskforce to \nexamine and report on changes in employment \ninduced by adoption AI. \nAccelerating Adoption of AI \nData Sharing Opening up government \ndatasets \nEstablish platforms for making datasets in the \narea of social sector (either collected during \nimplementation of a scheme or in normal \nbusiness processes) available for open public \nuse in a machine readable form.  \nData Annotation Creating and making \nIndia specific annotated \ndatasets public (on the \nlines of ImageNet) \n1. Catalyse partnerships with the various \nacademic institutions and public / \nprivate agencies in making annotated \nIndia specific data available for \nadvancing AI research. \n2. Explore partnerships and co-fund \nbuilding of large corpora of data across \ndomains, as a means of laying the \nfoundation for startups and enterprises \nto build applications and services tailor-\nmade to the Indian context. \nCrowdsourcing \nAnnotation \nAnnotation of data – \nimages, text, speech \netc. via crowdsourcing \nAnnounce grand challenge tasks for tagging of \nimages, text or videos, and devising reward \nbased mechanisms through data market place \nto aggregate the content from the various \nparticipating members. \nNation-wide \nadoption \nEnabling a multi-\nstakeholder owned and \nCreate governance guidelines, explore \npartnerships and co-fund the establishment of: \n1. Data marketplace \n \nNational Strategy for Artificial Intelligence \n \n93 \n \nmanaged National AI \nMarketplace \n2. Data annotation marketplace \n3. Deployable model marketplace \nto develop the data supply ecosystem, ease \ncollaboration, reduce time and cost of collecting \n& annotating data, and bring multiple solutions \ndeployment at one place for scale and network \neffect. \nVisibility in \nCollaboration \nMaking information \nsearch for collaborations \neasier \nSet up an AI Database portal for easy \ndissemination of information on projects being \nimplemented via collaboration among \ngovernment-academia-industry-researchers-\nstartups to enable resource matching. \nAwareness and \nAdoption in \nGovernment \nMaking decision makers \naware about \ntransformative potential \nof AI \n1. Workshops, live demonstrations, \n2. AI Readiness Index to highlight best \npractices across states, and \n3. Create Central-State shared fund for AI \nled development projects to be taken up \nby States. \nGovernment and \nPSUs as seeders for \nnetwork effect \nMaking governments \nand PSUs leaders in \nadoption of social AI \ntools  \nHelp create a pipeline of AI research projects for \nthe COREs, ICTAIs through grand challenges to \nbe given by the government and PSUs. \nIncentivise public agencies to adopt and employ \nAI in delivering service through financial \nsupport; extra budgets for R&D; tax incentives \nand awards. \nPartnerships and \nCollaboration \nIndustry – Academia – \nTrade Bodies – Venture \nCapital Collaboration \nEncourage close collaboration between \nindustry, academia, trade bodies and venture \ncapital to implement “AI+X” paradigm. \nStartup Support Support systems for AI \nbased startups \nEstablish incubation hubs and venture funds \nspecifically for AI startups in collaboration with \nState Governments. \nResponsible AI Development \nEthical and \nResponsible \nResearch in AI \nMaking COREs and \nICTAIs adopt ethical \npractices \nSet up a consortium of Ethics Councils at each \nCORE and ICTAI to define the standard \npractices and monitor their adoption. \nPrivacy and Security Instituting a data privacy \nlegal framework \nAddress and implement data protection \nframework, which protects human rights and \nprivacy without stifling innovation in India. \n Creating sectoral \nregulatory guidelines \nCollaborate with industry to come out with \nsector specific guidelines on privacy, security \nand ethics – on manufacturing, financial \nservices, identity, telecommunication, robotics \netc.  \n Collaborating on privacy \npreserving technology \nresearch in AI \nSupport COREs to do research in new \nmathematical models and technology for \npreserving privacy; encourage international \ncollaboration. \nSustainable \nResearch \nSetting up Centre for \nStudies on \nTechnological \nSustainability (CSTS) \nSet up CSTS to address issues relating to \nethics, privacy, legal aspects, social \nsustainability and global competitiveness of the \ntechnologies developed.  \n \n \nNational Strategy for Artificial Intelligence \n \n94 \n \nFinancial implication of recommendations \nA greater understanding of the financial implications of the recommendations made in this report will be \nrealised after consultations with the different stakeholders. Given the emphasis on research, skilling \nand creation of the ecosystem, the recommendations would involve a significant budgetary allocation \nby the government. This would be in addition to funds being provided as part of the Digital India and \nStartup India initiatives. \n  \n \nNational Strategy for Artificial Intelligence \n \n95 \n \n \n \n \n \n \n \n \n  \n \nNational Strategy for Artificial Intelligence \n \n96 \n \n \n Appendix I: Artificial Intelligence Explained \n An Executive Guide to Artificial Intelligence 39 \nMachine Learning: a definition \nMost recent advances in AI have been achieved by applying machine learning to  very large data sets. \nMachine learning algorithms detect patterns and learn how to make predictions and recommendations \nby processing data and experiences,  rather than by receiving explicit programming instruction. The \nalgorithms also adapt in response to new data and experiences to improve efficacy over time.  \n \n \n  \n                                                      \n39 Replicated from McKinsey Analytics An Executive Guide to AI \n\n \nNational Strategy for Artificial Intelligence \n \n97 \n \nUnderstanding the major types of machine learning \n Supervised Learning Unsupervised Learning Reinforcement Learning \n \n   \nWhat it is? An algorithm uses training \ndata and feedback from \nhumans to learn the \nrelationship of given \ninputs to a given output \n(e.g., how the inputs “time \nof year” and “interest \nrates” predict housing \nprices) \nAn algorithm explores input \ndata without being given an \nexplicit output variable \n(e.g., explores customer \ndemographic data to \nidentify patterns) \nAn algorithm learns to \nperform a task simply by \ntrying to maximise rewards \nit receives for its actions \n(e.g., maximises points it \nreceives for increasing \nreturns of an investment \nportfolio) \nWhen to \nuse it? \nYou know how to classify \nthe input data and the \ntype of behavior you want \nto predict, but you need \nthe algorithm to calculate \nit for you on new data \nYou do not know how to \nclassify the data, and you \nwant the algorithm to find \npatterns and classify the \ndata for you \nYou don’t have a lot of \ntraining data; you cannot \nclearly define the ideal end \nstate; or the only way to \nlearn about the \nenvironment is to interact \nwith it \nHow it \nworks? \n1. A human labels every \nelement of the input \ndata (e.g., in the case \nof predicting housing \nprices, labels the input \ndata as “time of year,” \n“interest rates,” etc.) \nand defines the output \nvariable (e.g., housing \nprices) \n2. The algorithm is \ntrained on the data to \nfind the connection \nbetween the input \nvariables and the \noutput \n3. Once training is \ncomplete – typically \nwhen the algorithm is \nsufficiently accurate – \nthe algorithm is \napplied to new data \n1. The algorithm receives \nunlabeled data (e.g., a \nset of data describing \ncustomer journeys on a \nwebsite) \n2. It infers a structure from \nthe data \n3. The algorithm identifies \ngroups of data that \nexhibit similar behavior \n(e.g., forms clusters of \ncustomers that exhibit \nsimilar buying \nbehaviors) \n1. The algorithm takes an \naction on the \nenvironment (e.g., \nmakes a trade in a \nfinancial portfolio) \n2. It receives a reward if \nthe action brings the \nmachine a step closer \nto maximising the total \nrewards available (e.g., \nthe highest total return \non the portfolio) \n3. The algorithm optimises \nfor the best series of \nactions by correcting \nitself over time \n \n  \n\n \nNational Strategy for Artificial Intelligence \n \n98 \n \nDeep Learning: a definition \nDeep learning is a type of machine learning that can process a wider range of data resources, requires \nless data preprocessing by humans, and can often produce more accurate results t han traditional \nmachine learning approaches. In deep learning, interconnected layers of software -based calculators \nknown as “neurons” form a neural network. The network can ingest vast amounts of input data and \nprocess them through multiple layers that learn increasingly complex features of the data at each layer. \nThe network can then make a determination about the data, learn if its determination is correct, and use \nwhat it has learned to make determinations about new data. For example, o nce it learns what an object \nlooks like, it can recognise the object in a new image. \n \n  \n\n \nNational Strategy for Artificial Intelligence \n \n99 \n \nUnderstanding the major types of deep learning \n Convolutional neural network Recurrent neural network \n \n \n \nWhat it is? A multilayered neural network with a \nspecial architecture designed to extract \nincreasingly complex features of the data \nat each layer to determine the output \nA multilayered neural network that can \nstore information in context nodes, \nallowing it to learn data sequences and \noutput a number or another sequence \nWhen to use \nit? \nWhen you have an unstructured data set \n(e.g., images) and you need to infer \ninformation from it \nWhen you are working with time-series \ndata or sequences (e.g., audio \nrecordings or text) \nHow it \nworks? \nProcessing an image \n1. The convolutional neural network \n(CNN) receives an image – for \nexample, of the letter “A” – that it \nprocesses as a collection of pixels \n2. In the hidden, inner layers of the \nmodel, it identifies unique features, \nfor example, the individual lines that \nmake up “A” \n3. The CNN can now classify a different \nimage as the letter “A” if it finds in it \nthe unique features previously \nidentified as making up the letter \nPredicting the next word in the \nsentence “Are you free …..?” \n1. A recurrent neural network (RNN) \nneuron receives a command that \nindicates the start of a sentence \n2. The neuron receives the word “Are” \nand then outputs a vector of \nnumbers that feeds back into the \nneuron to help it “remember” that it \nreceived “Are” (and that it received it \nfirst). The same process occurs \nwhen it receives “you” and “free,” \nwith the state of the neuron updating \nupon receiving each word \n3. After receiving “free,” the neuron \nassigns a probability to every word \nin the English vocabulary that could \ncomplete the sentence. If trained \nwell, the RNN will assign the word \n“tomorrow” one of the highest \nprobabilities and will choose it to \ncomplete the sentence \n \n  \n\n \nNational Strategy for Artificial Intelligence \n \n100 \n \nAppendix II: Global Country Strategy Review40 \n What is happening around the world in AI?  \n \nCountries around the world are becoming increasingly aware  of the potential economic and social \nbenefits of developing and applying AI. For example, China and U.K. estimate that 26% and 10% of their \nGDPs respectively in 2030 will be sourced from AI -related activities and businesses. There have  been \ntremendous activity concerning AI policy positions and the development of an AI ecosystem in different \ncountries over the last 18 to 24 month – the USA published its AI report in December 2016; France \npublished the AI strategy in January 2017 followed by a detail ed policy document in March 2018; Japan \nreleased a document in March 2017; China published the AI strategy in July 2017; and the U.K. released \nits industrial strategy in November 2017.  \nGovernments are reviewing and developing their position on the following area s to rapidly grow AI \necosystems: \na) Trigger demand in socially relevant sectors / segments \nb) Gear up the supply side to fulfil demand – infrastructure (including data ecosystem, data stacks, \nhigh speed computing, etc.), talent, research \nc) Set up an enabling system—governance, funding, partnerships \nTrigger demand in socially relevant sectors \nDifferent countries have identified different focus areas for AI development and deployment:  \na) the USA: areas of interest include economic prosperity, educational opportuni ties, quality of life, \nnational and homeland security. The USA is focusing on growing the AI ecosystem through public \nspending on contracts, e.g., the US Department of Defens e spent over USD 2.4 billion on AI-\nrelated technology in 2017 (2x increase from 2015). \nb) China: a reas of interest include education, healthcare, energy, transport, quality of life, city \nplanning /IoT / robotics. China is focusing on developing and using AI for delivery of public \nservices through financial support, developing talent pipelin e, and leveraging international \ncooperation. \nc) Japan: areas of interest include industrial productivity improvement, healthcare, medical care \nand welfare, mobility and information security. Japan is focusing on moving from the \"Industry \n4.0\" paradigm to \"Soc iety 5.0\" through the development of AI use cases for delivering public \nservices. \nd) France: areas of interest include healthcare, environment, transport mobility, defence -security. \nThe government is planning to support AI startups through data availability, public spending and \ntalent reskilling. \n                                                      \n40 McKinsey \n \nNational Strategy for Artificial Intelligence \n \n101 \n \ne) U.K.: areas of interest include services, life sciences, agriculture and public -sector applications. \nThe government is focused on growing innovative tech firms and making deals with the private \nsector to solve AI use-cases for delivery of public services.  \nGear up the supply side to fulfil demand \nInfrastructure  \nMost governments have taken the following two action steps, in varying degrees of engagement, to \nupgrade infrastructure and build a data ecosystem:  \na) Creation of a data-solutions marketplace \nb) Invest in upgrading computing infrastructure, 5G networks, etc.  \nMany countries have been reviewing a range of initiatives to facilitate creation of these marketplaces and \nupgrading computing infrastructure as well as connectivity. For example: \na) U.K. is exploring the feasibility of creating data trusts where the process of data sharing and \nstorage is underwritten by the government. There is also a focus on defining data rights for \npotential participants on the platform . There are plans to invest GBP 1 billion to upgrade digital \ninfrastructure including rolling out 5G and full-fibre networks.  \nb) Japan has announced expanding its R&D tax exemption to include AI and big data as well as \nsubsidies for building new robots with int egrated AI. Its focus is on developing sector -specific \nplatforms for public and private development of AI, followed by interlinking of different platforms \nto create an integrated AI ecosystem.  \nc) France is trying to streamline its innovation track with \"inno vation sandboxes\" which would \nprovide an open platform for innovation and offer resources for use in field -testing, etc. AI \nresearch institutes would have supercomputers specifically designed for AI usage and devoted \nto researchers and their economic partners during their shared initiatives.  \nd) China is focused on developing open source innovation platforms in partnership with private \nplayers like Baidu, Alibaba, and Tencent. Funding is available for 5G networks to enable \n\"intelligentisation\" and deployment o f supercomputers, high performance semiconductor chips \nfor AI use.   \ne) the USA is facilitating the creation of open source software libraries and toolkits, e.g., Open NLP, \nWeka toolkit, etc. \nTalent  \nCountries are also significantly increasing allocation of resources for STEM talent development through \ninvestment in universiti es, mandating new courses (e.g.  AI and law), and offering schemes to retrain \npeople.  \na) U.K. is planning to increase its R&D spend to 2.7% of its GDP by 2027, investing GBP42 million \nin teacher development and GBP64 million  in the retraining scheme including digital training. \nThey are planning to make it easier for Tier -1 applicants working on AI subjects to obtain work \npermits and h ave a path to residency. They are also planning to build over 1,000 government \nsupported PhD institutions by 2025 and set up a Turing fellowship to support an initial cohort of \nAI fellows.  \n \nNational Strategy for Artificial Intelligence \n \n102 \n \nb) France is trying to triple the number of AI graduates in three yea rs by offering new courses and \ndoubling the starting salary of researchers in public institutions. They also want to attract talent \nfrom across the world by offering substantial salary hikes, supporting improvements in the quality \nof life and reducing administrative formalities. Further, France and the US have labs and panels \nto assess the impact of AI on the workforce.  \nc) U.S. is p lanning a USD 200 million grant for STEM education focusing on co mputer science \nmatched by a USD300 million industry grant.  \nd) Japan has convened a “ national consultative body ” with 3 universities and the Japan business \nfoundation to develop education programs for reskilling.  \ne) China has launched a five -year university program to train at least 500 teachers and 5,000 \nstudents working on AI technologies. The program is a collaboration between government \nbodies, private companies and universities including Sinovation Ventures. Chin a has set in \nmotion a plan to develop 50 world-class teaching institutes and research institutions, 50 national-\nlevel high quality online open courses and 50 AI faculties by 2020 as part of the “AI+X” program. \nResearch \nUniversities and research institution s from the US A, China and Japan have led the publication volume \non AI research topics between 2010 and 2016. In the US, Carnegie Mellon University (CMU), \nMassachusetts Institute of Technology  (MIT) and Stanford are the top three universities in mean count \nof papers published across AI, systems, theory and interdisciplinary areas 41. These universities have \nbeen pioneers of AI research in the US and have more than 100 faculty members across different areas \nof AI research.  \na) CMU has one of the oldest AI progra ms in the world – it was also one of the first to offer an \nundergraduate program. It has started the CMU AI program which is a collaboration forum for \nfaculties across seven departments to work on multidisciplinary AI topics.  \nb) MIT has launched the Intelligenc e Quest to discover the foundations of human intelligence and \nits application to develop technology and tools.  \nc) Stanford has a AI4ALL program to increase diversity in AI research and education.  \nThese three universities also feature on the top of any list o n infrastructure and industry relations, e.g., \nIBM’s Watson was developed in research collaboration with CMU: \n                                                      \n41CSRankings.org \n \nNational Strategy for Artificial Intelligence \n \n103 \n \n \nSource: Press search \nChinese universities have also established extensive research partnerships with Baidu, Alibaba and \nTencent: \na) Baidu has announced a USD104 million partnership with the Peking University to further research \nin AI-related topics including information science and medicine.  \nb) An AI centre for law and legal issues was also unveiled at the Peking University to research on \napplications to improve legal efficiency while decreasing burden on judges.  \nc) Alibaba’s Aliyun is working with the National Engineering laboratory on big data systems and \nsoftware at the Tsinghua University.  \nJapanese research has historically been hardware centric  with robotics as one of the major focus areas \nof development. With the growing demand for AI, academia in Japan is rapidly reorienting itself to \ntheoretical and applied research in the sector . Riken Centre for Advanced Intelligence Project and \nAdvanced Industrial Science and Technology are the nodal research institutes for industry collaboration.  \nWith AI led disruption of multiple industries, it is imperative for traditional industry players to increase \nspeed and agility of insight generation, design and digitise customer journeys as well as develop \nefficiency in delivering journey transformations. Consequently, traditional players like GE and Merck are \ninvesting heavily to power their offering through AI interventions. \n▪ 80+ faculty across 17 \nresearch focus areas \n▪ Andrew Ng is an adjunct \nassociate professor for \nMachine learning\n▪ AI4ALL programme to \nincrease diversity in the field \nof AI research\n▪ 100+ faculty members \nacross departments\n▪ 1000+ students engaged in \nAI research and education\n▪ Launched undergrad degree \nin AI\n▪ IBM’s Watson, World \nchampion robot soccer \nplayers, etc.\n▪ CS & AI lab has 1200+ \npeople working on 900+ \nprojects\n▪ Research ranges from \nalgorithm and theory to \napplied AI and machine \nlearning\n▪ Launched MIT Intelligence \nquest to discover the \nfoundations of human \nintelligence and develop tools\n▪ JD.com\n▪ Google\n▪ DiDi\n▪ Panasonic\n▪ UST Global\n▪ Tencent\n▪ Samsung\n▪ $240 million investment from \nIBM for 10 years\n▪ 60+ member companies \nacross telcos, finance, \nconsumer electronics\n▪ U.S. department of defence\n▪ NASA\n▪ Amazon\n▪ Google\n▪ Microsoft\n▪ Oculus\nResearch and \ncourse-work\n▪ Stanford AI lab – Toyota \ncenter for AI research\n▪ Computational genomics lab\n▪ Distributed robotics lab\n▪ Vision lab\n▪ State of the art robotics \ninstitute\n▪ AI stack \n▪ Design for AI lab\nResearch \nfacilities\nResearch \npartnerships\n \nNational Strategy for Artificial Intelligence \n \n104 \n \n \nSource: Press search \nSet up an enabling system \nGovernance  \nMost of the governments have established  / utilised existing centrali sed umbrella body for budgetary \nplanning of AI interventions and for formulating strategy and drafting policies. The National Science and \nTechnology Council in the USA, Strategic Council for AI technologies in Japan, AI council in the U.K. are \nnodal agencies for planning and designing AI initiatives. These central bodies typically consist of \nministers, representatives from industry and nominated members from acade mia, e.g., the UAE has a \nMinister of State for AI. Similarly, for implementation and delivery of AI initiatives:  \na) U.K. has a dedicated department \"Office of AI\" to collaborate with multiple departments, \nministries and other stakeholders to deliver AI projects \nb) France has a shared specialist centre of 30 members to help provide specific inputs and \nimplement projects in other departments  \nc) In China and Japan, individual ministries and departments are responsible for implementing AI \nsolutions across different sectors. For example, \ni. China: National Development and Research Commission, Ministry of Science and \nTechnology, Ministry of Industry and Information Technology, Central Military -Civil Fusion \nDevelopment Commission Office, the Central Military Commission (CMC) Science and \nTechnology Commission, and the CMC Equipment Development Department, etc.  \nii. Japan: Ministry of Health, Labour and Welfare, Ministry of Land, Infrastructure, Transport \nand Tourism, and the Ministry of Agriculture, Forestry and Fisheries, etc. \n \n▪ Provides AI based face recognition software \nand robotics, e.g. KoalaCam\n▪ Developed an autopilot system allowing \nautomatic emergency breaking and traffic-\naware cruise control\n▪ Optimizing vaccine \nyield using ML on \nhuge quantities of \nmanufacturing data\n▪ Uses Al-driven \npredictive \nmaintenance tools in \nmining activities to \nhalve operations and \nmaintenance costs\n▪ Use Al and ML to \npredict ATV and to \ngenerate \npersonalized \nrecommendations\n▪ Tesla DC accelerator \n▪ Deep Learning SDK \n▪ Tegra mobile \nprocessor \n▪ DL supercomputer \nDGX-I\n▪ Watson, its Developer \nCloud Platform and \nWatson based APIs \n▪ TrueNorth: chip \nstructure for neural \nnetworks testing\n▪ Offers predictive \nanalytics on buying \npotential and B2B \npricing to current \nservice users through \nEinstein\n▪ Amelia: NLP based \nintelligent customer \nservices\n▪ Offers ML based healthcare big data platform \nfor personalized health analysis and prediction\n▪ Intelligent employee \nmanagement, e.g. \ncity-wise intelligent \nscheduling \nFace/voice \nrecognition for cash \ndeposit, customer \nprofiling\nMake existing \noperations \nmore efficient\nOffer new \nservices and \nproducts to \nexisting \ncustomer\nDevelop \ndisruptive \nbusiness \nmodels\n \nNational Strategy for Artificial Intelligence \n \n105 \n \nWhile due importance is given to the central planning and implementation entities, the role of local \ngovernments in solving area -specific challenges through application of AI is becoming increasingly \nimportant. For example,  \na) London has a Smart City boar d and a Chief Digital Officer to apply best practices in smart \ninfrastructure and AI. \nb) Over 19 Chinese cities, including Beijing, Shanghai, Hangzhou, Zhejiang, Tianjin, have been \nmandated to develop their own city-level AI agenda.  \nUtilising AI-related solutions at the grass -roots level to solve real local challenges has the potential to \ntruly democratise its use.  \nFunding  \nGovernments are significantly increasing the funding for the AI ecosystem.  \na) Apart from the increase in R&D spend to 2.7 % of its GDP, U.K. has created a GBP 725 million \nindustrial strategy challenge fund, GBP 1.7 billion for transforming cities fund, reforming \nenterprise investment scheme and ventu re capital trusts to unlock GBP 7 billion over 10 years. \nAdditionally, they have instituted a GBP 2.5 billion investment fund at the British Business Bank \nto incubate tech startups and reforming rules for pension funds to mandate inclusion of AI in their \ninvestment portfolio. \nb) Japan is planning to increase its science and innovation budget by JPY900 billion by 2020 for AI. \nDifferent Japanese ministries are also funding R&D centres, e.g., Ministry of Economy, Trade \nand Industry (METI) is funding R&D centres at the National Institute of Advanced Industrial \nScience and Technology (AIST).  \nc) France is planning to spend EUR1.3 billion to develop AI-led interventions.  \nd) China is funding a massive growth in network infrastructure and creating megaprojects.  \ne) the USA is increasing its spend on AI -related contracts with the Departmen t of Defense alone \nspending USD2.4 billion. Other large spenders include departments of agriculture, veteran affairs \nand homeland security. \nPrivate companies have dominated the investment in AI with internal corporate investments as the \nbiggest mode of spend. Other large sources of funding for startups within the sector include venture \ncapital (VC) and private equity (PE). The two biggest areas funded are machine learning and computer \nvision, followed by natural language processing, autonomous vehicles and smart robotics.  \n \nNational Strategy for Artificial Intelligence \n \n106 \n \n \nSource: McKinsey Global Institute report on ‘Artificial Intelligence and the next digital frontier’ \nDigital native companies in the US A and China are exploring new areas for the development and \napplication of AI to solve customer -centric use -cases. These range fro m developing smart AI game \nsystems like Google’s AlphaGo, to virtual assistants like Amazon’s Alexa and Apple’s Siri. There is also \na push to develop open source AI platforms like Google’s TensorFlow to Baidu Brain. Alibaba has also \npartnered with the Mala ysian government to launch the first smart city AI platform outside China \n(Hangzhou was the first example). Apart from internal incubation and development, these tech giants and \nVCs are also investing heavily in startups focused on AI. \nTechnology giants dominate investment in AI\n1 Estimate of 2016 spend by corporations to develop and deploy AI -based products. Calculated for top 35 high tech and advanced m anufacturing companies investing in AI. Estimate is \nbased on the ratio of AI spend to total revenue calculated for a subset of the 35 companies.\n2 VC value is an estimate of VC investment in companies primarily focused on AI. PE value is an estimate of PE investment in AI -related companies. M&A value is an estimate of AI deals \ndone by corporations. “Other” refers to grants and seed fund investments. Includes only disclosed data available in databases , and assumes that all registered deals were completed \nwithin the year of transaction. Compound annual growth rate values rounded. \n3 M&A and PE deals expressed by volume; VC deals expressed by value.\nVenture\ncapital\nM&A\n8–12\nOther\nPrivate\nequity\nInvestment in AI, 20161\n$ billion\nAI share of total \ninvestment \ncategory, 20163\n%\nCompound annual \ngrowth rate2\n%\n<1\n1–3\n2–3\n2010–13 2013–16\n55 85\n15 20\n35 40\nInvestment by tech giants and other corporations\n6–9\n2–3\n18–27\nInternal\ncorporate\ninvestment1 VC, PE, and\nother external\nfunding3\nM&A2\n \nNational Strategy for Artificial Intelligence \n \n107 \n \n \nSource: Press search \nPartnerships \nThese countries are also leveraging different combinations of public -private-academia to develop and \npromote AI: \na) In the U.K., a public-private-academia partnership was established as \"sector deals\" to improve \nproductivity. The expansion of tech parks through the tech nation program is also an example of \npublic-private partnership. The government is also trying to develop regional R&D partnerships \nbetween universities, large corporations and investors in the sector, e.g., BT has partnered wi th \n15 universities across the U.K. on creating AI powered, next generation data infrastructure.  \nb) Japan has instituted new programs to triple research -industry collaboration by 2025 (including \nco-locating industry employees with researchers). They have also signed collaboration pacts with \nthe US and Israel for technology transfer and joint R&D projects.  \nc) Japan is trying to foster solutioning of challenges faced by large corporation by connecting them \nwith startups, e.g., Japan Open Innovation Council, New Ene rgy and Industrial Technology \nDevelopment Organisation (NEDO) pitch, etc. to connect startups with corporations. \nd) China has formed a \"national team\" with large private players including Baidu and Tencent to \nundertake fundamental and applied research across different AI topics, e.g., Baidu is working \nwith the Chinese government to develop brain-inspired intelligent technology \nMeanwhile, recent developments in the digital ecosystem have triggered a discussion on implications for \nregulations on data protection and privacy. EU has released a comprehensive legal framework for data \nprotection called General Data Protection Regulation (GDPR). This framework details the rights of \nindividuals (consent, data portability, etc.), obligations of businesses (define and share how they will use \npersonal data, norms for data processing, data protection impact assessment, etc.) and plan of action in \ncase of a data breach (data breach notifications, compensation to individuals, penalties, etc.). As AI grows \n▪ Developed Amazon \nweb services for \ncloud computing\n▪ Customer services \n▪ Warehouse \nmanagement\nMake existing \noperations \nmore efficient\n▪ Customized search \nresults \n▪ Customized ad \nranking\n▪ Google cloud \nplatform\n▪ Leverages Al to \nimprove operational \nefficiency and \ncustomer \nexperience of e-\ncommerce ecosystem\nOffer new \nservices and \nproducts to \nexisting \ncustomer\n▪ Provides open \nsource Al platform \nTensorflow\n▪ Offers customized \nASIC for ML TPU \n▪ Online translation \n▪ Graphic and voice\nsearch\n▪ Integrates Al into \ncloud services \noffering \n▪ Provides mobile \nrobotic based \nautomated storage \nand retrieval \nsystems \n▪ Opens Al technology \nplatform Baidu Brain \n▪ Offers Al solutions in \nfinance, healthcare, \nand traffic & \ntransportation \nindustries\n▪ Netflix like streaming \nservice iQiyi\n▪ Launches Al data \napps construction \nplatform\n▪ Provides enterprise \nlevel Al solutions in \nwith Ali-cloud as \nfoundation\n▪ Tmall genie – voice \ncontrolled smart \nhome assistant\nDevelop \ndisruptive \nbusiness \nmodels\n▪ Virtual assistant: \nGoogle Now \n▪ Game Al system: \nAlphaGO\n▪ Self-driving car \nsystem \n▪ Smart home: Google \nHome\n▪ Virtual assistant \n– Alexa\n▪ Polly – life-like \nspeech \n▪ Rekognition – image \nanalysis\n▪ Lex – conversational \nengine\n▪ Customized search \nresults and ad \nranking \n▪ 020: order \nprioritization and \nroute planning\n▪ DuerOS: Virtual \nassistant\n▪ Little fish – voice \ncontrolled family \nrobot \n▪ Apollo – set of \nartificial intelligence \ndriven tools for self-\ndriving vehicles\n▪ Smart city AI \nplatform developed \nfor Kuala Lumpur \n▪ Smartmesh\nconnectivity \nsolution supports \nmany-to-many \nbluetooth mesh \ntechnology\n\n \nNational Strategy for Artificial Intelligence \n \n108 \n \nrapidly across geogra phies and sectors, governments across the world are actively working on \ndeveloping data privacy and security regulations.  \nFurthermore, governments are playing an active role in developing AI ecosystems to capitali se on the \nsocial, economic benefits and establish leadership in the field of AI.  \na) Social benefits : Governments are focusing on sectors ranging from education to healthcare, \nagriculture to transport mobility with a view to significantly improve quality of life of its citi zens. \nb) Economic benefits : Governments have defined substantial economic aspirations through \ndevelopment and implementation of AI. While China aims to grow AI’s contribution to GDP to 26 \npercent and the U.K. by 10 percent by 2030, Japan has estimated the economic impact of AI \napplication at JPY 1.1 trillion by 2045. \nc) Leadership in AI : Given the rapid pace at which AI technology is evolving, governments are \nsetting themselves up for success with support from the private sector and academia. However, \nthe models of engagement vary depending on starting points, challenges and appetite for public \nfunding and regulation.  \nIf some countries decide to wait for a few years to establish an AI strategy and put in place the foundations \nfor developing the AI ecosystem, it seems unlikely that they w ould be able to attain and match up to the \ncurrent momentum in the rapidly changing socio-economic environment. Therefore, the need of the hour \nis to develop a policy framework that will help set up a vibrant AI ecosystem in India.  \nThe following table highlights funding commitments made by governments across the world to promote \nAI research and application: \nCountry Area Funding  \nBelgium  AI research in \nacademia \n Two funding agencies – FWO (Flanders) and FNRS \n(Wallonia). \n FNRS spent approximately EUR1.8 million per year in \nthe period 2011-2017 and FWO approximately EUR6.7 \nmillion per year. \n Between 2011 and 2017 around 67 out of 241 AI-related \napplications (representing 2.3% of all applications) \nsubmitted to FNRS were funded, and 175 out of the 832 \nAI-related applications sent to FWO were also accepted. \nChina  AI startups  In China, governments play a deliberate and explicit role \nin funding scientific research (giving USD800,000 to \nUSD1 million in subsidies to AI companies). \nDenmark  AI startups  The Innovation Fund Denmark has provided EUR20 \nmillion as funding for big data in 2017. \nGermany  AI basic research \n Applied AI \nresearch \n With an annual budget of more than EUR3 billion, the \nGerman Research Foundation (DFG) is the main source \nof funding for basic research in AI in Germany. \n In the past thirty years (1988-2018), applied AI has been \nfunded continuously by the Federal Ministry of \nEducation and Research (Bundesministerium für Bildung \nund Forschung - BMBF), for a total of EUR215 million.  \n Current annual investment in AI is EUR40-50 million. \nBetween 1988 and 2017, DFKI received EUR200 million \nfrom BMBF. There is additional funding allocated to \nuniversities and other research centres by the \ngovernment. \n \nNational Strategy for Artificial Intelligence \n \n109 \n \n BMBF and the Federal Ministry of the Economy and \nEnergy (Bundesministerium für Wirtschaft und Energie - \nBMWi) are currently funding a selection of Industry 4.0 \nprojects such as Mixed Reality Production 4.0, for a total \nof EUR550 million since 2013. \n The program Smart Services World II aims to address \nareas where digitalisation could have an impact for the \neconomy. It has funding of EUR50 million from BMWi \n(2017-2021). \nIreland  AI startups  The Irish government spends, through the Irish \nEconomic Development Agency (IDA), Enterprise \nIreland, and Science Foundation Ireland, over EUR700 \nmillion on R&D annually. Enterprise Ireland funds Irish \ncompanies and is the largest VC fund in Europe. \nIsrael  AI ecosystem – \npartnerships \n The Israeli government has several grant funding \nschemes for promoting collaboration and knowledge \ntransfer between academia and industry, such as \nMagnet and Magneton. \n The Israeli Science Foundation has a rich history of \nfunding AI projects in academia that provide researchers \nwith a high degree of freedom in their research \ncompared to other countries. \nNetherlands  AI research – \nacademia \n The main funding body for academia in the Netherlands \nis the Netherlands Organisation for Scientific Research \n(NWO). Since 2002, NWO has funded 119 research \nprojects containing the term ‘artificial intelligence’ and \n142 research projects containing the term ‘machine \nlearning’. In 2015, a programme on ‘Natural Artificial \nIntelligence’ was launched, which has funded five \nprojects. \nSpain  AI in industry  Since 2016, EUR170 million has been invested in the \nIndustry 4.0 project Industria Conectada 4.0 under the \nNational R&D and Innovation Plan. Industry 4.0 focuses \non skills, cooperation, industrial adoption, and digital \ntechnologies (robotics, AI, cloud, cybersecurity, big \ndata). \nSweden  AI research – \nacademic \n AI ecosystem - \npartnerships \n The Research Institutes of Sweden is currently setting \nup an AI centre (RISE AI), with an initial turnover of \nSEK50 million (approx. EUR4.9 million) per year in R&D, \nover 4 startups, more than 50 experts and around 30 \nactive industrial collaborations (e.g. Nokia, Ericsson, \nABB, and H&M). \n Vinnova, Sweden's Innovation Agency, has funded 190 \nAI-projects totaling SEK398 million (approx. EUR38.9 \nmillion) in the past 6 years. \n \nAppendix III: Data Ecosystem \n A key enabler  \nThe true value of AI will not be found in, say, an algorithm or a neural network itself.  \nToday, the leading algorithms are available as software packages either commercially or open source. \nStorage infrastructure and huge computing resources has become commoditi sed and easily available \nfrom various vendors in the market. The challenge still remains with data – access to high quality, reliable \ndata along with appropriate mark-ups still remains a challenge.  \nUnderstanding key issues related to data thus becomes crucial while evolving the national AI strategy. \nAccess to vast quantities of data  is vital for AI to be effective. Large platform s and technology \ncompanies who create monoliths of high volumes of data have a distinct advantage compared to smaller \ncompanies and startups, thus leading to a very skewed market.  \nConvergence of data is another challenge, with much data being either “dark” (unstructured, not readily \nusable) or disparate (hard to combine). Organisations need to be able to converge and make sense of \ndata from sources such as Internet of Things (IoT) sensors and social networks. The greater the data \ndensity and variety, the greater are the chances of finding “unknown unknowns” – relationships that were \nnot known to exist or were not looked for at all. \nData annotation is crucial, given the requirement of tagged or annotated data required for machine \nlearning or AI. In most cases, data needs to be annotated manually or in semi-automated ways for the \npurpose of machine learning, even though sometime s annotated data can be generated automatically \nfrom the source. Technological advancements in data annotation systems are not at a level where manual \nannotation can be replaced. \nBox 21: ImageNet \nThe ImageNet database has led to huge advancement in the field of image recognition. The ImageNet \ndatabase was presented for the first time as a poster at the 2009 Conference on Computer Vision and \nPattern Recognition (CVPR) in Florida by researchers from the Computer Science department at \nPrinceton University. The ImageNet project is a large visual database designed for use in visual object \nrecognition software research. Over 14 million URLs of images have been hand annotated by ImageNet \nto indicate what objects are pictured; in at least 1 million of the images, bounding boxes are also provided. \nImageNet contains over 20,000  ambiguous categories; a typical category, such as \"balloon\" or \n\"strawberry\", contains several hundred images. The dat abase of annotations of third-party image URLs \nis freely available directly from ImageNet; however, the actual images are not owned by ImageNet.  \nThe 2010s saw dramatic progress in image processing. In 2011, a good ILSVRC classification error rate \nwas 25%. In 2012, a deep convolutional neural net achieved 16%; in the next couple of years, error rates \nfell to a few percent. With  the 2012 breakthrough \"combined pieces that were all there before\", the \ndramatic quantitative improvement marked the start of an ind ustry wide AI boom. By 2015, researchers \nreported that software exceeded human ability at the narrow ILSVRC tasks. However, as one of the \nchallenge's organisers, Olga Russakovsky, pointed out in 2015, the programs only have to identify \n \nNational Strategy for Artificial Intelligence \n \n111 \n \nimages as belonging to one of a thousand categories; humans can recogni se a larger number of \ncategories, and also (unlike the programs) can judge the context of an image.  \nOvercoming issues related to data access \na) Government data sharing: Government of India has large amounts of data lying in silos across \nministries. The government can launch a mission of making all these data available for public \ngood after undertaking proper privacy checks. For example – climate data, non-strategic remote \nsensing data, regional language speech (from All India Radio), soil health data etc.  \nb) Corporate data sharing: Corporates based in India may be mandated to share their data for social \ngood. For example, sharing transportation pattern of individuals  / mass transits, collected by \nservice providers and aggregators, can help the city planners help in planning routes, predicting \nand managing traffic. \nc) Consent based data sharing: Lot of the data about individuals are personal in nature and hence \ncannot be shared by third party who have the access to these data – like financial institutions or \nhospitals. Upon proper and informed consent from the citizen s, these anonymised data may be \nshared for the purpose of artificial intelligence and data analytics. \nd) Digitised and crowdsourced collection of data by government: Huge amounts of money and time \nis spend every few years to carry out the household consumption survey. A mechanism, as \nadopted by online social networks, to incentivise individuals to share details of their consumption \npattern via an app can greatly reduce the cost of  manual surveys and lend itself to big data \nanalysis and AI applicability.  \nOften AI based transformation follows as a next step to digital transformation – in that case, the data for \nthe purpos e of learning exists with the organi sation. For example, a financial institution would have \naccess to large amounts of historic customer records to develop the AI system for predictive loan defaults.  \nPeople or organisations outside the data ecosystem might have limited access to this data. For example, \na new company entering in the space of rural financing would need access to huge amounts of customer \ndata along with information on the default rates to develop the risk based lending model. To bridge this \naccess gap, some of these datasets are made available by the research communities as shown in the \ntables below.  \nDatasets for Computer Vision and Image Processing \nDataset Name Dataset Size Publisher Publisher Type Country \nMNIST 60,000 United States Census \nBureau \nInstitute and \nBureau \nUSA \nCIFAR 10 & CIFAR 100 60,000 Canadian Institute for \nAdvance Research \nResearch \nInstitute \nCanada \nImageNet 14,000,000 Princeton University University USA \nLSUN 10,000 Princeton University University USA \nPascal Voc 5,00,000 Oxford University University UK \nSVHN 6,00,000 Stanford University University USA \nMS COCO 2,50,000 Google, CMU, etc. Research \nConsortium \nUSA \nVisual Genome 108,000 Stanford University University  \n \nNational Strategy for Artificial Intelligence \n \n112 \n \nLabeled Faces In The Wild 13,000 University of Massachusetts University USA \n \nDatasets for Natural Language Processing and Text Mining \nDataset Name Dataset Size Organisation Org. Type Country \nTREC 120K-3.6M NIST Information \nTechnology Laboratory \nResearch USA \nWikiText 100 million Salesforce Company USA \nQuestion Pairs 4,00,000 Kaggle Company USA \nSQuAD: 1,00,000 Stanford University USA \nCMU Q/A Dataset:  Carnegie Mellon University University USA \nMaluuba Datasets: 120K Microsoft Company USA \nBillion Words: 1 Billion Google Company USA \nCommon Crawl: 1.81 billion Common Crawl Non-Profit \nOrganisation \nUSA \nbAbi: Multiple Facebook AI Research \n(FAIR) \nCompany USA \nStanford Sentiment \nTreebank: \n107,785 Stanford University USA \n20 Newsgroups: 20,000 CMU & UCI University USA \nReuters 21578 AT&T Labs Research USA \nIMDB 25,000 Amazon Company USA \nUCI’s Spambase 4601 UCI University USA \n \nIt is clearly evident that  universities and research laboratories in the USA in particular, have been the \nleaders in sharing these datasets, which has contributed significantly in developing a strong AI research \nbase in the USA. The Government of India can also play a crucial role, working with the various academic \ninstitutions, in making annotated India specific data available for advancing AI research.  \n \n  \n \nNational Strategy for Artificial Intelligence \n \n113 \n \n \nAppendix IV: What Do the Markets Say? \n Approaches to evaluating focus sector areas  \n \nOne way of evaluating which sectors to focus on is to follow the money i.e. which sectors have seen the \nmost amount of VC funding in AI space. Venture Capital funding, in a sense, is a long -term view on a \ntechnology / solution, given that the median time to exit for most VC investments is more than 8 years.  \nThe AI ecosystem is essentially based on 5 pillars:  \na) policy makers,  \nb) large companies,  \nc) startups,  \nd) universities and  \ne) multi-stakeholder partnerships.  \nTaking into account this perspective as well, sectors that are most pursued by VCs could be a good proxy \nfor sectors to be focused on, for well-funded startups are capable of pushing the technology frontiers and \nbringing ambitious solutions to fruition.  \nAI remains the most active industry vertical in VC funding landscape, with an aggregate total investment \nof more than USD31 billion globally across more than 3,600 disclosed deals in past 5 years42. Healthcare \nhas been the hottest area of AI startup investments, with USD2.5 billion in VC investments in last 5 years, \nmuch of which has been fueled by medical imaging and diagnostics compa nies. Other active healthcare \nsub-segments include clinical trials & drug discovery and insights and risk analytics. The healthcare AI \nspace has also been dominated by strong public partnerships in diagnostics: nVIDIA and GE, Google \nDeep Mind and NHS, AliHealth and AstraZeneca.  \nAnother way to evaluate this could be to see where the large global tech companies (Google,  Apple, \nFacebook, Amazon and Microsoft, popularly termed GAFAM, as well as Alibaba and  its peers Baidu, \nTencent and Xiaomi, popularly termed BATX in China) are most invested in. Among GAFAM, looking at \nthe mentions of “Machine Learning” in respective earning transcripts since 2013, Google emerges as the \ncompany that has been highlighting its progress in AI / ML more than others.  \nTracking Googl e’s AI  / ML initiatives, it appears that the Mountain View based company is heavily \ninvested in Healthcare. Indeed, GV, formerly Google Ventures, the venture capital investment arm of \nAlphabet Inc., has tripled its investments in Healthcare deals in 2017 c ompared to 2013. Google’s \napproach to using AI to tackle diseases and lifestyle management is based on three pillar approach :  \na) Data generation: digitisation and consolidation of data through wearables and medical imaging \netc. \n                                                      \n42 CB Insights: “Artificial Intelligence Trends To Watch In 2018” and “Up And Up: Healthcare AI Startups See Record Deals” \n \nNational Strategy for Artificial Intelligence \n \n114 \n \nb) Disease detection \nc) Disease / lifestyle management \nGoogle is currently heavily focused on eye diseases (diabetic retinopathy), diabetes (detection and \nmanagement), heart disease (including hearth condition monitoring), Parkinson’s disease and multiple \nsclerosis. Google’s AI algorithm for  diabetic retinopathy, trained on 128,000 images, was on -par with a \npanel of ophthalmologists. Other areas that Google may be exploring include Chronic Lower Respiratory \nDisease, several types of cancer, mental and behavioral health and aging.  \nGoogle is also invested in powering the healthcare data infrastructure, as evident in its USD625 million \nacquisition of Apigee, which is building healthcare APIs catering to the latest health records \ninteroperability protocols. Similarly, Deep Mind is building a data infrastructure to enabling building of \napps that can analyse different data elements. Furthermore, Google is also building health data streams \nthat third parties could integrate in their research. Another one of Google’s interesting foray in healthcare \nAI is developing tools for doctors that are designed to augment their expertise.",
    "metadata": {
      "filename": "NationalStrategy_for_AI_Discussion_Paper.pdf",
      "source": "uploads"
    }
  },
  {
    "id": "bbd6e644-a0ed-4380-9280-e20f76d41d45",
    "text": "OECD ARTIFICIAL \nINTELLIGENCE PAPERS\nOctober 2023  No. 3\nTHE STATE OF \nIMPLEMENTATION \nOF THE OECD AI \nPRINCIPLES FOUR \nYEARS ON\n2    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nThis report was approved and declassified by written procedure by the Committee on Digital Economy \nPolicy on 31 August 2023 and prepared for publication by the OECD Secretariat. For more information, \nplease visit www.oecd.ai.  \n \nNote to Delegations: This document is also available on O.N.E. under the reference code: \nDSTI/CDEP/AIGO(2023)5/FINAL \n \nThis document, as well as any data and any map included herein, are without prejudice to the status of or \nsovereignty over any territory, to the delimitation of international frontiers and boundaries and to the name \nof any territory, city or area.  \nThe statistical data for Israel are supplied by and under the responsibility of the relevant Israeli authorities. \nThe use of such data by the OECD is without prejudice to the status of the Golan Heights, East Jerusalem \nand Israeli settlements in the West Bank under the terms of international law. \nNote by the Republic of Türkiye \nThe information in this document with reference to “Cyprus” relates to the southern part of the Island. There \nis no single authority representing both Turkish and Greek Cypriot people on the Island. Türkiye recognises \nthe Turkish Republic of Northern Cyprus (TRNC). Until a lasting and equitable solution is found within the \ncontext of the United Nations, Türkiye shall preserve its position concerning the “Cyprus issue”.  \nNote by all the European Union Member States of the OECD and the European Union \nThe Republic of Cyprus is recognised by all members of the United Nations with the exception of Türkiye. \nThe information in this document relates to the area under the effective control of the Government of the \nRepublic of Cyprus. \n \n \n \n \n \n \n \n \n \n© OECD 2023  \nThe use of this work, whether digital or print, is governed by the Terms and Conditions to be found at \nhttp://www.oecd.org/termsandcon.  \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   3 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nForeword \n \nThe state of implementation of the OECD AI Principles four years on was prepared under the aegis of the \nOECD Working Party on Artificial Intelligence Governance (AIGO) and the OECD Committee on Digital \nEconomy Policy (CDEP). AIGO delegates and national contact points f or the database of national AI \npolicies of the OECD.AI Policy Observatory contributed significantly.  \nThis report was a collective effort co-ordinated by the AI Unit in the OECD Digital Economy Policy division. \nLucia Russo, Noah Oder (consultant to the OEC D), and Karine Perset led the report development and \ndrafting. Audrey Plonk (head of the OECD Digital Economy Policy Division) provided comments and \nfeedback. Valéria Silva (consultant to the OECD) contributed to drafting the section on emerging AI-specific \nregulations. András Hlács and Sharon Ho (consultant to the OECD) contributed to the report with \nbackground research. \nThis report benefited from the contribution of  the following experts who presented their AI policy work at \nthe AIGO meetings in November 2022 and April 2023. They are: Paula Garnero and Vanina Martínez \n(Argentina), Ciro Eduardo Ferreira (Brazil), Roxane Sabourin (Canada), Juraj Bilic (Croatia), Kilian Gross, \nTatjana Evas and Yordanka Ivanova (European Commission), Kristine Alanko (Finland), Jibu Elias (India), \nSangwon Ko (Korea), Luis Ricardo Sánchez Hernández (Mexico), Ioan Istrate (Romania), Larissa Rim and \nZee Kin Yeong (Singapore), Jana Novohradska (Slovakia), Miguel Valle Del Olmo (Spain), Isabelle Lois  \nand Livia Walpen (Switzerland), Oleksandr Bornyakov (Ukraine), Abraham Baldry (United Kingdom), Lilly \nMcFeeters, Lindsey Barrett, Elham Tabassi, Sam Schofield and Sorelle Friedler (United States).  \nIn addition, the quality of the report benefited significantly from the engagement and writ ten contributions \nof national delegations to the OECD Working Party on Artificial Intelligence Governance and the OECD \nCommittee for Digital Economy Policy, namely: Tim McInerney (Australia’s International, Trade & National \nSecurity Division), Ciro E Ferre ira (Brazil’s Ministry of Foreign Affairs), Allison O’Beirne and Roxane \nSabourin (Canada’s Ministry of Innovation, Science and Economic Development), Philipp Schulte \n(Germany’s Federal Ministry for Digital and Transport), Kása Ferenc (Hungary’s Neumann Tec hnológiai \nPlatform), Ziv Katzir (Israel Innovation Authority), Nobuhisa Nishigatan and Takayuki Honda (Japan’s \nMinistry of Internal Affairs and Communications), Soohyeon Kang (Korea’s Ministry of Science and ICT), \nLuis Ricardo Sánchez Hernández (Mexico’s National Institute of Transparency, Access to Information and \nProtection of Personal Data), Christine Hafskjold (Norway’s Ministry of Local Government and Regional \nDevelopment), and David C. Turnbull (United States’ Department of State).  \nThe authors are gra teful to John Tarver for editing this report and to Andreia Furtado for editorial and \npublishing support. The overall quality of this report benefited significantly from their engagement.  \n4    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nAbstract \nThe OECD AI Principles were adopted in 2019. They set out a  framework \ncontaining ten principles – divided into five values-based principles and five \nrecommendations to governments – for Members and adhering non -\nMembers to promote and implement in their policies responsible stewardship \nof trustworthy AI. This repor t takes stock of the initiatives launched by \ncountries worldwide to implement the OECD AI Principles and reported to \nthe OECD.AI Policy Observatory as of May 2023. It provides an overview of \nnational AI strategies, including their oversight and monitoring bodies, expert \nadvisory groups, as well as their monitoring and evaluation frameworks. This \nreport also discusses the different regulatory frameworks countries are \nimplementing to ensure the trustworthiness of AI systems. Th ese include \nnational ethics frameworks and principles, emerging AI -specific regulation, \nand regulatory sandboxes. Finally, it presents illustrative examples of policies \nthat implement each of the ten OECD AI Principles, to help policy  makers \nlearn from one another. \n \n \n \n \n \n \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   5 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nAbrégé \nLes Principes de l’OCDE sur l’IA ont été adoptés en 2019. Ils fixent un cadre \nqui s’articule autour de dix principes – cinq principes fondés sur des valeurs \net cinq recommandations à l’intention des pouvoirs publics – destinés à aider \nles Membres et les non-Membres ayant adhéré à l’instrument à promouvoir \net mettre en œuvre, dans leurs politiques, une approche responsable à \nl’appui d’une IA digne de confiance. Le présent rapport propose un tour \nd’horizon des initiatives lancées par les pays  du monde entier pour donner \ncorps aux Principes de l’OCDE sur l’IA et consignées dans l’Observatoire \nOCDE des politiques relatives à l’IA (OECD.AI) jusqu’en mai  2023. Il donne \nun aperçu des stratégies nationales en matière d’IA, y compris des organes \nde suivi et de contrôle, des groupes consultatifs d’experts et des cadres de \nsurveillance et d’évaluation. En outre, il examine les différents cadres \nréglementaires mis en place par les pays pour veiller à la fiabilité des \nsystèmes d’IA, qu’il s’agisse de cadr es et de principes d’éthique nationaux, \nde nouvelles réglementations propres à l’IA, ou de bacs à sable \nréglementaires. Enfin, il présente des exemples concrets de politiques qui \ndonnent effet aux dix Principes de l’OCDE sur l’IA, afin d’aider les décideurs \nà tirer des enseignements de leurs expériences respectives. \n  \n6    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nTable of contents \nExecutive summary 8 \nRésumé 9 \nIntroduction 11 \n1 National AI strategies are on the rise worldwide 11 \nOversight and monitoring bodies manage national AI strategy implementation  12 \nExpert groups advise governments on advantages and challenges linked to AI systems  14 \nCountries establish monitoring and evaluation frameworks for national AI strategies  14 \n2 Countries use different regulatory frameworks to ensure the trustworthiness of AI \nsystems 15 \nSeveral countries have issued national ethics frameworks and principles 15 \nAI-specific regulation is emerging in several jurisdictions 17 \nA comparative analysis of similarities and differences in AI-specific regulations across selected \njurisdictions 23 \nGovernments and innovators are testing innovative AI solutions in controlled environments  26 \n3 Implementing the OECD AI values-based principles 28 \nInclusive growth, sustainable development, and well-being (Principle 1.1) 30 \nHuman-centred values and fairness (Principle 1.2) 34 \nTransparency and explainability (Principle 1.3) 38 \nRobustness, security, and safety (Principle 1.4) 41 \nAccountability (Principle 1.5) 43 \n4 Implementing the five recommendations to governments 47 \nInvesting in AI Research & Development (Principle 2.1) 47 \nFostering a digital ecosystem for AI (Principle 2.2) 52 \nFostering an enabling policy environment for AI (Principle 2.3) 55 \nAI skills, jobs and labour market transformation (Principle 2.4) 56 \nInternational and multi-stakeholder co-operation on AI (Principle 2.5) 59 \nReferences 65 \nAnnex A. 75 \nTable A.1. AI definitions, risk classifications, scope and oversight in select AI-specific \nregulations 75 \nAnnex B. 80 \nFurther aspects of selected AI-specific regulations 80 \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   7 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFIGURES \nFigure 1.1. A timeline of national AI strategies 13 \nFigure 2.1. Regulatory sandboxes in AI: definition, risks, opportunities and policy considerations 27 \nFigure 3.1. Breakdown of issues covered by policy initiatives linked to OECD AI values-based principles 29 \nFigure 3.2. Select national policies that implement OECD AI Principle 1.1 on inclusive growth, sustainable \ndevelopment and well-being 30 \nFigure 3.3. Select policies that implement OECD AI Principle 1.2 on human-centred values and fairness 35 \nFigure 3.4. Select policies that implement OECD AI Principle 1.3 on transparency and explainability 38 \nFigure 3.5. Select policies that implement OECD AI Principle 1.4 on robustness, security and safety 42 \nFigure 3.6. Select policies that implement OECD AI Principle 1.5 on accountability 44 \nFigure 4.1. Select policies that implement OECD AI Principle 2.1 on investing in AI R&D 48 \nFigure 4.2. AI research publications, 2000-2021, top publishing countries 48 \nFigure 4.3. High-impact AI research publications, 2000-2021, top publishing countries 49 \nFigure 4.4. Select policies that implement OECD AI Principle 2.2 on fostering a digital ecosystem for AI 52 \nFigure 4.5. Language Models: definition, risks, opportunities, policy considerations and national initiatives 55 \nFigure 4.6. Select policies implementing OECD AI Principle 2.3 on Fostering an enabling policy environment \nfor AI 56 \nFigure 4.7. Select policies that implement OECD AI Principle 2.4 on AI skills, jobs and labour market \ntransformation 57 \nFigure 4.8. Select policies implementing the OECD AI Principle 2.5 on international co-operation 60 \nFigure 4.9. Globalpolicy.AI: a platform for intergovernmental co-operation on AI 63 \nTABLES \nTable 2.1. Examples of existing and emerging AI-specific regulatory approaches in select countries 16 \nBOXES \nBox 2.1. The United Kingdom AI Standards Hub 26 \n \n8    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nExecutive summary \nIn 2022 , artificial intelligence (AI) took centre stage  in policy discussions . The release of ChatGPT in \nNovember exposed the potential of generative AI to a mainstream audience. AI tools continue to \ndemonstrate potential to bring transformative changes in many areas, promising productivity gains and \nnew opportunities for individua ls, businesses, and society. At the same  time, AI’s potential risks, such as \nthe perpetuation of existing inequalities, the massive spread of manipulated conten t, and the threat to \npersonal autonomy, have been brought to the fore, particularly by generative AI.   \nAs the first intergovernmental standard on AI in 2019, the OECD AI Principles are now a global reference \npoint for trustworthy AI. Countries are now working on policies based on the principles to tackle AI’s risks \nand capitalise on opportunities. \nIn 2017, only a few countries had national AI strategies. Today, the OECD.AI Policy Observatory contains \nover 50 national strategic and government-wide initiatives on how to comprehensively steer trustworthy AI \ndevelopment and deployment . Beyond OECD countries, Arab, African , and South American partner \neconomies have also committed to actions that promote the AI principles. Overall, over 930 related policy \ninitiatives across 70 jurisdictions had been reported to the OECD.AI policy hub by May 2023.   \nCountries have continued to develop policies to foster research and development (R&D) in AI and increase \nefforts to build and provide access to the required infrastructure to enable  broader AI adoption. Some \ncountries have strengthened their efforts to ensure different social groups develop AI skills, recognising \nthe need to both prepare workers and citizens at large, and to monitor and accompany labour market \ntransitions. Because AI transcends borders, many international co -operation initiatives have been \nlaunched or deepened. While some types of policy are still emerging, others are more advanced allowing \ncountries to reflect on achievements to date and analyse further steps to advance national ambitions.  \nThe most notable development pertains to policy actions that translate the OECD AI value s-based \nprinciples into concrete, operational initiatives. This report takes stock of various types of policies designed \nto enhance inclusive growth, address bias and increase fairness and to make AI systems transparent, safe \nand accountable.  Ethics framework , guidelines, codes of conduct , standards, and algorithmic impact \nassessment are being piloted at both national and international level, by both the public and private sectors.  \nExisting legislation, including on data protection and consumer protection, includes provisions relevant to \nAI. However, a  major development in recent years has been the proposal of AI -specific regulatory \nframeworks that address AI high-risk systems or impacts , albeit with key differences in approach across \njurisdictions. AI-specific regulation also raises new challenges in relation to international interoperability, \nwhich calls for international action to promote alignment of key de finitions and their technical \nimplementation where appropriate. \nThe report presents illustrative examples of the implementation of each AI Principle to help policy makers \nlearn from one another.  \n  \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   9 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nRésumé \nEn 2022, l’intelligence artificielle (IA) s’est retrouvée au cœur des débats sur l’action des pouvoirs publics. \nAvec le lancement de ChatGPT, en novembre, le grand public a découvert le potentiel de l’IA générative. \nLes outils d’IA montrent chaque jour un peu plus leur capacité à impulser des changements profonds dans \nde nombreux domaines, avec à la clé des perspectives de gains de productivité et de nouvelles possibilités \npour les individus, les entreprises et la s ociété. Dans le même temps, les avancées de la technologie, en \nparticulier de l’IA générative, ont mis en lumière un certain nombre de risques (persistance des inégalités \nexistantes, propagation massive de contenus manipulés, ou encore risque de perte d’au tonomie des \npersonnes). \nPremière norme intergouvernementale dans le domaine de l’intelligence artificielle lors de leur adoption \nen 2019, les Principes de l’OCDE sur l’IA sont devenus une référence mondiale pour la mise en place \nd’une IA digne de confiance. Les pays s’en inspirent aujourd’hui pour élaborer des politiques afin de gérer \nles risques qu’elle induit, tout en capitalisant sur ses potentialités. \nEn 2017, seuls quelques pays disposaient d’une stratégie nationale en matière d’IA. Désormais, \nl’Observatoire OCDE des politiques relatives à l’IA (OECD.AI) recense plus de 50 stratégies nationales et \ninitiatives gouvernementales traitant des moyens d’orienter le développement et le déploiement d’une IA \ndigne de confiance. Au-delà des Membres de l’OCDE, des pays partenaires du monde arabe, d’Afrique et \nd’Amérique du Sud se sont également engagés à prendre des mesures favorisant l’application des \nPrincipes sur l’IA. Au total, plus de 930  programmes d’action mis en place dans 70  pays et territoires \nétaient recensés en mai 2023 sur la plateforme OECD.AI. \nLes pays ont continué d’élaborer des politiques propres à stimuler la recherche et le développement (R -\nD) dans le domaine de l’IA et encourager les efforts pour bâtir les infrastructures indispensables à une \nadoption plus large de l’IA et y donner accès. Certains ont intensifié leurs efforts pour faire en sorte que \ndifférents groupes sociaux acquièrent des compétences en IA, conscients de la nécessité de préparer non \nseulement les travailleurs, mais aussi l’ensem ble des citoyens, et de suivre et d’accompagner les \nmutations à l’œuvre sur les marchés du travail. Parce que l’IA transcende les frontières, de nombreux \nprogrammes de coopération internationale ont été lancés ou étendus. Si de nouveaux types de politiques \ncontinuent de voir le jour, d’autres sont plus avancés  ; les pays peuvent alors en examiner les premiers \nrésultats et analyser la marche à suivre pour donner corps aux ambitions nationales.  \nLes mesures transposant les principes de l’OCDE fondés sur des va leurs en initiatives opérationnelles \nconcrètes constituent une évolution particulièrement notable. Ce rapport propose un tour d’horizon de \ndifférents types de politiques visant à favoriser la croissance inclusive, lutter contre les biais et accroître \nl’équité, et à faire en sorte que les systèmes d’IA soient synonymes de transparence, de sécurité et de \nresponsabilité. Des cadres d’éthique, lignes directrices, codes de conduite, normes et évaluations d’impact \ndes algorithmes sont mis à l’essai aux niveaux na tional et international, par les secteurs public comme \nprivé. \nLes législations existantes, notamment celles sur la protection des données et des consommateurs, \nintègrent certes des dispositions applicables à l’IA. Néanmoins, depuis quelques années, des propositions \nde cadres réglementaires propres à l’IA voient également le jour, qui traitent des systèmes ou des impacts \nà haut risque de l’IA, même si les approches varient sensiblement d’un pays à l’autre. Par ailleurs, la \nréglementation de l’IA pose des difficultés nouvelles en termes d’interopérabilité internationale, qui exigent \nune action coordonnée pour favoriser une harmonisation des principales définitions et de leur mise en \nœuvre technique, le cas échéant. \n10    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nCe rapport présente des illustrations concrèt es de  la mise en application de chacun des Principes de \nl’OCDE sur l’IA, afin d’aider les décideurs à tirer des enseignements de leurs expériences respectives. \n \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   11 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nThis report provides an update on policy initiatives launched since 2021, the year the OECD published the \nfirst report on the “State of Implementation of OECD AI Principles ” (OECD, 2021[1]). It presents data from \nthe OECD.AI Policy Observatory as of May 2023. While the first report focused on the implementation of \nthe five recommendations to governments to foster trustworthy AI, this report also provides an overview of \ninitiatives to put the OECD AI values-based Principles into practice, as well as a comparative analysis of \nselected emerging AI-specific regulations. \nThe report is organised as follows. Sect ion 1 details national AI strategies, i.e. , countries’ strategic \ndocuments and bodies  which structure government’s efforts in the field of AI . Section 2 presents an \noverview of regulatory approaches to promote trustworthy AI and includes a comparative analysis of key \nAI-specific emerging regulations in selected jurisdictions . Section 3 reports on national initiatives \noperationalising the five OECD value -based principles . Section 4 discusses national initiatives \nimplementing the five recommendations to governments. \nIn 2019, only  a few countries had national AI strategies. Canada, Finland, and Japan were among the \nfirst to develop national AI strategies, setting targets and allocating budgets in 2017. Australia, Denmark, \nFrance, Germany, Korea, and the United States  followed suit in 2018 and 2019. In 2020 and 2021 , \nadditional countries announce d national AI strategies, including Brazil, Chile, Spain, the Republic of \nTürkiye (hereafter “Türkiye”) , the United Kingdom , and Ukraine. In 2022, Belgium, Israel, Italy and \nThailand launched their national AI strategies , while Croatia, Greece, Iceland, and Romania are \nestablishing theirs. Some countries, such as Canada, France and Germany, have updated their national \nAI strategies, taking stock of achievements and keeping pace with technical, societal and economic \ndevelopments. As of May 2023, 51 countries had reported a national AI Strategy to the database of national \nAI policies in OECD.AI (Figure 1.1), from all regions in the world.  \nEach country’s national AI strategy has its specificities and tackles different aspects of AI policy. Mapping \nnational AI strategies to the recommendations t o governments included in the OECD AI Principles shows \ncommonalities, as most strategies focus on  inclusive growth, sustainable development and well -being, \nhuman-centred values and fairness, investing in AI R&D and building human capacity .  \nIsrael’s National AI Program me (2022) aims to address all the key challenges at the national level and \nensure Israel’s leading position on AI for the years to come. Key pillars of the programme include, but are \nnot limited to, supporting academic excellence and increasin g the number of AI researchers in the Israeli \nIntroduction \n1 National AI strategies are on the rise \nworldwide \n12    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nacademia, providing the needed computation infrastructure for R&D in academia, the private as well as \nthe public sector, and accelerating AI adoption within the public sector (Ben-Israel, Matania, E and \nFriedman, 2020[1]), (Artificial Intelligence and Data Science Committee, 2020[2]). In 2022, Japan’s Cabinet \nOffice published the “AI Strategy 2022 ” (Japanese Government, 2022 [3]), which sets five strategic \nobjectives (human resources, industrial competitiveness, technology systems, international co -operation, \nand AI for imminent crises , e.g., natural disaster pandemic) to elaborate the three philosophies (dignity, \ndiversity and i nclusion, and sustainability) and mitigate Japan’ social issues by accelerating social \nimplementation of AI. The United Kingdom’s National AI Strategy (2021-2031) rests upon the three pillars \nof investment and planning for the long-term needs of the UK’s AI ecosystem; support to the UK’s transition \nto an AI -enabled economy ; and national and international governance of AI to encourage innovation, \ninvestment, and protect the public and fundamental values (AIGO, 2022 [4]). Ukraine’s Strategy of AI \nDevelopment (2021-2030) seeks to harness the country’s existing AI capacity and channel it towards a \nnumber of strategic national priorities (Atlantic Council, 2021 [5]). According to the Ukrainian gover nment, \nits strategy states that AI is an important basis for Ukraine to develop its digital economy, improve quality \nof life, increase public administration efficiency, and to deliver goods and services. It focuses on education, \ndefence and security, the p ublic sector, and Ukraine’s key economic sectors. It also aims to attract \ninvestment in AI through the Ukrainian Start-up Fund – the largest angel investor – and the Blue and Yellow \nHeritage Fund, which supports AI start-ups. India’s approach to AI is based on the following structure: (1) \nAI for All, (2) Responsible AI for Social Empowerment; (3) Becoming the global garage for AI tools, \nespecially in the developing world; (4) Focus on AI -powered language technology  (Indian Government, \n2023[6]). Its National Data Governance Framework Policy  is a crucial development through which the \ngovernment gathers datasets from all Ministries to make them machine readable and available on demand.  \nIndia’s government  took the lead on AI and crafting the country’s AI ecosystem, which will diffuse AI \nlearning material in local languages (AIGO, 2022[4]). \nIn some countries, the national AI strategy is part of a broader digitalisation strategy. “Digital Switzerland” \n(2020-2022) is an umbrella strategy for all digital topics, including AI. Its principles are applicable to AI and \nother technologies.  \nFor policy coherence and effective implementation of national AI policies, governments are using different \nmodels, including: (1) creating a new governmental or co-ordination body for AI; (2) assigning oversight of \nthe development and implementation of a strategy to an existing ministry and establishing AI inter -\nministerial and multi-stakeholder committees; (3) receiving input from oversight and expert advisory bodies \nor groups for AI and data ethics bodies. Countries are also establishing monitoring and evaluation \nframeworks for their national AI strategies. \nOversight and monitoring bodies manage national AI strategy implementation \nIn 2019, Korea established the Artificial Intelligence Policy Bureau as the lead centre for strategies related \nto AI, including data and cloud. The Bureau develops and implements government-wide AI strategies. The \nUnited Kingdom’s Government Office for AI, a unit within the Department for Science, Innovation and \nTechnology, is responsible for overseeing the implementation of the National AI Strategy (OECD.AI, \n2023[8]). Likewise, the United States’ National Artificial Intelligence Initiative Office (NAIIO), established in \n2021, is mandated by the National Artificial Intelligence Initiative Act (NAIIA) to coordinate and support the \nNAIIA and is located within the White House Office of Science and Tech nology Policy (National Artificial \nIntelligence Initiative Office, 2023[7]).  \n \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   13 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 1.1. A timeline of national AI strategies \nNational AI strategies reported to the OECD.AI database of national AI policies, as of May 2023 \n \n \nOther countries have established inter -ministerial and multi -stakeholder committees to monitor the \nimplementation of the ir national AI strategy. The Governance Committee of the Brazilian AI Strategy is \nresponsible for monitoring and evaluating the Brazilian AI Strategy. It is composed of private and public \norganisations, civil organisations, and specialists (OECD.AI, 2023[8]). In Egypt, the National Council for AI \n(NCAI) outlines, implements , and governs the AI strategy in close coordination with the relevant experts \nand entities. The NCAI is chaired by the Minister of Communications and Information Technology (MCIT) \nand includes representatives from government entities, private sector, and independent experts and heads \nof several bodies concerned (OECD.AI, 2023 [8]). The Serbian government will form an AI Council \ncomposed of Ministers as well as AI industry and research leaders to follow the implementation of the AI \nstrategy (OECD.AI, 2023[8]). \nIn the People’s Republic of China (hereafter “China”), the National New Generation AI Promotion Office \nwas established by the Ministry of Science and Technology and 14 other government agencies. It oversees \nthe execution of the Next Generation AI Development Plan  (OECD.AI, 2023 [8]). The Saudi Data and \nArtificial Intelligence Authority (SDAIA) was estab lished in 2019 as the owner of Saudi Arabia’s national \nand AI agenda, mandated with unlocking the value of data and AI to elevate the country as a pioneering \nnation among the elite league of data -driven economies. Addressing data, innovation and capability -\nbuilding, SDAIA foster s the digital ecosystem while supporting the values -based G20 AI Principles  \n(OECD.AI, 2023[8]).  \n\n14    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nExpert groups advise governments on advantages and challenges linked to AI \nsystems \nCountries have established AI expert advisory groups and multi-stakeholder groups of AI experts to provide \nrecommendations while identifying  and reporting on the current and future opportunities, risks , and \nchallenges of the public use of AI.  \nIn 2019, as part of its national AI strategy, France established a national consultative committee (FNCDE) \non digital ethics and AI . The FNCDE has 27 members from different disciplines , and the working groups \nthat prepare FNCDE’s opinions often include external experts. The Prime Mi nister has already asked the \ncommittee for  opinions on ethical issues related to  digital applications that use machine learning: 1) \nconversational agents (chatbots); 2) autonomous cars; and 3) medical diagnosis and health AI (OECD.AI, \n2023[8]). Since February 2022, Korea has been operating a multi-stakeholder forum (with participants from \nindustry, academia, civil society, education and legal experts) to discuss ethical aspects of AI technologies \nand form a social consens us on how to build trust in AI. There are three expert committees (ethics, \ntechnology, education) within the forum to facilitate the consensus building. Sweden’s Committee for \nTechnological Innovation and Ethics continuously delivers policy proposals to the government and, where \nrelevant, also surveys the need to adapt regulatory frameworks (OECD.AI, 2023[8]). Similarly, Singapore’s \nAdvisory Council on the Ethical Use of AI and Data was established in 2018 to advise the government on \npolicy or regulatory intervention in the commercial deployment of AI. \nIn 2019, Canada established an Advisory Council on AI to advise its government on global leadership, \nbuilding strengths, identifying opportunities for economic growth through AI that benefits all Canadians, \nand ensuring that AI advancements reflect national values. The Advisory Council is composed of experts \nfrom Canadian academia, industry, and civil society. Since 2019, it has convened two working groups: one \non the commercialisation  of AI, which explored ways to translate Canadian -owned AI into economic \ngrowth, and one on public awareness, which is exploring the public understanding of AI and its potent ial \nbenefits and risks  (Government of Canada, 2023 [8]). In response to its Action Plan for the Digital \nTransformation of Slovakia for 2019-2022, the country’s government  formed a Standing Committee for \nEthics and Regulation of AI (2020). The Committee is an independent expert advisory committee to the \nMinistry of Investments, Regional Development and Informatisation, which centrally co -ordinates \nSlovakia’s AI agenda (AIGO, 2022 [4]). The Swiss Federal  Council established Switzerland’s \nInterdepartmental Working Group on AI  within the framework of the  \"Digital Switzerland\" strategy \n(OECD.AI, 2023[8]). China’s New Generation AI Expert Governance Committee (2019) was created by its \nMinistry of Science and Technology to research policy recommendations for AI governance and identify \nareas for international co-operation (Laskai and Webster, 2019[9]). It produced the “Governance Principles \nfor a New Generation of Artificial Intelligence: Developing Responsible Artificial Intelligence ” (Laskai and \nWebster, 2019[9]).  \nCountries establish monitoring and evaluation frameworks for national AI \nstrategies  \nA few countries have launched policy intelligence activities and annual reports to evaluate the \nimplementation of their national AI strategies.  Canada, Germany, the United Kingdom, the United \nStates, the European Commission, and Singapore published reports after monitoring and evaluating \nthe implementation of their AI strategies (OECD, 2021[1]). Several national or regional institutions, such as \nChile, the Czech Republic, France, Germany , and Quebec in Canada, have also established AI \nobservatories to monitor the implementation of national AI strategies and policies.  Brazil established the \nBrazilian Observatory of AI (OBIA) on the model of the OECD.AI Policy Observatory, serving as a \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   15 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nrepository of data to monitor the development of AI as well as a hub for collaboration and knowledge \nexchange among stakeholders in the Brazilian AI ecosystem.  \nTo date, only a few countries have conducted evaluations of their national AI strategies. The most recent \nexample is the interim evaluation of the French National Strategy for AI (SNIA) conducted by France’s \ncourt of auditors  (Court des Comptes, 2023 [11]). The evaluation focuses on “Research” and “Higher \nEducation”, the main areas of funding of the French national AI strategy, endowed with EUR1.527 billion \nand EUR1.545 billion in the first and second phases, respectively. The analysis provided a total of seven \nkey recommendations, which range from governance and monitoring (i.e., t ranslate public policy on AI in \na budget document to measure its impact ; clarify the missions  and funding of the centres of excellence ; \nestablish shared objectives and priority indicators for public policy on AI ; create a scientific and steering \ncommittee to monitor the implementation of the strategy and to define future strategic directions), to specific \nactions to strengthen AI skills (i.e.,  produce a map of AI training courses to be promoted with a common \nlabel; develop a  skills assessment needs for AI trai ners and AI researchers and establish appropriate \neducation plans), to measures related to environmental impact and the development of responsible AI (i.e., \ndraw up a charter and catalogue of best practices to define and monitor the environmental impact of  AI \nresearch; promote the development responsible AI). \nThese types of monitoring and evaluation frameworks and observatories are still too scarce, yet they are \nexpected to expand across countries as national AI strategies move into later stages of implemen tation.   \nCountries are exploring approaches to ensure trustworthy AI and mitigate risks associated with the \ndevelopment and deployment of AI systems. In addition to exploring the application and need to adapt \ncurrent legislation for AI, emerging regulatory actions for  trustworthy AI include: i) establishing ethical \nframeworks and principles, ii) considering hard law approaches, iii) supporting international standardisation \nefforts and international law efforts ( Table 2.1), and iv) promoting controlled environments for regulatory \nexperimentation (Figure 2.1).  \nSeveral countries have issued national ethics frameworks and principles \nSeveral countries have launched national ethical frameworks and principles for AI development and \ndeployment that largely overlap with the OECD AI Principles (up to May 2023, 17 such guidelines we re \nreported in the OECD database). Some countries, such as Japan, Korea, and India, provide guidelines \nto developers and operators on how to implement the principles. Furthermore, Colombia has set up an \nonline platform to monitor the framework’s implementation. \n2 Countries use different regulatory \nframeworks to ensure the \ntrustworthiness of AI systems \n16    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nTable 2.1. Examples of existing and emerging AI-specific regulatory approaches in select countries \nCountry Legislation and regulation Standards Principles  \nCanada • Directive on Automated Decision-\nMaking (2019) \n• Proposed Bill C-27, Digital Charter \nImplementation Act, including AI \nand Data Act (AIDA) (2022l) \n• Proposed CAN-ASC-6.2: \nAccessible and Equitable AI \nSystems (2023) \n• Canada’s Digital Charter (2019) \nUnited Kingdom • Proposed Online Safety Bill \n(2022) \n• Proposed Data Protection and \nDigital Information Bill (2023) \n• Algorithmic Transparency \nStandard (Central Digital Data \nOffice, 2021) \n• A pro -innovation approach to AI \nregulation (2023) \n \nUnited States • Federal Trade Commission Act, \nfor deceptive practices from \ndeepfakes or chatbots (1914) \n• Proposed Algorithmic \nAccountability Act (US AAA) \n(2022) \n• Executive Order 13960: \nPromoting the Use of Trustworthy \nArtificial Intelligence in the Federal \nGovernment (2020) \n• National Institute of Standards \nand Technology (NIST) AI Risk \nManagement Framework (2023) \n• Blueprint for an AI Bill of Rights \n(2023) \nEuropean Union  • Proposed EU AI Act (2021) \n• Proposed u pdates to the EU \nProduct Liability Directive (2022) \n• Proposed AI Liability Directive \n(2022) \n• EU’s Digital Services Act (2022) \n• CEN/CENELEC standards for AI \nand related data (forthcoming) \n• Ethics guidelines on AI (2018)  \nBrazil • Report and proposed substitute \ntext for draft bills 5051/2019, \n21/2020 and 872/2021 (2022) \n• Proposed Bill 705 on the \ncompatibility of AI use in the public \nsector with ESG practices (2022) \n• Incorporation of international \nstandards National standards by \nthe Brazilia n Association of \nTechnical Norms (ABNT) \n• Proposed Art. 3 of the proposed \nsubstitute text for draft bills \n5051/2019, 21/2020 and \n872/2021 (2022) \nChina • Chinese Internet Information \nService Algorithmic \nRecommendation Provisions \n(2021) \n• Opinion on Strengthening  the \nEthics and Governance of \nScience and Technology (2022) \n• National Standards for \nAutonomous Vehicle Testing \n(2018) \n• New Generation AI Ethics \nSpecifications (2019) \n• New Generation AI Code of Ethics \n(2021) \n• White Paper on Trustworthy AI \n(2021) \n• Internet Information Service \nAlgorithmic Recommendation \nManagement Provisions (2021) \nIntergovernmental organisations • Proposed Council of Europe \nConvention on AI, Human Rights, \nDemocracy and the Rule of Law \n(2023) \n• ISO 31000 Risk management \n(2009, 2018) \n• ISO/IEC 23053:2022 \nFramework for AI Systems \nUsing Machine Learning (ML) \n(2022) \n• OECD Recommendation of the \nCouncil on AI (2019) \n• UNESCO Recommendation on \nthe Ethics of AI (2021) \nNote: This table is a sample of emerging AI -specific initiatives from select jurisdictions at the time of writing in Ma y 2023. Elements may have \nchanged since. The table should thus be taken for illustrative purposes only.   \nAustralia’s Department of Industry, Science, and Resources developed its AI Ethics Framework (2019) in \nan effort to guide businesses and governments on the responsible design, development, and \nimplementation of AI. The AI Ethics Framework includes eight principles designed to ensure that AI is safe, \nsecure, and reliable, aligned with the OECD AI Principles (OECD.AI, 2023[8]). Similarly, Colombia’s Ethical \nFramework for AI provides a set of principles to consider in the design, development, and implementation \nof AI systems. It illustrates a methodology for determining how these principles should be considered and \nimplemented. It includes a toolbox that describes a series of strategies to develop these principles in public \nentities (OECD.AI, 2023[8]). The Korean government has developed a checklist for implementing AI ethical \nstandards, which is based on the National Guidelines for AI Ethics (AIGO, 2022[4]). The checklist is guided \nby ten key AI ethics requirements, including human ri ghts, protection of privacy, respect for diversity, \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   17 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nprevention of harm, public good, solidarity, data management, accountability, safety, and transparency \n(OECD.AI, 2023[8]). Switzerland has developed its Guidelines on AI for th e Confederation (2020), which \nare meant as a general frame of reference for the use of AI within the Federal Administration. The \nGuidelines overlap with several of the OECD AI Principles (OECD.AI, 2023 [8]). Argentina’s Ethics \nPrinciples for the Development of AI also reflect both the five value-based OECD AI Principles (Principles \n1.1–1.5) as well as the five recommendations to national governments (Principles 2.1-2.5) (AIGO, 2022[4]).  \nIndia’s approach document  is divided into two parts.  Part 1 , Principles for Responsible AI , proposes \nprinciples for the responsible management of AI systems that stakeholders may leverage. Part 2, \nOperationalizing Principles for responsible AI, identifies a series of actions for the government, the private \nsector and for research institutions that must be adopted to drive responsible AI (OECD.AI, 2023 [8]). \nSerbia’s Ethical Guidelines for Development, Implementation, and Use of Robust and Accountable AI \n(2022) set ethical standards all AI solutions should embrace (OECD.AI, 2023[8]). Singapore’s AI Ethics \nand Governance Body of Knowledge is based on the Infocomm Media Developme nt Authority’s (IMDA) \nModel AI Governance Framework. It is tailored for practical issues related to human safety, fairness , and \nthe prevailing approaches to privacy, data governance , and general ethical values (OECD.AI, 2023 [8]). \nThailand’s Ministry of Digital Economy and Society (DES) drafted the country’s first AI ethics guidelines \n(2021) for researchers, developers, and service providers engaging in tech development (OECD.AI, \n2023[8]). The AI Princi ples and Ethics for the Emirate of Dubai  (2019) aim to help AI developers, \ngovernment and society develop AI in a safe, responsible, and ethical way (OECD.AI, 2023[8]). \nAI-specific regulation is emerging in several jurisdictions \nExisting provisions in different fields of legislations already regulate AI systems.  But in  recent years, \ncountries have started codifying OECD AI Principles into binding , AI-specific legislative and regulatory  \nframeworks. This section provides an overview and comparative analysis of  selected legislative \ndevelopments across world regions. Further information is provided in Annex A and Annex B. \nCanada  \nCanada opted for separate regulations to address automated decision systems for the provision of public \nfederal services to the population, vis-à-vis AI systems aimed at trade and commerce.  \nSince 2019, Canada has implemented specific federal policy requirements around the use of automated \ndecision-making systems in the provision of services through the Directive on Automated Decision-Making. \nThe Directive establishes measures for algorithmic impact assessment, transparency, quality assurance, \nand recourse automated systems used in administrative decision-making. The Treasury Board of Canada \nSecretariat is obliged to review the Directive on a regular basis to account for technological and regulatory \nchange (the third review was completed in April 2023, when the amended Directive was published).  \nAs for the use of AI systems by the private sector in the digital economy, Canada has put forward a \ncomprehensive regulatory framework at the federal level, the Digital Charter Implementation Act (Canadian \nParliament, 2022 [12]). It includes a reform of the national data protection legislation and introduces \noverarching, mandatory rules on AI systems. The Digital Charter comprises three self -standing pieces of \nlegislation: Part 1, the Consumer Privacy Protection Act, aimed at upd ating the country’s personal data \nprotection rules to align them better with the digital economy; Part 2, the Personal Information and Data \nProtection Tribunal Act, creates a specialized administrative tribunal to enforce the Consumer Privacy \nProtection Act provisions also through fines; and Part 3, the Artificial Intelligence and Data Act (AIDA), the \ncentral piece of national legislation aimed at regulating the use of AI systems in the digital market  in the \ncourse of international and inter-provincial trade and commerce.  \n18    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nThe proposed AIDA’s approach is to ensure  the safe and responsible design, development, and \ndeployment of AI systems that respect Canadians’ values , and to establish an Artificial Intelligence and \nData Commissioner that administers and enforces the Act , while relying on sectoral regulators to enforce \nin their areas of responsibility. It establishes an impact-based approach that focuses on mitigating the risks \nof harm and bias of “high-impact” AI systems  (Fasken, 2022[13]). Its main objectives are: (1) to establish \ncommon requirements throughout Canada with respect to the design, development, and use of AI in the \nprivate sector; and (2) to proh ibit conduct in relation to AI systems that could result in serious harm to \nindividuals or their interests (Landry et al., 2022[14]).  \nThe Canadian Government recently published a companion document to the proposed AIDA (Canadian \nGovernment, 2023[15]) to clarify the legislation’s goals, approach, scope , and enforcement choices. For \nexample, AIDA does not specify the scope of high-impact systems, whereas its c ompanion document \nexplains that AIDA will cover systems that could significantly impact health, safety, and human rights. This \naligns with the proposed approach in the EU AI Act. It stresses that the proposed AIDA does not intend to \ndiscourage innovation or target private actors acting in good faith but rather “to regulate the most powerful \nuses of this technology that pose the risk of harm”. The AIDA companion document acknowledges AIDA’s \nalignment with the OECD AI Principles, the proposed EU AI Act, and the United States National Institute \nof Standards and Technology ( NIST) Risk Management Framework. It also recognizes the relevance of \ninternational coordination with these and other international partners such as the United Kingdom and the \nUnited States, to ensure regulatory interoperability in the global marketplace.  \nThe Digital Charter Implementation Act is currently under debate in the House of Commons  and will then \nproceed to the Senate. The new legal provisions are therefore projected to come into force as early as  \n2025 (Canadian Government, 2023[15]). \nIsrael \nIn November 2022, the Israeli Ministry of Innovation Science and Technology and the Ministry of Justice \npublished a “Draft Policy White Paper for Regulation and Ethics in the Field  of AI” for public consultation \n(Ministry of Innovation, Science and Technology and Ministry of Justice, the Office of legal counsel and \nlegislative affairs, 2022[16]). The white paper covers the main regulatory and legal challenges related to AI \nand makes regulatory policy recommendations.  \nDrawing on the OECD AI Principles, the white paper adopts non -binding AI ethical principles  to be \nconsidered when developing, using , and regulating AI. It calls for  sector-based regulatory ef forts (rooted \nin risk assessment and management approaches), rather than overarching sector -crossing regulation, \nencouraging regulators to develop their respective frameworks in a manner that is consistent with those of \nleading countries in the field.  The white paper also prioriti ses the use of \"soft\" regulation and advanced \nregulatory tools such as ethical principles, standards, recommendations for voluntary adoption or self -\nregulation. It further suggests adopting a gradual modular framework to develop re gulation by using tools \nfor controlled regulatory experimentation, such as sandboxes. Finally , the white paper calls for continued \nmulti-stakeholder dialogue to develop AI regulation. \nIn July 2022, the Office of Legal Counsel and Legislative Affairs (Depar tment of Economic Law) in the \nIsraeli Ministry of Justice published a report on the use of AI in the financial sector, compiled by an \ninterdisciplinary team of renowned researchers from Tel Aviv University. After reviewing the report, the \nfinancial regulators, the Ministry of Justice, the Ministry of Finance, and the Competition Authority decided \nto establish a joint taskforce to provide appropriate recommendations on these matters. The taskforce was \nestablished in December 2022, with a mandate to review th e regulatory and legal implications of the use \nof AI in the financial sector, to recommend steps to promote innovation in this sector , and to suggest \nrequired amendments to current regulation (Israeli government, 2023[17]). The taskforce includes members \nfrom the Ministry of Finance, the Ministry of Justice, the Bank of Israel, the Capital Market, Insurance and \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   19 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nSavings Authority, Israel Securities Authority , and the Competition Authority . It is expected to present its \nrecommendations by October 2023. \nJapan \nIn Japan, the Cabinet Office has published the Social Principles of Human -Centric AI (Japanese \nGovernment, 2019[18]). The Social Principles resulted from multi -stakeholder expert group discussion, set \nforth three basic philosophies (dignity, diversity and inclusion, and sustainability), as well as seven AI \nprinciples (human-centric, education and literacy, privacy, fair competition, security, innovation and fair, \naccountable and transparent). Thes e principles share some common elements with the OECD AI \nPrinciples since several members of the group have made contribution to the development of the OECD \nAI Principles. The Governance Guidelines for the Implementation of AI Principles (2022) summarise h ow \nto implement the Social Principles of Human -Centric AI decided by the Council for Integrated Innovation \nStrategy in 2019. The AI Governance Guidelines provide practical guidance for AI system operators and \ndevelopers (Ministry of Economy, Trade and Industry, 2022[19]). \nIn April 2023, Japan hosted the G7 Digital and Tech Ministers’ Meeting in Takasaki. The Ministers agreed \non the Ministerial Declaration, which emphasises the importance of: i) international discussions on the \ninteroperability between different AI governance frameworks , and ii) stock-taking of the opportunities and \nchallenges brought by generative AI. The Ministers also adopted the “G7 Action Plan for promoting global \ninteroperability between tools for trustworthy AI”  (MIC, 2023 [20]). The Ministerial discussion on AI was \nescalated to the Leaders’ discussion at the G7 Summit meeting in May, hosted in Hiroshima. The Leaders \nagreed to task their Ministers to establish the “Hiroshima AI process”, where G7  members continue the \ndiscussion on generative AI in an inclusive manner and in co -operation with the OECD and GPAI  by the \nend of 2023 (Ministry of Foreign Affairs of Japan, 2023[21]). In addition to these efforts on the governmental \nlevel, the G7 Data Protection and Privacy Authorities adopted a “Statement on Generati ve AI” at their \nTokyo roundtable meeting in June 2023 highlighting specific challenges related to data protection  in \ngenerative AI. \nWorking group meetings to advance for the Hiroshima AI process , supported by the  OECD, took place \nfrom June through September 2023, and will continue through the remainder of 2023. On 7 September, \n2023, G7 Digital and Tech Ministers issued a statement (MIC, 2023[23]) endorsing: 1) a report by the OECD \nsummarising a stocktaking of priority risks , challenges, and opportunities of generative AI based on \npriorities highlighted in the G7 Leaders’ Statement   (OECD, 2023 [23]), (2) ongoing work towards \ninternational guiding principles applicable for all AI act ors, (3) developing a code of conduct for \norganisations developing advanced AI systems, to be presented to the G7 Leaders , and (4) developing \nresponsible AI tools and best practices.   \nResponding to the emergence of generative AI and related international discussion advancements, Prime \nMinister Kishida announced the launch of the “AI Strategic Council” in May 2023. The Council brings \ntogether the AI experts and responsible Ministers in the Prime Minister’s office to identify the opportunities \nand challenges of generative AI, and to develop a whole-of-government policy program me to accelerate \nthe R&D and use of AI in Japan, including to lead the international discussion on rulemaking. The Council \npublished a preliminary summary of discussion and list of AI is sues in May 2023 (Japanese government, \n2023[22]).   \nUnited Kingdom \nIn a policy paper that delineates the country’s regulatory choice  on AI, the United Kingdom lays down a \ncontext-specific, sectoral approach to regulating AI. “Establishing a pro-innovation approach to Regulating \nAI”, presented to the Parliament on July 18, 2022, is a vertical approach that characterises regulation with \nthe following key principles: (1) context-specific; (2) pro-innovation and risk-based; (3) coherence; and (4) \n20    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nproportionate and adaptable  (UK Government, 2022 [23]). On March  29, 2023 , the United Kingdom \npublished its policy paper “AI regulation: A pro-innovation approach” (UK Government, 2023[24]).  \nThe government opted for what it calls a pro -innovation framework, based on the following four key \nelements: cross-sectoral principles, leveraging existing regulator expertise, context-specific approach, and \ncentral functions to drive coherence (AIGO, 2023 [25]). The policy paper establishes  five cross-sectoral \nprinciples for AI systems, based on the OECD AI Principles. It also recommends regulators to favour a soft \napproach instead of mandatory regulation , and leaves them to decide on the rules for each sector. This \nregulatory approach seeks to enable sectoral flexibility and adjustment as the technology evolves. Instead \nof defining AI, the paper opts to determine what it considers to be its core characteristics and capabilities. \nIt proposes that regulators take them into account when developing their own definitions of AI according \nto their specific domains or sectors.  \nThe United Kingdom’s approach differs significantly from the proposed EU AI Act: the former is highly \ndecentralised, focuses on actual risks and harms , relies on sectoral regulation , and favours voluntary \nmeasures and guidance over mandatory regulation (UK Government, 2022[26]).  \nOn March 3, 2023, the United Kingdom introduced the Data Protection and Digital Information Bill in the \nHouse of Commons (UK Parliament, 2023 [27]). Th is proposed bill brings an entire new section on \nautomated decision-making. It adopts a more business-friendly approach in comparison to the existing UK \nGeneral Data Protection Regulation (UK GDPR). Whereas the UK GDPR generally prohibits aut omated \nprofiling decision -making - for example, to obtain a job or a bank loan -, the proposed bill allows it by \ndefault, placing the responsibility to challenge such decisions and request a human review  on the \nindividuals affected . In addition, only activ ities posing high risks will have to carry out data processing \nrequirements (UK Parliament, 2023[27]), with the aim to reduce paperwork and compliance obligations for \nbusinesses.  \nUnited States \nThe United States has advanced federal AI policies on four fronts.  \nA horizontal framework, the Algorithmic Accountability Act  (AAA) (US Congress, 2022[28]), was proposed \nin the Senate on February 3, 2022. The US AAA establishes a horizontal framework for companies to \nassess the impact of automated decision systems they sell and use through impact assessments and post-\nmarket monitoring. The bill also aims to increase the transparency and traceability of automated decisions. \nThere is still uncertainty on whether the United States Senate and House will support the bill , which two \nmembers of Congress introduced . Other legislative packages are also being considered (e.g., proposals \nby Sen. Schumer (NY)), but their future path remains equally uncertain. \nGiven this context, Executive Branch policies have acquired major relevance in the United States. Two \nmajor documents of voluntary nature have shaped these policies: t he Blueprint for an AI Bill of Rights, a \nwhite paper published by the White House Office of Science and Technology Policy in October 2022 (US \nGovernment, 2022 [29]), and the AI Risk Management Framework (AI RMF), consisting of t echnical \nguidelines released by the National Institute of Standards and Technology (NIST) on January 26, 2023 . \nThe Blueprint aims to support the development of policies and practices that protect civil rights and promote \ndemocratic values in the development, deployment, and governance of AI systems (US Government, \n2022[29]). It establishes five principles: (1) safe and effective systems; (2) algorithmic discrimination \nprotections; (3) data privacy; ( 4) notice and explanation; and (5) human alternatives, considerations, and \nfallback, to mitigate risks to civil rights and democratic values posed by the use of automated systems \nacross sectors. The AI RMF issued by NIST incorporates similar rights-preserving principles into technical \nguidelines and standards , and builds on the OECD AI Principles and the OECD Framework for the \nClassification of AI Systems  (OECD, 2022[34]) to propose a framework to map, measure , and manage AI \nrisks. \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   21 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nBinding federal regulations in force are mostly sectoral or domain -specific, for existing regulations and \nemerging regulations expressly targeting AI and related technologies, both of which may have the \nrecommendations of the Blueprint or the AI RMF incorporated into their application. An important piece of \nAI-specific policy is Executive Order 13960, which mandates all design, development, acquisition, and use \nof AI systems by the US federal government to adhere to eight principles (Executive Office of the President, \n2020[31]). Many of these principles, including lawfulness, safety, security, and resilience, understandability, \nresponsibility and traceability, transparency, and accountability, overlap signifi cantly with the OECD AI \nPrinciples. In addition, Executive Order 14091 includes provisions to root out bias in the design and use of \nAI, and protect the public from algorithmic discrimination by instructing the Federal Government, when \ndesigning, developin g, acquiring, and using artificial intelligence and automated systems, to do so, \nconsistent with applicable law, in a manner that advances equity. Binding AI implementation guidance for \nfederal agencies from the White House Office of Management and Budget is forthcoming (Executive Office \nof the President, 2020[31]).  \nFinally, on July 21, 2023, the White House secured voluntary commitments from seven leading AI \ncompanies to manage the risks posed by AI and to help move toward safe , secure, and transparent \ndevelopment of AI technology (The White House, 2023[32]). The White House also announced it would be \ndeveloping another Executive Order and will pursue bipartisan legislation on AI and responsible innovation.   \nEuropean Union \nThe EU is advancing the EU AI Act (“AI Act”), proposed by the European Commission in 2021 (European \nCommission, 2021a [33]). The proposed Act follows a risk -based approach and presents a uniform, \nhorizontal legal framework for AI to ensure legal certainty (European Commission, 2021a[33]).  \nThe proposed EU AI Act is part of the European Coordinated Plan on Artificial Intelligence (2018, revised \nin 2021 (European Commission, 2021b [34])), aimed at shaping Europe as a major world player in AI \ninnovation, embedding it with human-centric, trustworthy, secure, and sustainable values, and addressing \nrisks through harmonised regulation. This regulation emerged as one of the major outputs of the Plan. It \nseeks to establish an overarching regulation to avoid fragmented regul ation across Member States. The \nproposed AI Act seeks to balance the EU’s goals to accelerate innovation and to mitigate the risks of AI \nsystems, including potential threats to European values.  \nThe proposed EU AI Act introduces a classification of AI systems based on the levels of risk they represent. \nRisks subject to the AI Act are those for health and safety, adverse impact on fundamental rights (as  well \nas on the environment, democracy and rule of law, in the compromise text adopted by the European \nParliament), or those that can result from the areas of application of AI specified in Annex III of the AI Act \n(European Commission, 2021a [33]). Risks are classified as unacceptable (prohibited), high (subject to \nconformity assessment procedures before placing an AI system on the market, as well as to post -market \nmonitoring); limited (subject to transparency obligations) , and minimal  or no risk  (not covered by the \nRegulation). The proposed EU AI Act allows some level of flexibility in the specification of AI systems that \npose a high risk. As such, rather than listing them in the text of the regulation, an annex (Annex III) is \nproposed for this purpose. This choice will enable the European Commission, when the conditions of Article \n7 are satisfied, to amend Annex III and adequate it to new or unforeseen uses, as well as to emerging \ntechnologies that may pose significant risks.  \nNegotiations in the Council of the European Union led to a Compromis e Version of the proposed EU AI \nAct on December 6 , 2022 (European Council, 2022 [35]).  In June 2023, negotiations at the European \nParliament led to a compromise text which notably adopts the OECD definition of AI systems  (Bertuzzi, \n2023[36]). The inter -institutional negotiations between these institutions and the European Commission \n(“trilogue”) are expected to lead to a final proposal in the second half of 2023, with the AI Act set to come \ninto force at the end of 2023 or in early 2024.  \n22    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nThe original text of the proposed EU AI Act did not include general purpose AI (that is, AI systems that can \nbe used for several purposes). Following negotiations at the Council of the European Union, general  \npurpose AI systems were included in a compromise version of the AI Act. As per the text, the European \nCommission will carry out consultations and impact assessments, to determine when the requirements for \nhigh-risk AI systems would be equally applicable to general purpose AI systems.  \nThe Parliament’s compromise text (European Parliament, 2023 [37]) introduces a new definition of \n\"foundation model\", an AI model that is trained on broad data at scale, designed for generality of output, \nand that can be adapted to a wide range of distinctive tasks. Generative AI is a subcategory of AI foundation \nmodels. Generative AI is also defined in a newly inserted Article 28a: “foundation models used in AI \nsystems specifically intended to generate, with varying levels of autonomy, content such as complex text, \nimages, audio, or video (“generative AI”)”. \nCouncil of Europe \nFrom 2019 to 2021, the Council of Europe’s Ad Hoc Committee on Artificial Intelligence (CAHAI) examined \nthe feasibility and potential  elements of a possible legal framework to ensure that AI is used to promote \nand protect CoE’s standards. CAHAI was succeeded by the Committee on Artificial Intelligence (CAI) in \n2022 that built on the work of its predecessor and began to work on a “Framework Convention Artificial \nIntelligence, Human Rights, Democracy and the Rule of Law” (Council of Europe, 2023[79]). The objective \nof the drafting process is to focus on common principles ensuring the seamless application and respect for \nhuman rights, democracy, and the rule of law in a context where AI systems assist or replace human \ndecision-making. CAI is expected to  finish drafting its framework by the end of 2023 and send it to the \nCommittee of Ministers for approval. Once approved, the Convention will become an international legal \ninstrument binding to its signatories. \nBrazil \nIn Brazil, a committee of legal practitioners and experts was charged by the government to draft a proposal \nfor an AI regulation. The findings and the report of the committee resulted in the proposal of a Bill (Bill nº \n2338/2023), whose main aspects are: human rights -oriented approach; risk cla ssification of AI systems; \nAI governance and risk-based-approach; the establishment of a supervisory authority; rules for civil liability; \nfostering of innovation by promoting regulatory sandboxes, among others (OECD.AI, 2023[8]). \nThe proposed Brazilian AI Bill differs from the proposed EU AI Act in which it addresses the rights of those \naffected by AI systems and puts them upfront (Chapter II). Those rights are grounded on principles such \nas human -centred AI; human rights and de mocratic values; personality, privacy and data protection; \nenvironment protection and sustainable development; equality, non -discrimination, plurality and labour \nrights; free enterprise, competition and consumer protection; informative self -determination; access to \ninformation and education; and the promotion of R&D to stimulate innovation (Art. 2).  \nChapter IV addresses the governance of AI systems with the same approach as the  proposed EU AI Act \nand the proposed Canadian AIDA. The proposed Brazilian Bill establishes duties for providers and \ndeployers of AI systems, including transparency related to their use and internal governance. Conformity \nassessment procedures are required for high-risk systems.   \nChapter V of the proposed Brazilian Bill addresses civil liability and is applicable when the violation of rights \nleads to damages, whether patrimonial, moral, individual or collective in nature. The rights of those affected \nby AI systems and the resulting civil liability are established regardless of the lev els of risk: those not \nclassified as unacceptable or high -risk lead to a legal presumption of guilt on the part of the agent who \nallegedly caused the damage, thus shifting the burden of proof in favour of the allegedly injured party.    \nThe Bill was submit ted by the President of the Brazilian Senate in May 2023, and it is currently under \nanalysis before the National Congress (OECD.AI, 2023[8]). \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   23 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nChina \nChina is advancing AI policies with both soft principles and hard rules, in addi tion to incentives for private \nactors and local governments to innovate (CAIDP, 2021[39]); (Roberts et al., 2021[40]). In 2019, the Chinese \nMinistry of Science and Technology issued ethical norms for the use of AI in the country, although several \nreports have expressed  concern that AI systems ’ purpose and application  in China, notably in facial \nrecognition applications, negatively impact human rights, which may not align with the OECD and G20 AI \nPrinciples (CAIDP, 2021 [39]); (Shaughnessy et al., 2023 [41]). The Ministry of Industry and Information \nTechnology published  a white p aper acknowledging the risks posed by AI systems  and proposing  an \noverarching “trustworthy AI framework” for the implementation of ethical principles and the creation of \nindustry-wide standards of trust.  The white paper calls for the acceleration  of legislation and supervision \nof trustworthy AI by the central government and for regulatory experimentation through the use of \nsandboxes and other methods. Finally, in 2022, the Central Office of the Communist Party of China and \nthe Office of the State Council issued an Opinion on Strengthening the Ethics and Governance of Science \nand Technology (Chinese Government, 2022[42]). This document in intended to have broad application and \nsignals the creation of additional regulation to determine the scope of its implementation.  \nIn China, there are also efforts on the provincial level to develop AI regulation. For example , in 2022, the \nShanghai Municipal People ’s Congress introduced  the Shanghai Regulations on Promoting the \nDevelopment of the AI Industry  (Holistic AI, 2023 [43]). The Shanghai Regulations are the first provincial -\nlevel regulations in the field of AI. They set a graded management system and enforce sandbox supervision \nwhereby companies have a designated space to test and explore AI  technologies (Holistic AI, 2023 [43]). \nSimilarly, Shenzhen has its Regulation for the Promotion of the AI Industry. The Regulation sets a risk -\nbased approach to encourage governmental organisations in Shenzhen to be at the forefront of AI adoption \nand development by increasing financial support for these endeavours (Holistic AI, 2023[43]). \nA comparative analysis of similarities and differences in AI-specific regulations \nacross selected jurisdictions \nSome jurisdictions are taking a cross-sectoral “horizontal” approach to AI regulation, \nwhile others consider a more sectoral or “vertical” approach \nCanada and the European Union have proposed to regulate AI systems across domains and applications, \nbuilding an AI-specific regulatory framework applicable to all sectors.  They have proposed a “horizontal” \nregulatory approach, so as to establish minimum standards of mandatory application across industries. \nStandards will clearly determine how businesses should manage the technology, enabling them to do it \nresponsibly, while simultaneously enhancing consumer trust in their use and creating safeguards to protect \nindividuals and groups against risks to health, safety, and human rights.  \nOther jurisdictions consider a more sectoral or “vertical” approach , developing regulations by sector or \ndomain. This is the case in Israel, the United Kingdom, the United States, and in China.  \nThe United Kingdom  laid down a context -specific, sectoral approach  on AI  in the policy paper that \ndelineates the country’s regulatory approach  (OECD.AI, 2023 [44]). It establishes cross -sectoral, non -\nbinding principles while leaving regulators the task of implementing, regulating, and enforcing them in their \nrespective sectors and domains. It has also expressly called regulators to favour “lighter-touch options, \nsuch as guidance or voluntary measures” as a first choice  (OECD.AI, 2023 [8]). Israel follows a similar \napproach, refraining from enacting broad, horizontal legislation and rather preferring a sectoral based \napproach (Ministry of Innovation, Science and Technology and Ministry of Justice, the Office of legal \ncounsel and legislative affairs, 2022[16]). \n24    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nThe United States has some regulation that applies to federal government activities around AI, but for \nother actors it does not have AI-specific cross-regulatory frameworks at the federal level at this stage. It \nhas so far relied on voluntary frameworks, such as the White House Blueprint for AI Bill of Rights (US \nGovernment, 2022 [29]) and NIST’s technical gu idelines and voluntary AI Risk Management Framework  \n(National Institute of Standards and Technologies - US Department of Commerce, 2023 [33]). Sectoral \nregulation is still incipient. For example, the United States Department of T ransportation announced it will \ndevelop regulation that advances innovation for automated vehicles while accounting for safety in the \nintegration of new technologies (US Government, 2021[45]). In addition, individual state (e.g., Colorado and \nIllinois) and local governments (e.g., New York) have undertaken initiatives to regulate limited and specific \nuses of AI such as for recruitment, insurance , and auditing. The AI industry has even adopted its own \ninitiatives, such as the Alg orithmic Bias Safeguards for the Workforce, a questionnaire for employers to \nassess algorithmic discrimination prior to acquiring software for the evaluation of their workers  (Data & \nTrust Alliance, 2022[46]).  \nChina has advanc ed domain -specific regulation and standards in recent years, in areas such as data \nprotection (Chinese Government, 2021 [47]) and automated driving  (OECD.AI, 2023 [8]). Another recent \nregulatory initiative is the Internet Information Service Algorithmic Recommendation Management \nProvisions (Zhang, 2021 [48]), which establish rules on the use of algorithms that recommend and \ndisseminate online information. They prohibit the dissemination of misinformation and place legal liability \nfor algorithmic-based recommendations on the digital information providers.  \nBoth sectoral and cross-sector regulations usually exist within one country:  \n• Canada and the European Union have aimed for a higher level of harmonisation of definitions, the \nscope of application and centralisation of enforcement at the national level. This can be combined with \ntargeted sector or domain -specific initiatives (laws, guidelines, or standards) to take into account \nspecific characteristics and needs.  \n• Israel, the United Kingdom and the United States have opted for horizontal, overarching strategies \nand ethical principles to set a common ground for sectoral regulations, decentralising competence to \ngain agility when accounting for changes of scenarios. In these jurisdictions, courts will have a \nfundamental role in harmonizing interpretation across regulators when appropriate.  \nChina also applies a horizontal approach to bring its vertical regulations into coherence. However, it differs \nfrom the United States and the United Kingdom with regards to the level of compliance required. The latter \ncountries favour voluntary horizontal princ iples and guidance, as well as inter -ministerial exchanges to \nfoster coordination towards coherence  (UK Government, 2022 [23]), whereas China seems to prefer \ncentralized, mandatory horizontal rules. For example, it has created e thical norms applied to AI and \nconferred the National New Generation Artificial Governance Specialist Committee exclusive competence \nfor the publication, interpretation, and guidance on the implementation of the norms (Chinese Government, \n2021[47]).  \nNew AI-specific regulation also calls for new governance and enforcement bodies \nIn Canada, the Minister of Innovation, Science and Industry would be in charge of the governance and \nenforcement of the proposed AIDA. AIDA intends to create a new AI and Data Commissioner, which would \neffectively carry out administration and enforcement; track potential systemic effects of AI systems to \ninform decisions; and establish coordination across the government to ensure consistency in the \nimplementation of the proposed AIDA (Canadian Government, 2023 [15]). Enforcement for criminal \nviolations would remain under the competence of the Public Prosecution Service of Canada.  \nIn February 2023, Israel established a government centre for AI regulation, which will assist and coordinate \nthe work of sectoral regulators. \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   25 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nThe United Kingdom  preconises policy coordination to ensure policy coherence across sectors. The \nalready existing Digital Regulation Cooperation Forum , which fosters co -operation across national \nregulatory authorities on online matters, is likely to steer co-operation on AI policy without excluding other \ninitiatives. The Alan Turing Institute has called attention to the current lack of capacity, understand ing of \nAI and co-operation among regulators. It proposes the creation of a dedicated regulatory hub to address \nthese issues (Aitken et al., 2023[49]). \nUnder the proposed EU AI Act, the European Union will create the European AI Board or Office, a new \nauthority to ensure EU-wide oversight of the AI Act. As with the GDPR, national authorities will be charged \nwith enforcement and establishing administrative sanctions according to the baseline established in the \nproposed EU AI Act. At this initial stage, some EU member states  (among which the Netherlands, see \nbelow) have opted to create a unit in charge of algorithms inside their national Data Protection Authorities. \nIn addition, each EU member state will determine whether, under what conditions, and to what extent civil \nand criminal sanctions will be applicable. \nNational courts of EU member states already have the competences  to establish non-contractual civil \nliability and to sanction manufacturers of defective products for the harms they cause to consumers under \nthe existing EU Product Liability Directive. Two proposals published by the European Commission in \nSeptember 2022 – one to update the Product Liability Directive, another to create an AI Liability Directive \n– aim to equip national courts to establish liability through out the lifecycle of an AI system. Courts will \nfurther be able to establish liability and sanction not only for manufacturers outside of the EU, but also \noperators and users of AI systems.  \nChina established a new, centralised data regulator, the National Data Administration (2023), which is in \ncharge of the creation and enforcement of rules on data, investigation of algorithmic manipulation and \ndeepfakes, although the scope of its regulatory power rema ins unknown (Yang, 2023[50]). The National \nData Authority will help advance smart cities and governmental digital services , facilitate data sharing \nacross governmental agencies, and improve digital infrastructure (Yang, 2023[50]).  \nTechnical standards will be crucial to implementing trustworthy AI \nStandard setting bodies are stepping up to create technical standards for AI systems.  \nIn the United States, NIST has established voluntary guidelines and technical standards for AI risk \nmanagement (NIST, 2023[51]). These standards build on the OECD Framework for the Classification of AI \nSystems and have received broad support. Given their voluntary nature, their effectiveness will depend on \nthe extent to which organisations adopt and implement them.  \nIn 2022, Türkiye established the AI Mirror Committee to effectively oversee the standardization efforts \nwithin the field of AI, and to ensure that the country's perspectives and evaluations are duly incorporated. \nThis committee brings together representatives from the public sector, private sector, academia, and from \nNon-Governmental Organisations (NGOs). By convening diverse expertise a nd perspectives, the Mirror \nCommittee aims to represent the interests and viewpoints of Türkiye, contributing to the development of \nrobust and inclusive standards in AI.  \nIn the European Union, the European Committee for Electrotechnical Standardization (C EN-CENELEC) \nwill develop technical standards to operationalise the EU AI Act. The International Organization for \nStandardization (ISO) has developed ISO/IEC 23053  (2022), establishing a Framework for Artificial \nIntelligence Systems Using Machine Learning , in addition to ISO 31000 (2009) for risk management that \nis applicable across sectors and activities (ISO, 2022[52]; ISO, 2009[53]).  \n26    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nStandard-setting organisations play a critical role in building consensus among AI actors. They can also \nhelp to promote interoperability between jurisdictions and offer market certainty for those using or \ndeveloping AI systems in different parts of the world. Promoting wide participation from relevant parties in \nthe establishment of  such standards will be critical, to ensure different perspectives are considered for \neffective risk management. To increase participation in AI standards development, the United Kingdom \nestablished the AI Standards Hub (Box 2.1).  \nGovernments and innovators are testing innovative AI solutions in controlled \nenvironments  \nAn increasing number of countries use regulatory sandboxes (Figure 2.1), i.e., spaces in which authorities \nengage firms to test innovative products and services that challenge existing legal frameworks  (OECD, \n2023[59]). Germany’s Federal Ministry for Economic Affairs and Climate Action (BMWK) has launched the \nRegulatory Sandbox Strategy (2019) with the aim of fostering digital innovation and further developing the \nregulatory framework for AI and other digital technologies (German Government, 2022 [56]). Promoting \nregulatory experimentation is also one of Israel’s national AI strategy’s key tools to ensure safe and \ninnovative AI deployment. Spain created an AI regulatory sandbox in 2022 as the first pilot programme to \ntest the future proposed EU AI Act. The initiative is in collaboration with the EC and is seeking to onboard \nother EU member countries (OECD, 2023[59]). The key distinguishing feature of the Spanish AI regulatory \nsandbox is that it was established to test a regulation that has not yet entered into force. The objective of \nthis pilot programme is to test the proposed regulatory framework with real AI applications to assess how \nboth the regulation and applications respond, and to suggest modifications or explanatory guidelines.  \nThe United Kingdom launched two regulatory sandboxes through the Financial Conduct Authority (FCA) \nand the Information Commissioner’s Office (ICO). The FCA Sandbox (2016) focuses on FinTech whi le \nalso admitting AI -related solutions applied in the financial sector (OECD, 2021[57]). Inspired by the ICO \nregulatory sandbox, the Norwegian Data Protection Authority ( Datatilsynet) Regulatory Sandbox (2020) \naims to promote ethical, privacy-friendly, and responsible innovation within AI. The Norwegian sandbox \nfollows the principles of responsible AI as proposed by the EU High Level Group on Trustworthy AI, and \nBox 2.1. The United Kingdom AI Standards Hub \nAs part of the UK’s National AI Strategy, the Hub’s mission is to advance trustworthy and responsible \nAI, focusing on standards’ role as governance tools and innovation mechanisms. The Alan Turing \nInstitute leads the AI Standards Hub in partnership wi th the British Standards Institution (BSI) and the \nNational Physical Laboratory (NPL). The initiative is supported by the UK Government through the \nDCMS Digital Standards team and the Office for AI. \nThe AI Standards Hub aims to help stakeholders navigate the rapidly growing range of AI-related activity \nby an increasing number of Standards Development Organisations (SDOs) around the world. It actively \nparticipates in international AI standardisation and informs the direction of these efforts. Dedicated to \nknowledge sharing, community and capacity building, and strategic research, the Hub brings together \nindustry, government, regulators, consumers and civil society, and academia to shape debates about \nAI standardisation and promote sound, coherent, and effective standards. These standards inform and \nstrengthen AI governance practices domestically and internationally, and increase multi -stakeholder \ninvolvement, while facilitating the assessment and use of relevant published standards.  \nSource: (AI Standards Hub, 2023[54]). \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   27 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nin 2020-2023, it selected twelve projects  (Datatilsynet, 2020[61]), (Datatilsynet, 2021[61]). Another example \nof sandboxes in the financial sector is represented by the FinTech Regulatory Sandbox established by the \nMonetary Authority of Singapore’s (MAS), which has facilitated the live testing of AI applications such as \nthe Kristal.AI case (Singapore Government, 2023[58]).  \nFigure 2.1. Regulatory sandboxes in AI: definition, risks, opportunities and policy considerations  \n \nSource: (OECD, 2023[59]), “Regulatory sandboxes in Artificial Intelligence”, https://doi.org/10.1787/8f80a0e6-en. \n\n28    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nThe OECD Recommendation of the Council on Artificial Intelligence  (OECD, 2019 [65]) identifies five \ncomplementary values-based principles for the responsible stewardship of trustworthy AI: (1.1) inclusive \ngrowth, sustainable development and well -being, (1.2) human-centred values and fairness , (1.3)  \ntransparency and explainability, (1.4) robustness, security and safety, and (1.5) accountability.  \nThese principles have been endorsed by 46 countries (OECD and partner economies) as the core values \nto lead the trustworthy deployment, development, and use of AI. When developing national AI strategies, \nmost countries refer to these guiding principles. National and international ethics frameworks and principles \nalso largely embed these principles. Finally, emerging AI-specific legislation require the implementation of \nthe five OECD AI values-based principles. \nThe following sections describe practical initiatives for the implementation of the OECD value s-based \nprinciples, based on an allocation framework of policy initiatives to each principle.  This framework \nenhances the practical use of the values-based principles by defining them in a clearer and more practical \nway. Multiple principles can be attributed to each  policy and is desirable in most cases, since this means \nthat a policy considers several principles.  \nThe following sections include policy examples for each principle. While the examples are drawn from the \ndatabase of national AI policies, the rapid development of AI policies and regulations makes it challenging \nto be exhaustive. This collection is therefore illustrative.   \n3 Implementing the OECD AI values-\nbased principles  \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   29 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 3.1. Breakdown of issues covered by policy initiatives linked to OECD AI values-based \nprinciples \n \n1.1 Inclusive \ngrowth, \nsustainable \ndevelopment and \nwell-being\nInclusive growth\nInitiatives targeted at reducing economic, social, and gender inequalities\nBeneficial outcomes through multidisciplinary and multi-stakeholder collaboration\nBeneficial outcomes through social dialogue, citizen consultation, and inclusion of underrepresented \npopulations\nSustainable \ndevelopment Initiatives to promote the use of AI for environmental sustainability\nWell-being Initiatives to augment human capabilities and enhancing creativity\n1.2 Human-centred \nvalues and \nfairness\nHuman-centred \nvalues\nInitiatives to align values by promoting human righs and human-centred values and through \nmandatory HRIAs\nInitiatives to protect privacy\nQuality labels and certifications to promote human-centred values\nFairness Initiatives to reduce AI bias\n1.3 Transparency \nand explainability\nTransparency \nand explainability\nInitiatives requiring disclosure and information about use of AI systems\nInitiatives to provide information on AI functioning\nInitiatives to provide information on factors and decision processes and enable redress seeking from \ndecision\n1.4 Robustness, \nsecurity, and \nsafety\nRobustness, \nsecurity and \nsafety\nRisk management approaches\nInitiatives to maintain records of data characteristics for traceability\nPrevent unreasonable safety risks by AI systems through laws and regulations & determination by \ngovernments how these laws apply to AI systems\n1.5 Accountability Accountability\nLegislation that requires the documentation of the proper functioning of the AI systems throughout \ntheir lifecycle\nCodes of ethical conduct and practical technical tools\nIndependent oversight bodies to audit the use of algorithms\n30    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nInclusive growth, sustainable development, and well-being (Principle 1.1) \n “Stakeholders should proactively engage in responsible stewardship of trustworthy AI in pursuit of beneficial \noutcomes for people and the planet, such as augmenting human capabilities and enhancing creativity, \nadvancing inclusion of underrepresented populations, reducing econo mic, social, gender and other \ninequalities, and protecting natural environments, thus invigorating inclusive growth, sustainable development \nand well-being.” \nAI has the potential to increase countries’ productivity and lead to economic growth. Most countries have \nrecognised this and are trying to boost AI research and development (R&D), infrastructure, capacities, and \ntools through diverse initiatives. However, AI systems could also perpetuate existing inequalities and have \ndisparate impact on vulnerable an d underrepresented populations such as ethnic minorities, women, \nchildren, the elderly , and the less educated or low-skilled. This principle  therefore calls for countries to \nsteer AI development, deployment , and use in a way that empowers all members of so ciety. If AI is not \ndriven towards societal benefit at large  and AI policies are not developed in an inclusive way , there is a \nrisk that economic growth is unequal and endangers the environment (OECD, 2023[66]). \nNational approaches to implementing principle 1.1 Inclusive growth, sustainable \ndevelopment and well-being \nTo achieve inclusive growth, sustainable development and well-being, governments are pursuing different \napproaches. Most national AI strategies and AI ethics frameworks or guidelines for the implementation of \nAI refer to this principle. At the policy level, countries have launched initiatives to ensure vulnerable groups \nin the population are involved in and benefit from the development of AI systems, either thr ough targeted \ninitiatives or in policy design. Governments are also funding projects that use AI to address environmental \nchallenges (Figure 3.2). \nFigure 3.2. Select national policies that implement OECD AI Principle 1.1 on inclusive growth, \nsustainable development and well-being \n \n• France's \"IA Booster\"\n• United Kingdom's \"Women in AI and Data Science\"\nReducing economic, social and \ngender inequalities\n• Canada's \"Quebec AI Forum\"\n• Colombia's \"Coordination Bodies for AI Policy Implementation\"\nMultidisciplinary and \nmulti-stakeholder collaboration\n• Chile's \"Participation Process on AI\"\n• United States' \"AIM-AHEAD Program\"\nSocial diaogue, citizen consultation \nand inclusion of underrepresented \npopulations\n• Germany's \"AI Lighthouses for the Environment, Climate, Nature \nand Resources\"\n• European Commission's \"Destination Earth (DestinE)\"\nPromote the use of AI for \nenvironmental sustainability\n• Germany's \"AUTONOM - Performing Arts and AI\"\n• Türkiye's \"Breast Cancer Detection with AI\"\nAugment human capabilities and \nenhancing creativity\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   31 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nInclusive growth \nInitiatives targeted at reducing economic, social, and gender inequalities \nAustralia is addressing inequalities across society through progressing practical initiatives that are \nunderpinned by Australia’s AI Ethical Principles (which are built upon OECD values). Program mes such \nas the National Artificial Intelligence  Centre (NAIC) provide a country -level perspective on Australia’s AI \ncapability and capacity and support the uplift in AI skills across the nation. The NAIC will be supported by \na network of Responsible AI Centres that will be located throughout the countr y, providing a flow of \ncontemporary knowledge and information from industry leaders directly to business to grow ethical AI \nprincipals at a grassroots level. In building this pipeline of capacity, both programs aim to address how \nResponsible AI can be directly used to have positive impact on significant regional, social, economic, and \ngender inequalities. \nFrance’s IA Booster ’s mission is to  reduce social and economic inequalities by supporting small - and \nmedium-sized enterprises (SMEs) by accelerating the digitalisation of their activities through AI solutions. \nThe programme is tailored to meeting the specific needs of each company and provides support throughout \nthe entire transformation process, from the initial audit phase to selecting and implementing the appropriate \nsolution. The program me also considers the evolution of workstations, job roles, and necessary skills to \nensure a seamless transition (French Government, 2021[61]).  \nIn the United Kingdom, the Alan Turing Institute's public policy programme’s Women in Data Science and \nAI initiative provides a  notable example of reducing gender inequalities. They collaborate with policy  \nmakers and industry stakeholders to provide practical insights and recommendatio ns to address various \nethical, economic, and governance -related issues arising from AI inequalities. Their approach involves \nthree tiers: Firstly, they map the participation of women in data science and AI both in the UK and globally, \nwith the goal of incr easing the number of women in these fields ; secondly, they examine diversity and \ninclusion in online and physical workplace cultures ; l astly, they explore how the gender gap affects \nscientific knowledge and technological innovation while promoting responsible, gender-inclusive AI design \n(The Alan Turing Institute, 2023[62]). \nBeneficial outcomes through multidisciplinary and multi-stakeholder collaboration \nCanada’s Quebec AI Forum uses AI as a lever for the economic and social development of Quebec. It \nfollows a collaborative approach. In doing so, it rallies and mobilises a wide array of stakeholders around \ncommon projects and carries out monitoring and strategic thinking activities. The Forum works on several \nprojects to increase the international competitiveness of Quebec’s AI solutions providers and to promote \nthe responsible adoption of AI by all organisations, especially SMEs. Moreover, it supports government \nstakeholders in their efforts to adopt AI in an ethical and socially responsible manner (Forum IA Quebec, \n2023[63]). \nColombia’s Coordination Bodies for AI Policy Implementation represent an illustrative example of \ncoordinating AI policies among different stakeholders. These bodies con sist of the “Technical Committee \nfor Transformation and Digital Economy” and the “Presidential Advisory Office for Digital Transformation”, \nboth public entities dedicated to promoting, coordinating, and supporting the implementation of AI policies \nacross national and local public entities in Colombia. Additionally, they facilitate decision -making related \nto digital transformation and the digital economy and provide guidance for developing the digital ecosystem \namong public entities, the private sector, academia, and the national government (OECD.AI, 2023[8]). \nGermany's initiative “Civic Coding – Innovation Network AI for the Common Good” sets an example for \ninter-ministerial and multi-stakeholder collaboration in the field of common good-oriented AI. It was created \nas a joint effort by the Federal Ministry of Labour and Social Affairs (BMAS), the Federal Ministry for Family \nAffairs, Senior Citizens, Women and Youth (BMFSFJ) , and the Federal Ministry for the Environment, \n32    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nNature Conservation, Nuclear Safety and Consumer Protection (BMUV). The network strengthens the data \nand AI competencies of civil society, enable s interdepartmental support programmes and measures, and \nthus promote the societal use of AI for the common good (Civic Coding, 2018[64]). \nSince February 2022, Korea has been operating a multi -stakeholder AI Ethics Policy Forum (with \nparticipants from industry, academia, education circles, legal circles , and civil society) to discuss ethical \nissues induced by the development of AI technologies and to form a social consensus on how to build trust \nin AI. There are three expert committees (ethics, technology, education) within the forum to facilitate the \nconsensus building. The forum also develops  practical tools such as a self -assessment checklist for AI \nEthics Guidelines and Trustworthy AI Development Guidelines. \nBeneficial outcomes through social dialogue, citizen consultations and inclusion of \nunderrepresented populations \nThe Austrian National AI Strategy was developed and implemented with the involvement of more than \n160 experts from a variety of disciplines (technology, economics, natural sciences to law, social sciences, \nor educational sciences) and civil society organisations. These organisations include social partners such \nas the Chamber of Labour and the Chamber of Commerce, which have historically played an important \nrole in Austrian policymaking. Mobilising civil society organisations in the design of AI policies can be seen \nas a positive continuation of traditional policy design patterns and as being inclusive of the interests of \ndiverse social groups (Austrian Government, 2021[66]). \nIn 2020, Canada's Advisory Council on AI launched the Public Awareness Working Group to explore public \nawareness of and trust in AI. The Working Group partnered with the Canadian Institute for Advanced \nResearch and Algora Lab to conduct virtual workshops across Canada as part of the Open Dialogue: AI \nin Canada initiative. The workshops engage d the public in discussions about AI, its potential uses, and \nassociated risks to achieve regional representation and inclusivity for marginalized populations and youth \n(Government of Canada, 2023[67]). \nChile's Participation Process on AI provides a strong example of how citizens' voices can be taken into \naccount in AI policy design. The Science, Technology, Knowledge, and Innovation Ministry launched a \nprocess to collect the visions, perceptions, opinions, and concerns of people and organisations regarding \nthe use and development of AI in Chile. This process foster s discussions about AI opportunities and \nchallenges in Chile and diffuses AI-related knowledge (OECD.AI, 2023[8]). \nMexico’s National Alliance of Artificial Intelligence (ANIA) was launched on April 21, 2023, by the Mexican \nSenate as an inclusive, open, plural, and objective space for the analysis of advances , opportunities, \nchallenges, and risks of the use of AI in differen t sectors and by different individuals, organisations, and \nsociety in general. It seeks to recognize and strengthen the AI ecosystem in Mexico and maintain an open \ndialogue on AI and its impacts. It uses a comprehensive, plural istic, and multidisciplinary perspective, \nincluding the participation of different stakeholders and society in general. Its main objective is to deploy \nAI for the benefit of humanity and as a transversal axis for the sustainable development of Mexico.  \nFurthermore, ANIA operates on the  following principles: democratising AI discussions, engaging multiple \nstakeholders, being plural istic and non -partisan, encouraging collaboration and shared responsibility, \nadopting a multidisciplinary and transdisciplinary approach, and promoting a multi sectoral vision for the \npresent and future of AI. \nScotland’s AI Alliance commissioned the Democratic Society to develop AI Co -Creation Public \nEngagement Workshops in Edinburgh, Inverness, and online. The programme developed design principles \nand a participatory decision tree on how people in Scotland should engage in future AI decision making. \nParticipants did not need to have any knowledge about AI nor have AI skills (Democratic Society, 2022[65]). \nIn 2021, the United Kingdom ’s Alan Turing Institute conducted the AI Ecosystem Online Survey in \ncollaboration with the AI Council to gather the perspectives of individuals involved in the AI ecosystem, \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   33 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nincluding researchers, developers, and users of AI technologies. The survey aimed to inform the Office for \nAI's development of the National AI Strategy. With over 400 respondents, the survey stands as an excellent \nexample of incorporating a broad range of diverse voices into AI policy design (OECD.AI, 2023[8]). \nThe United States ’ National Institutes of Health’s AI/Machine Learning Consortium to Advance Health \nEquity and Researcher Diversity ’s (AIM-AHEAD) objective is to enhance diversity in the development of \nAI/ML models , and to improve their capacities to address health disparities and inequities. The AIM -\nAHEAD Coordinating Centre represents the heart of the AIM-HEAD consortium and consists of institutions \nand organi sations dedicated to serving underrepresented and underserved groups affected by health \ndisparities. The centre consists of four cores: (1) the Administration/Leadership Core, which is responsible \nfor leading, recruiting, and coordinating the AIM-AHEAD Consortium; (2) the Data Science Training Core, \nwhich assesses, develops, and implements data science training curricula;  (3) the Data and Research \nCore, which prioritizes and addresses research needs to create an inclusive basis for AI/ML; and (4) the \nInfrastructure Core, which evaluates data, computing, and software infrastructure to facilitate AI/ML and \nhealth disparities research (OECD.AI, 2023[8]). \nSustainable development \nInitiatives to promote the use of AI for environmental sustainability \nSince 2019, the AI Lighthouses for the Environment, Climate, Nature and Resources initiative, funded by \nthe German Federal Ministry for the Environment, Nature Conservation and Nuclear Safety (BMUV), has \nbeen supporting efforts to leverage AI to address environmental challenges and promote sustainable \ndigitalisation. The initiative focuses on two main areas. First, the development of “AI innovations for climate \nprotection” aims at reducing greenhouse gas emissions and adapting to rapid climate change. Second, \nsupporting projects seek to reduce energy and resource consumption of AI systems and their \ninfrastructure. The funded projects range from smart grid control for the energy transition to urban climate \nadaptation and AI -optimized rail transport. Other projects focus on mitigating nitrate pollution in \ngroundwater or improving waste sorting (German Government, n.d.[68]). \nPortugal has been using AI to promote environmental sustainability. For example,  it deploys AI to fight \nagainst illegal fishing by identifying marine areas with the highest fish population, quantity and quality, and \nby measuring which fish species are most abundant in specific marine zones . Furthermore, Portugal has \nalso been developing algorithms that enable the analysis and control of waste management . With these \ninitiatives, Portugal contributes to the protection of biodiversity and the ecosys tem. \nThe European Commission ’s Destination Earth (DestinE) initiative represents another illustrative \nexample of how AI can be leveraged to fight climate change. DestinE is a flagship initiative that develops \na highly accurate digital model or “digital twin” of the Earth. By utilizin g advanced observation and \nsimulation capabilities powered by Europe's HPC computers and AI capacity, DestinE will enhance \npreparedness for natural disasters, climate change adaptation, and socioeconomic impact prediction. It \nconsolidates access to valuable sources of data across Europe and allows non-scientific experts to access \nand interact with vast amounts of system and socio -economic data. By generating highly accurate \nsimulations of the Earth, DestinE will support both EU policymaking and the practica l implementation \nthereof. For example, it will contribute to achieving the objectives stated in the European Commission’s \nGreen Deal and its Digital Strategy, and therefore help to align the green and digital twin transition \n(OECD.AI, 2023[8]). \n34    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nWell-being \nInitiatives to augment human capabilities and enhance creativity \nThe AUTONOM - Performing Arts and AI program me, offered by the German Performing Arts Fund, \nexemplifies how the intersection of AI and art can enhance creativity. As one of the six federal cultural \npromotion funds, the German Performing Arts Fund focuses on promoting the liberal performing arts and \nthe cultural landscape of the Federal Republic. It offers three general funding programs with four \napplication periods each year to facilitate the targeted and professional development of artistic concepts. \nIn addition, the fund offers special program mes that address topics such as diversity, digitization, art in \nrural areas, and the development of aesthetic -artistic forma ts. In doing so, it provides opportunities for \nqualification and further development. Participants are expected to explore various questions, including \nwhether AI is a catalyst for innovation or a frightening portrayal of dystopian scenarios, whether it re nders \nartists obsolete, who will determine what is presented and expressed in theatre spaces, and whether art \nand artists will continue to be autonomous in the future (Fond Darstellende Künste, 2021[69]). \nPortugal’s analysis and prediction of patterns in the utilisation of emergency and medical services is an \nexample of how AI can enhance human capabilities. It proposes the use of microdata from medication \nprescriptions by healthcare professionals from healthcare institutions and medical services. Analysing the \nrelationship between prescription patterns at this level of detail can serve as a proxy for predicting the \nutilisation of these services. In doing so, the initiative renders the management of healthcare entities in the \nNational Health Service (SNS) more efficient. \nTürkiye’s Breast Cancer Detection with AI project represents another example of increasing human \ncapabilities through AI.  Radiologists use an AI labelling tool developed by the Presidency Digital \nTransformation Office to identify benign and malignant anomalies in mammography images. This process \nreduces the likelihood of errors, particularly in mammography screening, and enables radiologists to \nprioritize images detected as high -risk by AI. The projec t hence serves as a decision support system, \nallowing radiologists in Türkiye to work more efficiently and effectively while also freeing up time and \nenhancing their capabilities. Ultimately, the project has the potential to save lives by enabling early \ndiagnosis of breast cancer (OECD.AI, 2023[8]). \nHuman-centred values and fairness (Principle 1.2) \n “AI actors should respect the rule of law, human rights and democratic values, throughout the AI system \nlifecycle. These include fre edom, dignity and autonomy, privacy and data protection, non -discrimination and \nequality, diversity, fairness, social justice, and internationally recognised labour rights. To this end, AI actors \nshould implement mechanisms and safeguards, such as capacity for human determination, that are appropriate \nto the context and consistent with the state of art.” \nSome uses of AI systems have implications for human rights, including risks that (as defined in the \nUniversal Declaration of Human Rights) human-centred values can be deliberately or accidentally infringed \nupon. To avoid this, countries are implementing human-centred values and fairness through polic y \ninitiatives (OECD, 2023[66]). \nNational approaches to implementing principle 1.2 Human-centred values and fairness \nTo implement principle 1.2, governments have issued primarily non -binding guidelines or initiatives \ntargeted at reducing AI biases as well as at values -alignment by promoting human rights and human -\ncentred values. By contrast, only very few H uman Rights Impact Assessments (HRIA) and quality seals \nhave been developed to date (Figure 3.3). \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   35 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 3.3. Select policies that implement OECD AI Principle 1.2 on human-centred values and \nfairness \n \nHuman-centred values \nInitiatives to align values by promoting human rights and human-centred values and through \nmandatory human rights impact assessments (HRIAs) \nNational AI policies often underline the significance of human -centred AI and reference one or several \nhuman rights. In this regard, the right to privacy is most frequently raised in national AI strategies, possibly \nbecause of its interconnectivity with other human rights, such as freedom of expression, association, and \npersonal autonomy. This is followed by the right to equality/non-discrimination and the right to an effective \nremedy. Merely referencing human rights in a National AI Strategy does not guarantee, however, that a \ngovernment’s approach and actions will effectively ensure respect for human rights in practice  (Stanford \nUniversity, 2021[70]). \nIn addition, human-centred AI and the adherence to human rights is stated as an important guiding principle \nin many national and international AI ethical guidelines. However, in some cases, guidelines and tools for \noperationalizing human-centred values and hum an rights are also being developed for more specific AI \nuse cases.  For example, with “Addressing human rights concerns arising from facial recognition \ntechnology”, France and the World Economic Forum  are co-designing a policy framework to deal with \nhuman rights concerns arising from the use of facial recognition technology. This framework will establish \na set of principles for responsible use of the technology. Furthermore, it will also comprise an assessment \nquestionnaire that applies these princip les to specific use cases, allowing organi sations to evaluate their \nrisk mitigation strategies . Furthermore, it will contain  an audit framework to ensure compliance with the \nprinciples for action (OECD.AI, 2023[8]). \nThe United States’ State Department “Guidance on products or services with surveillance capabilities ” is \na tool designed to offer practical and accessible human rights guidance to US businesses. Its goal is to \nhelp these businesses prevent their products or services w ith surveillance capabilities from misuse by \ngovernment end -users to commit human rights violations. T he tool  is aligned with the UN Guiding \nPrinciples on Business and Human Rights, as well as with the OECD Guidelines for assessing the human \nrights impacts of relevant products or services. It is meant to be an easy-to-use roadmap for assessing the \nhuman rights impacts of relevant products or services and for evaluating a series of considerations before \nengaging in transactions with governments. It also recommends human rights safeguards, such as creating \n• France's \"Addressing Human Rights Concerns Arising From \nFacial Recognition Technology\"\n• The Netherland's \"Fundamental Right and Algorithmic Impact \nAssessment\"\nInitiatives to align values by \npromoting human rights and \nhuman-centred values and \nthrough mandatory HRIAs\n• Korea's \"AI Personal Information Protection Self-Checklist\"\n• Mexico's \"Recommendations for the Processing of Personal Data \nderived from the use of AI\"\nInitiatives to protect privacy\n• Germany's \"AI Seal of Quality\"\n• Türkiye's \"Trustworthy AI Stamp\"\nQuality labels and certifications \nto promote human-centred \nvalues\n• United Kingdom's \"Review into Bias in Algorithmic Decision-\nMaking\"\n• United States' \"Artificial Intelligence and Algorithmic Fairness \nInitiative\"\nInitiatives to reduce AI bias\n36    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \na grievance mechanism and publicly reporting on sales practices, if a U.S. business decides to proceed \nwith a transaction (OECD.AI, 2023[8]). \nWhile OECD countries widely acknowledge that AI should be human-centred and respect human rights in \ntheir national AI strategies, countries have yet to implement this in a practical and legally binding way. \nNational regulations do not seem to address significant risks that AI systems pose to fundamental human \nrights beyond discriminatory biases.  \nTo date, only the Netherlands has introduced mandatory HRIA for the use of algorithms within Dutch \npublic authorities. The Netherlands’ Ministry of Interior and Kingdom Relations has created a Fundamental \nRights and Algorithms Impact Assessment (FRAIA), which facilitates an interdisciplinary dialogue to help \nmap the risks to human rights from the use of algorithms and determine measures to address these risks. \nThis example is, however, a unicum across countries (Dutch Government, 2021[71]).  \nAt the international level, the Council of Europe’s Recommendation CM/ REC (2020)1 of the Committee \nof Ministers to member States on the human rights impacts of algorithmic systems  suggests that “States \nshould ensure that they, as well as any private actors engaged to work with them or on their behalf, \nregularly and consultatively conduct human rights impact assessments prior to public procurement, during \ndevelopment, at regular milestones, and throughout their context -specific deployment in order to identify \nthe risks of rights -adverse outcomes ” (Council of Europe , 2020 [72]). The CoE’s Committee on Artificial \nIntelligence (CAI) is currently working on a “Framework Convention  Artificial Intelligence, Human Rights, \nDemocracy and the Rule of Law”, aimed at developing common principles to ensure respect for human \nrights, democracy, and the rule of law in a context where AI systems assist or replace human decision -\nmaking (Council of Europe, 2023[79]). \nInitiatives to protect privacy \nCountries are implementing various policies to protect privacy in AI , including through regulatory \nsandboxes to promote the development of privacy -friendly use of AI solution s (Figure 2.1). The Korean \ngovernment issued guidance relative to development of AI in its “AI Personal Information Protection Self -\nChecklist” in May 2021. In Mexico the National Institute for Transparency, Access to Information, and \nPersonal Data Protection ( Instituto Nacional de Transparencia, Acceso a la Información y Protección de \nDatos Personales , INAI), an autonomous constitutional entity, developed “Recommendations for the \nProcessing of Personal Data derived from the use of AI” (INAI, 2022[81]) as well as two instruments with \nthe Ibero-American Data Network (Ibero-American Data Protection Network, 2019 [82]); (Ibero-American \nData Protection Network, 2019 [83]). Several countries (e.g. Estonia, Turkey, the United Kingdom, United \nStates) are also promoting Privacy Enhancing Technologies (PETs)  to p revent privacy infringement in \ndeveloping or operating AI technologies and services (OECD, 2023[82]). \nQuality labels and certifications to promote human-centred values \nSo far, only a few count ries have developed quality  labels and certifications confirming that an AI tool is \nethical and human-centred.  \nThe German AI Association  has created the AI seal of quality aimed at promoting the use of human -\ncentred and human-serving AI. The seal of quality enforces a shared set of values and processes, thus \nensuring that services and products developed using it are ethically compatible. Its key quality criteria  \ncomprise ethics, impartiality, transparency, security, and data protection . E ach criterion has defined \nmeasures that must be met (KI Bundesverband, 2019[73]). \nIn 2019, Malta’s Digital Innovation Authority (MDIA) launched the AI Certification Programme. It consists \nof a set of guidance notes that assist Service Providers and AI Innovative Technology Arrangements (ITA) \napplicants when approaching the MDIA for registration and certification. The objectives are to expand the \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   37 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nITA certification framework for AI -based solutions and to become the world’s first national AI certification \nprogramme developed in an ethical, transparent, and socially responsible manner (OECD.AI, 2023[8]). \nSimilarly, Türkiye is in the process of developing a Trustworthy AI Trust Stamp that signals the use of \ntrustworthy AI in products, thereby increasing customer trust. The Turkish Standards Institute is currently \nengaged in research to establish criteria and metrics for the stamp.  \nFairness \nInitiatives to reduce AI bias \nFrance’s HPC Support to NLP Bigscience Workshop  is an international collaboration of over 800 \nresearchers working on large multilingual language models and datasets for a year. France provides the \nHPC support for the training of the workshop’s models, along with scientific contributions from the French \nNational Institute for Research in Digital Science and Technology (INRIA) and the National Centre for \nScientific Research (CNRS). The workshop has several object ives, including offering advanced large \nlanguage models and datasets for French and other EU languages, and addressing legal and ethical issues \nrelated to ownership and storage of large datasets. Most importantly for this section, t he project also \nanalyses biases within and ethical problems regarding the language models and proposes metrics and \ntools for their evaluation and mitigation (OECD.AI, 2023[8]). \nIn 2021, t he Netherlands’ Ministry of the Interior and Kingdom Relations commissioned  \nan investigation into the cause s of discrimination in AI. A team of experts from Tilburg University, \nEindhoven University of Technology, Vrije Universiteit Brussel , and The Netherlands Institute for Human \nRights collaborated on creat ing the guideline “Non-discrimination by design” . The guideline aims at  \nexplaining how organi sations can prevent their systems from being discriminatory . It  discusses the \ntechnical, legal, and organisational conditions that should be applied before, during  and after creating an \nAI system (Dutch Ministry of Internal Affairs, 2021[74]). \nIn 2020, the United Kingdom’s Centre for Data Ethics and Innovation (CDEI) published its Review into \nBias in Algorithmic Decision -Making. The review analyses the influence of the growing utilisation of \nalgorithmic tools on decision-making bias, the measures necessary to mitigate risks,  and the potential for \nimproving fairness through better data utilisation. It focuses on significant decisions made by algorithms \nabout individuals in four sectors: recruitment, financial services, policing, and local government. The review \nalso presents overarching suggestions to create appropriate systems that enhance, rather than undermine, \ndecision-making through algorithms (UK Government, 2020[75]). \nThe United States’ Equal Employment Opportunity Commission has launched the agency-wide AI and \nAlgorithmic Fairness Initiative to ensure that the use of AI complies with American anti-discrimination laws. \nThe initiative has several objectives , i.e., offering technical guidance on algorithmic fairness and AI ’s use \nin employment dec isions, identifying effective practices, hosting listening sessions with significant \nstakeholders about algorithmic tools and their employment implications, and collecting data on the \nadoption, development, and effect of hiring and other employment-based technologies (OECD.AI, 2023[8]). \nFurthermore, the Executive Order on Advancing Racial Equity and Support for Underserved Communities \nthrough the Federal Government of 2023 includes an AI clause, which places new equity obligation s on \nfederal agencies that deploy AI systems . It  directs agencies to “prevent and remedy discrimination, \nincluding by protecting the public from algorithmic discrimination” (OECD.AI, 2023[8]) . \nIn 2021, the United States ’ Federal Trade Commission (FTC)  designated eight key areas of focus for \nenforcement and regulatory action, one of which directly implicate s investigations in unfair, deceptive, \nanticompetitive, collusive, coercive, predatory, exploitative, and exclusionary acts or practices relating to \nalgorithms and biometrics. T he FCT will “investigate whether any persons, partnerships, corporations, or \nothers have engaged or are engaging in unfair, deceptive, anticompetitive, collusive, coercive, predatory, \n38    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nexploitative, or exclusionary acts or practices relating to algorithms and biometrics, in or affecting \ncommerce, including but not limited to bias in algorithms and biometrics, in violation of Section 5 of the \nFederal Trade Commission Act, 15 U.S.C. § 45, as amended or any statutes or rules enforced by the \nCommission” (OECD.AI, 2023[8]). It will then determine the appropriate action or remedy, including whether \ninjunctive and monetary relief would be in the public interest. \nTransparency and explainability (Principle 1.3) \n“AI Actors should commit to transparency and responsible disclosure regarding AI systems. To this end, they \nshould provide meaningful information, appropriate to the context, and consistent with the state of art: to foster \na ge neral understanding of AI systems, to make stakeholders aware of their interactions with AI systems, \nincluding in the workplace, to enable those affected by an AI system to understand the outcome, and, to enable \nthose adversely affected by an AI system to challenge its outcome based on plain and easy -to-understand \ninformation on the factors, and the logic that served as the basis for the prediction, recommendation or \ndecision.” \nMost national AI strategies, ethical frameworks, and general principles for the implementation of AI list \ntransparency and explainability among the key properties of a trustworthy AI system. Transparency and \nexplainability also figure prominently in several non -binding guidelines for ethical AI implementation. \nHowever, despite broad agreement on the need for transparent and explainable AI, operationalising these \nconcepts is complex due to their  multifaceted nuances. AI transparency entails: i) clearly communicating \nto users that they are dealing with an AI system , ii) the interpretability of decision-making processes, and \niii) the explainability of decision-making logic.  \nNational approaches to implementing principle 1.3 transparency and explainability \nGovernments are taking a variety of approaches to ensure AI transparency, ranging from guidelines for \nimplementation of AI to the establishment of oversight bodies. Regulatory bodies have recognise d the \nimportance of AI transparency and explainability. Transparency provisions are laid down in existing \nlegislation (e.g., data protection and privacy legislation, consumer protection legislation) and are also being \nincluded in proposed AI -specific regulations, with several specific provisions pertaining to the workplace . \nIn the public sector, governments are enhancing transparency around the use of AI for public services, for \nexample through AI registers (Figure 3.4). \nFigure 3.4. Select policies that implement OECD AI Principle 1.3 on transparency and explainability \n \n• Finland's and the Netherland's \"Open AI Register of \nAI systems' use in the public sector\"\n• Spain's \"Provision to the Workers' Statute Law \nDetailing Employers' Algorithmic Transparency \nRequirements\"\nRequire disclosure and \ninformation about use of AI \nsystems\n• European Union's proposed \"AI Act\"\n• Canada’s proposed Artificial Intelligence and Data Act \n(AIDA) \nProvide information on AI \nfunctioning\n• European Union’s General Data Protection Regulation\n• Canada’s Directive on Automated Decision-Making \nProvide information on \nfactors and decision \nprocesses; enable redress \nseeking from decisions\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   39 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nInitiatives requiring disclosure and information about use of AI systems \nJapan has introduced transparency requirements in the Digital Platform Transparency Act (DP \nTransparency Act) by requiring designated digital platform providers (online malls, app sto res, and digital \nadvertising businesses) to ensure transparency and fairness in transactions with business users (Habuka, \n2023[76]). Other transparency requirements are included in soft law, such as the AI utilisation guidelines, \nthe contract guidelines on utilising AI and data, and machine learning quality management guidelines.  \nThe proposed EU AI Act includes transparency obligations for high-risk AI systems (Article 13), as well as \nthose that meet at least one of the criteria below (Article 52): \n• intended to interact with natural persons (e.g., chatbots);  \n• used for emotion recognition; \n• used for biometric categorization; or \n• used to generate or manipulate image, audio or video content.  \nThe transparency obligations arising under Article 52 concern informing users that they are interacting with \nan AI system. \nThe EU’s Digital Services Act (DSA) (passed in July 2022) also includes requirements for enhanced \ntransparency of algorithms. The DSA requires Very Large Online Platfor ms (VLOPs) and Very Large \nOnline Search Engines (VLOSEs) operating in the EU to identify, analyse , and assess certain systemic \nrisks stemming from the design and functioning of their service and related systems, including algorithmic \nsystems. Moreover, they must commit to addressing identified risks, whether directly or indirectly related \nto the functioning of the algorithmic system in use (European Union, 2022[77]). The European Commission \nalso set up the European Centre for Algorithmic Transparency (ECAT) to support its supervisory role with \nin-house and external multidisciplinary knowledge. The Centre, hosted by the Joint Research Centre (JRC) \nin close co -operation with the Directorate General Communications Networks, Content and Technology \n(DG CONNECT), will support the regulator in assessing whether the functioning of algorithmic systems is \nin line with the risk management obligations established by the DSA for VLOPs and VLOSEs (OECD.AI, \n2023[8]). \nAs governments are increasingly integrating AI into the design and delivery of public policies and services, \nthey are also increasing efforts to enhance transparency and explainability.  \nIn France, the 2016 Digital Republic Law mandates transparency of gov ernment-used algorithms. Public \nagencies are required to publicly list any algorithmic tools they use and to publish their rules. Etalab, a \ndepartment of the Inter-ministerial Digital Direction (DINUM), has provided guidance on the implementation \nof this commitment by publishing two guidance documents : one shows how to open public source codes, \nand the other explains the legal framework of accountability and transparency of public sector algorithms \n(OECD.AI, 2023[8]). Helsinki (Finland) and Amsterdam ( Netherlands) have launched open AI registers \nthat track how algorithms are being used in the municipalities. Following this example, nine European cities \n(Barcelona, Bologna, Brussels Capital Region, Eindhoven, Mannheim, R otterdam, and Sofia) have \ncollaborated in 2023 through the Eurocities Digital Forum Lab network to develop an AI algorithm registers \nstandard (OECD.AI, 2023 [8]). At the end of 2022, the United Kingdom  government published the \nAlgorithmic Transparency Recording Standard, which comprehensively organizes how the public sector, \nincluding government, should disclose information when using algorithmic tools. The United Kingdom, \nthrough The Alan Turing Institute, has also created an AI Standards Hub to advance trustworthy AI through \nstandards. The Algorithmic Transparency Standard is one example of this (OECD.AI, 2023[8]). \nVarious approaches are being also employed to promote the transparent utilisation of AI in the workplace. \nThese comprise the reliance on existing policies as well as the development of self - and co-regulation \napproaches and new polices (Salvi del Pero, Wyckoff and Vourc’h, 2022[78]).  \n40    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nExamples include the requirement to prior agreement with workers’ representatives on the monitoring of \nworkers using digital technologies (e.g., France, Germany and Italy), and regulations requiring employers \nto notify employees about electronic employee monitoring policies. For example, s uch regulations have \nbeen adopted by the Canadian Province of Ontario in 2022 (Ontario Working for Workers Act). Similarly, \nin the United States, a number of states have laws in place that require employers to notify employees of \nelectronic mo nitoring. New York’s Electronic Monitoring Bill (2022) imposes compliance obligations on \nNew York City employers using AI tools to notify employees of their electronic monitoring practices. Illinois’ \nAI Video Interview Act and Maryland Facial Recognition L aw require employers to disclose the use of AI \nanalysis in video interviews. In the United Kingdom , the Information Commissioner's Office issued \nguidance in October 2022 on employers ’ legal obligations when monitoring workers (Monitoring at Work \nDraft Guidance).  \nInitiatives to provide information on AI functioning \nCanada’s proposed  Artificial Intelligence and Data Act (AIDA) introduces requirements to promote \ntransparency on the use of AI. Transparency means providing the public with appropriate information about \nhow high-impact AI systems are being used. The act stipulates in Article 11 that where the system is made \navailable for use, the person responsible must publish on a publicly available website a plain -language \ndescription of the system that explains how the system is to be used, the types of content that it is intended \nto generate, and the types of decisions, recommendations, or predictions it is intended to make, along with \nthe risk mitigation measures established. The information provided should b e sufficient to allow the public \nto understand the capabilities, limitations, and potential impacts of the systems (Government of Canada, \n2023[79]). \nSpain’s Royal Decree -Law 9/2021 (the “Rider Law”) modifies the Workers ’ Statute Law detailing \nemployers’ algorithmic transparency requirements. The legislation renders transparency mandatory for AI \nsystems that make decisions about or influence working conditions or employment status. It requires \nemployers using algorithmic d ecision-making to disclose the key attributes of algorithms to employee \nrepresentatives. The “attributes include the algorithm’s parameters and general logic used to make \ndecisions, a significant leap in regulation beyond the GDPR” (Chavez, Bahr and Vartanian, 2022[80]).  \nThe United States’ proposed Algorithmic Accountability Act (AAA) of 2022 includes transparency \nrequirements for companies employing Automated Decision Systems (ADS) to make critical decisions, i.e., \nany decision that has significant legal or material effects on a consumer’s life. This includes access to \neducation, employment, essential utilities, healthcare, and financial services (Section  2.7). The AAA also \nrequires organisations deploying new ADS to “describe the existing decision-making process [and] explain \nthe intended benefits of augmenting [it]” (Section 4) (US Congress, 2022[28]).  \nThe proposed EU AI Act includes transparency obligations for high -risk AI systems (Article 13). The \nRegulation prescribes information requirements which would allow users to interpret a system’s output and \nuse it appropriately (European Commission, 2021a[33]). \nThe Chinese regulation on algorithmic recommendation systems, which entered into force in March 2022, \nfocuses on the use and impact of algorithmic recommendation systems. It creates transparency obligations \nwhich entail  user notifications regarding the criteria for recommendation and clear ind icators of \nalgorithmically generated or synthetic information. It orders the implementation of mechanisms of manual \nintervention and autonomous user choice. The regulation also mentions the creation of a registry and \ncategorisation system to manage algorithms placed on the market (OECD.AI, 2023[8]).  \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   41 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nInitiatives to provide information on factors and decision processes and enable redress \nseeking from decisions \nCanada’s Consumer Privacy Protection Act includes transparency requirem ents regarding “the \norganization’s use of any automated decision system to make predictions, recommendations or decisions \nabout individuals that could have a significant impact on them”  (Article 62) . It also features a “right to \nexplanation” in Article 63 (2): “If the organization has used an automated decision system to make a \nprediction, recommendation or decision about the individual that could have a significant impact on them, \nthe organization must, on request by the individual, provide them with an ex planation of the prediction, \nrecommendation or decision” (Canadian Parliament, 2022[12]).  \nCanada’s Directive on Automated Decision -Making sets a wide range of mandatory requirements to \nensure the responsible use of AI by federal institutions. The Directive applies to systems used to make \ndecisions affecting legal rights or carry out assessments about  clients to inform these decisions. The \nrequirements mandate completion and publication of an algorithmic impact assessment; adoption of \nseveral transparency measures such as notice and explanation to clients; application of quality assurance \nmeasures such as bias testing, peer review, and ongoing monitoring of outcomes; provision of recourse to \nsubjects of automated administrative decisions; and public reporting on system effectiveness and \nefficiency (OECD.AI, 2023[8]).  \nMexico’s legal framework in matters of personal data protection, applicable to the public and private sector, \nalso contains provisions related to automated decisions.  \nNorway’s Public Administration Act states that public sector decision -making pertaining to spec ific \nindividuals must provide explanations for the decisions taken in order to ensure accountability and support \na complaints/appeals process. There is also a requirement for accountability/transparency and equal \ntreatment. This law is the legal foundation for all types of casework systems in the public sector, including \nsystems that use AI/machine learning. \nIn the United Kingdom, the proposed Data Protection and Digital Information Bill (No.2) introduced in the \nHouse of Commons on March 28 , 2023, introduces a more business-friendly approach in comparison to \nthe existing UK General Data Protection Regulation (UK GDPR). Whereas the UK GDPR generally \nprohibits automated profiling decision-making, the proposed bill allows it by default, placing on the affected \nindividuals the responsibility to challenge such decisions and request a human review  (UK Parliament, \n2023[30]). \nThe EU’s General Data Protection Regulation (GDPR), which entered into force in 2018, implies a “right \nto explanation” in Article 22 by giving individuals the right not to be subjected to “a decision based solely \non automated processing, including profiling, in cases where they produce legal or similarly significant \neffects affecting the data subject” (European Union, 2018[84]). A number of cases have been brought to EU \nCourts on the use of AI in the workplace, based on the legal rights accorded by the GDPR (Salvi del Pero, \nWyckoff and Vourc’h, 2022[78]).  \nRobustness, security, and safety (Principle 1.4) \n“AI systems should be robust, secure and safe throughout their entire lifecycle so that, in conditions of normal \nuse, foreseeable use or misuse, or other adverse conditions, they function appropriately and do not pose \nunreasonable safety risk. \nTo this end, AI actors should ensure traceability, including in relation to datasets, processes and decisions \nmade during the AI system lifecycle, to enable analysis of the AI system’s outcomes and responses to inquiry, \nappropriate to the context and consistent with the state of art. \n42    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nAI actors should, based on their roles, the context, and their ability to act, apply a systematic risk management \napproach to each phase of the AI system lifecycle on a continuous basis to address risks related to AI systems, \nincluding privacy, digital security, safety and bias.” \nIssues of robustness, security and safety of AI are interlinked. For example, digital security can affect the \nsafety of connected products, such as automobiles and home appliances, when risks are not appropriately \nmanaged (OECD, 2023 [66]). Therefore, they are analysed together . However, there are different, not \nmutually exclusive ways in which countries can operationalise them. \nNational approaches to implementing Principle 1.4 on robustness, security and safety  \nCountries are drawing on guidelines, ethics frameworks, impact assessments, new legislation , \namendments to existing legislation, and other instruments to implement Principle 1.4 (Figure 3.5). \nFigure 3.5. Select policies that implement OECD AI Principle 1.4 on robustness, security and safety \n \nAlgorithmic Impact Assessments  \nCanada’s Directive on Automated Decision -Making requires federal institutions planning to use an \nautomated system, including those that rely on AI, to make or support administrative decisions to complete \nand publish an Algorithmic Impact Assessment (AIA) before the launch of the system. The AIA helps \nevaluate and anticipate the potential social, economic, ethical, and legal effects of AI systems before they \nare implemented and throughout their lifecycle, by providing a framework for identifying and mitigating \nrisks. The Canadian AIA was developed in consultation with academia, civil society, and other public \ninstitutions to help departments and agencies understand and manage the risks associated with automated \ndecision systems. The AIA consists of over 80 questions in different formats aimed at evaluating six areas \nof risk, i.e., risks related to the project, system, algorithm, decision, impact, and data, and assessing \nmitigation measures in place to manage identified risks. The AIA evaluates the impact of automation \nprojects based on the responses to the questions, which are assigned weights that are used to calculate \nraw impact and mitigation scores. A final score is produced based on these scores, which is used to classify \nthe automated decision system into one of four possible impact levels (little to no impact; moderate impact; \nhigh impact; and very high impact). The level of impact determines applicable requirements u nder the \nDirective (Canadian Government, 2023[85]).  \nMexico’s Principles and Impact Analysis Guide for the Development and Use of Systems Based on AI in \nthe Federal Public Administration are based on Canada’s AIA and are designed to assess the societal and \nethical implications of AI systems developed by the Federal Public Administration. It is an online \n• Mexico's \"Principles and Impact Analysis Guide for the \nDevelopment and Use of Systems Based on Artificial \nIntelligence in the Federal Public Administration\"\n• Uruguay's \"Algorithmic Impact Study (EIA)\" \nAlgorithmic Impact Assessments\n• Türkiye's \"National Data Dictionary\"\n• United States' \"National Security Presidential Memorandum on \nProtecting the United States' Advantage in Artificial Intelligence \nand Related Critical Technologies\"\nMaintaining records of data \ncharacteristics for traceability\n• Austria's \"Automated Driving Regulation\"\n• Lithuania's \"Law on Road Traffic Safety\"\nLaws and regulations preventing \nunreasonable safety risks & \ndefinition of how these laws apply \nto AI systems\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   43 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nquestionnaire that determines the impact level of an automated decision -system, outlining appropriate \nsafeguards that must be put in place according to the potential impacts of these systems. Each level \nrequires the system to fulfil a different set of requirements before, during, and after its implementation  \n(OECD, 2022[93]).  \nSimilarly, Uruguay’s Agency for Electronic Government and the Information and Knowledge Society \n(AGESIC) developed the Algorithmic Impact Study (EIA) to analyse machine learning -based automated \ndecision support systems. It poses different questions that assess various aspects of AI systems. These \ninclude the predicted social impact, an impact evaluation of the automated decision system, the origin of \nthe data used, stakeholders involved, actions to reduce and mitigate the risks of the automated decision \nsystem, and procedural fairness. It is primarily aimed at project managers or teams involved in AI projects, \nand designed to identify crucial aspects of the systems requiring additional attention or treatment. Users \nof the tool can then share, analyse, and evaluate the results obtained from the questions (OECD, 2022[93]). \nInitiatives to maintain records of data characteristics for traceability \nEfforts are currently underway in Türkiye to introduce a National Data Dictionary, which aims at compiling \na national data inventory and establishing management and monitoring processes through national data \nintegration architecture (Turkish Government, 2019[87]). The United States’ National Security Presidential \nMemorandum (NSPM) Protecting the United States Advantage in AI and Related Critical Technologies  \nsafeguards the country’s advantage in critical technologies, including AI, against foreign adversaries and \nstrategic competitors. One of the primary objectives of the NSPM is to improve access to high-quality and \ncompletely traceable Federal data, models, and computing resources (US Presidential Office, 2019[88]). \nLaws and regulations preventing unreasonable safety risks of AI systems: autonomous \ndriving  \nThe main field where countries are increasingly implementing laws and regulations pertains to autonomous \ndriving. For example, Austria’s Automated Driving Laws and Regulations (32nd Amendment to the \nAustrian Motor Vehicles Act ; Automated Driving Regulation) grants permission for certain automotive \nfunctions or tasks to be done by automated systems. It regulates the basic requirements for the testing of \nin-vehicle drive assistance systems , automated, or networked driving systems and defines the first \napplications of autonomous vehicles (e.g. , autonomous minibuses in urban areas)  (OECD.AI, 2023 [8]). \nGermany’s Automated Vehicles (AV) Bill in the Road Traffic Act as well as its Act Amending the Road \nTraffic Act and the Compulsory Insurance Act (“Autonomous Driving Act”) represent further examples for \nthe increase in legislation in this sector. They legalise automated vehicles by modifying the current Road \nTraffic Act and define the requirements for highly and fully automated vehicles to use public roads  \n(OECD.AI, 2023 [8]). Similarly, Denmark’s Road Directorate, Japan’s Legal Regulati on of Autonomous \nDriving Technology, Lithuania’s Law on Road Traffic Safety as well as the United Kingdom’s Automated \nand Electric Vehicles Bill all represent new legislation s that define the use of self -driving cars on the  \ncountries’ national roads (OECD.AI, 2023[8]). \nAccountability (Principle 1.5) \n“AI actors should be accountable for the proper functioning of AI systems and for the respect of the above \nprinciples, based on their roles, the context, and consistent with the state  of art.” \nAccountability refers to the expectation that organisations or individuals will ensure and be held responsible \nfor the proper functioning, throughout their lifecycle, of the AI systems that they design, develop, operate \nor deploy, in accordance with their roles and applicable regulatory frameworks, and for demonstrating this \n44    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nthrough their actions and decision -making processes. In the case of a negative outcome, it also implies \ntaking action to ensure a better outcome in the future (OECD, 2019[65]).  \nUpcoming specific-AI legislations establish the scope for  accountability (both in terms of who should be \nheld responsible and in which circumstances) in relation to the development, deployment , and use of AI \nsystems, which may vary according to the jurisdiction (Annex A).  \nDemand for tools and processes to document AI system decisions and to ensure accountability is on the \nrise in both the public and private sectors. This field encompasses major AI standardization initiatives led \nby organizations like the International Organization for Standardization (ISO), Institute of Electrical and \nElectronics Engineers (IEEE), International Telecommunication Union (ITU), NIST, European \nTelecommunications Standards Institute (ETSI), Internet Engineering  Task Force (IETF), and European \nCommittee for Electrotechnical Standardization (CEN -CENELEC). These initiatives focus on various \naspects including AI design (like trustworthiness by design), impact assessments, conformity evaluations, \nand risk management frameworks for AI. Additionally, there are governmental and intergovernmental \nefforts such as the EU's proposed AI Act, the UK's AI Standards Hub, the European AI Alliance, the Council \nof Europe's Committee on Artificial Intelligence (CAI), and the EU -US T rade and Technology Council. \nCertification schemes are also a part of this landscape (OECD, 2023[66]). \nNational approaches to implementing Principle 1.5 on accountability \nCountries have developed codes of ethical conduct for the use or implementation of AI in several sectors \n(public administration, health care, autonomous driving) . Proposed AI -specific regulation requires  the \ndocumentation of the proper functioning of the AI systems throughout their lifecycle. Lastly, countries have \nestablished independent oversight bodies that audit the use of algorithms (Figure 3.6). \nFigure 3.6. Select policies that implement OECD AI Principle 1.5 on accountability \n \nLegislation that requires documenting the proper functioning of the AI systems throughout \ntheir lifecycle \nCanada’s proposed Artificial Intelligence an d Data Act (AIDA) defines accountability as follows: \n“Accountability means that organisations must put in place governance mechanisms needed to ensure \ncompliance with all legal obligations of high -impact AI systems in the context in which they will be used” \n(Government of Canada, 2023 [79]). To ensure this in practice, policies, processes, and measures \nimplemented as well as ways to meet requirements for design and development , must be proactively \ndocumented. Moreover, organisations are required to provide appropriate documentation to users \nconcerning datasets, limitations, and appropriate uses.  \n• Canada's proposed \"Artificial Intelligence and Data Act \n(AIDA)\"\n• European Union's proposed \"AI Act\"\nLegislation that requires the \ndocumentation of the proper \nfunctioning of the AI systems \nthroughout their lifecycle\n• Portugal's \"Guide to Artificial Intelligence in the Public \nAdministration\"\n• Singapore's \"AI in Healthcare Guidelines\"\nCodes of ethical conduct and \npractical technical tools\n• The Netherland's \"Algorithm Supervision Body\"\n• European Union's proposed \"AI Act\"\nIndependent oversight \nbodies to audit the use of \nalgorithms\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   45 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nThe p roposed United States  Algorithmic Accountability Act (AAA) includes requirements on \ndocumentation for Automated Decision Systems . Article 4(7) requires operators to “ maintain and keep \nupdated documentation of any data or oth er input information used to develop, test, maintain, or update \nthe automated decision system or augmented critical decision process” (US Congress, 2022[28]). \nThe EU’s proposed AI Act also makes technical documentation obligatory. Article 11 states that the \ntechnical documentation of a high -risk AI system must be prepared in a way that proves the system ’s \nconformity with the requirements specified in the AI Act . It includes all p ertinent information for national \ncompetent authorities and notified bodies to evaluate the system ’s compliance. This documentation must \nbe completed prior to the system’s introduction to the market or its utilisation and must be kept up-to-date \n(European Commission, 2021a[33]). \nCodes of ethical conduct and practical technical tools \nCodes of ethical conduct and practical technical tools are the most commonly employed instruments by \ncountries to ensure accountability, with particular prominence in the public and healthcare sectors. \nFrance’s Etalab can serve as the first example of such codes of ethical conduct in the public sector. Etalab \nhas issued guidance on Accountability for Public Algorithms, which sets out how public organis ations \nshould report on their use to promote transparency and accountability. The guidance proposes six \nprinciples for the accountability of algorithms in the public sector (OECD.AI, 2023[8]).  \nColombia’s Dashboard for monitoring the Ethical Framework for AI provides an overview of AI projects by \npublic entities and of how these entities are implementing ethical principles and tools in their projects. By \nhaving them report on several key issues, it is an example of holding public entities that use AI accountable. \nThe issues  include the ethical principles employed, non -discrimination measures taken, and potential \nethical risks, along with mechanisms installed to mitigate them (OECD.AI, 2023[8]).  \nPortugal has also published a Guide to A I in the Public Administration. Th e guide outlines the definition \nAI, highlights its presence in society, and offers insights into the potential effects of its use. In recognition \nof the critical role data plays in developing and sustaining these systems, the guide also touches upon the \ndata ecosystem within the Public Administration, including the principles that must govern it. These \ncomprise human rights as well as inclusion, equality, sustainable development, and well-being (OECD.AI, \n2023[8]).  \nNorway’s Guidance on the development and use of AI in the public sector  aims at operationalising the \nethics principles from its national AI strategy. It promotes the use and development of AI in the public sector \nwhile ensuring this is done following an ethical framework. Another code of ethical conduct can also be \nfound in the United Kingdom. Its Guide to Using AI in the Public Sector , published by the Government  \nDigital Service (GDS) and the Office for Artificial Intelligence (OAI) , is based on the UK’s Data Ethics \nFramework, which sets out clear principles for how data should be used in the public sector. The “Guide \nto Using AI in the Public Sector” applies thes e principles to the context of public procurement  (OECD.AI, \n2023[8]). \nCodes of ethical conduct in the healthcare sector have been established in France, among other countries. \nThe French “Good Practice Recommendations to Integrate Ethics in the Development of AI Solutions in \nHealthcare” provides a comprehensive list of ethical guidelines necessary for AI solutions in healthcare, \ndeploying a two-tier “ethics by design” approach. This firstly entails incor porating ethical values , to the \ngreatest extent feasible , into the hardware and software architecture of the systems during the design \nphase. Secondly, it involves validating proposed solutions and preventing the risks arising from t oday’s \npredominantly inductive nature of AI at the end of the development process (OECD.AI, 2023[8]). Likewise, \nSingapore has also implemented AI in Healthcare Guidelines. Co-developed by the Ministry of Health \n(MOH), the Health Sciences Authority (HS A), and the Integrated Health Information Systems (IHiS) , they \nare to strengthen patient safety and trust of AI in healthcare. Further, the recommendations aim at \n46    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nencouraging the safe development and implementation of both AI medical devices (\"AI -MDs\") and any \nother AI system implemented in healthcare settings (OECD.AI, 2023[8]). \nCodes of ethical conduct are also implemented in various other sectors. Austria’s Codes of Practice for \nTesting of Automated Driving on Public Roads can be mentioned as an example for the transport sector. \nThe Codes comprise a set of measures for companies seeking to test automated vehicles (AV), which \nshould minimise risk and maximise safety when AV are deployed on roads  (OECD.AI, 2023[8]). \nKorea also established National AI Ethics Guideline s in December 2020, which lay out comprehensive \nbasic guidelines that should be followed by all members of society in all stages of AI development and use \nto realize human -centred A I. For practical implementation of the National AI Ethics Guidelines, Korea \nannounced the Trustworthy AI Implementation Strategy in May 2021  and developed a self-assessment \nchecklist in February 2022. The country also provided detailed guidelines, which co nsider sector-specific \nand/or case-specific in April 2023, to help the private sector to voluntarily follow the guidelines and verify \ntheir development methods in various fields (e.g., general purpose, health, autonomous driving, \npublic/social services). \nThe OECD.AI Expert Group on Risk and Accountability developed a catalogue of tools and metrics for \ntrustworthy AI (OECD, 2023[89]) to provide an interactive collection of resources for the development and \nimplementation of  AI systems that respect human rights and are fair, transparent, explainable, robust, \nsecure, and safe. These tools, mapped to the OECD AI Principles and the phases  of the AI system \nlifecycle, are expected to facilitate accountability in AI, from documenting and monitoring risks to \ncertification and assurance. The United Kingdom Centre for Data Ethics and Innovation (CDEI) is  also \ndeveloping a portfolio of use cases and an online searchable repository of AI assurance tools . AI \nAssurance tools are market -based means of managing AI risks and a complement to regulation that will \nempower industry to ensure that AI systems meet their regulatory obligations  (OECD.AI, 2023 [8]). \nSingapore’s A.I. Verify, an AI governance testing framework and toolkit for companies, is another tool that \nhelps companies with transparency. It is a framework and software tool to conduct objective, verifiable \ntests and to record process checks. Key features include covering crucial international governance \nframeworks and guidelines, validating companies’ claims about AI systems’ performance, a single and \nintegrated toolkit for self -testing, and customised testing reports t o be available for different groups of \nstakeholders (OECD.AI, 2023[8]). \nIndependent oversight bodies to audit the use of algorithms \nWhile voluntary codes of ethical conduct and practical tools to ensure accountability are already \nwidespread, only very few countries have implemented legally -binding, independent oversight bodies to \naudit the use of algorithms. \nIn December 2022, Spain announced the establishment of the Spanish Agency for the Supervision of AI \n(Agencia Española de Supervisión de la Inteligencia Artificial , AESIA). One of its objectives is to promote \nresponsible, sustainable, and trustworthy AI, while also fostering collaboration and coordination with other \nnational and supranational authorities responsible for AI oversight. (Proteccion Data, 2022 [90]). Another \nindependent oversight bod y is the Dutch algorithms supervision unit , located within the Dutch Data \nProtection Authority (DPA). The unit’s objective is to enhance the supervision of algorithms that process \npersonal data.  Its responsibilities include monitoring algorithms to promote transparency, prevent \ndiscrimination and bias, and identifying and analysing potential risks of these systems (OECD.AI, 2023[8]). \nIn parliament on June 16, 2023, the Norwegian Liberal Party proposed the establishment of an Algorithmic \nSupervision Authority. They also urged the government to assess how relevant aspects of the Norwegian \nlegal framework should be interpreted and applied to AI use.  \nThrough the implementation of new structures for algorithm oversight, Spain  and the Netherlands also \nanticipate a critical element of the EU’s proposed AI Act. In compliance with Article 59, member states are \nrequired to appoint national supervisory authorities that typically act as Market Surveillance Authorities \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   47 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \n(MSAs). These public bodies are endowed with regulatory powers, such as accessing training, validation, \nand testing datasets used by the provider and the AI source code. They also have the authority to withdraw \nproducts and require intermediaries to cooperate in removing products from the market. Given this legal \nrequirement under the proposed AI Act, many independent oversight bodies are expected to  emerge in \nEU countries once the proposed AI Act is officially adopted (European Commission, 2021a[33]). \nInvesting in AI Research & Development (Principle 2.1) \n“Governments should consider long-term public investment, and encourage private investment in research and \ndevelopment, including inter-disciplinary efforts, to spur innovation in trustworthy AI that focus on challenging \ntechnical issues and on AI-related social, legal and ethical implications and policy issues.” \n“Governments should also consider public investment and encourage private investment in open datasets that \nare representative and respect privacy and data protection to support an environment for AI research and \ndevelopment that is free of inappropriate bias and to improve interoperability and use of standards.”  \nMany countries have recognised the importance of policies that support AI R&D and are responding with \ninitiatives to ramp up efforts in this area. Most national AI strategies focus on AI R&D as one of the key \nareas for action. Countries have dedicated AI R &D funding and support it through different instruments. \nMain trends include launching AI R&D -focused policies, plans, and programmes, supporting the creation \nof national AI research institutes and centres, and consolidating AI research networks and collab orative \nplatforms (Figure 4.1). \nOne way to measure progress in AI R&D is to examine the quantity of AI research published by countries. \nOver the past decades, there has been consistent growth in the number of AI research publications in both \nthe United States and the European Union, including journal articles, books, conference proceedings, \npatents, and academic repositories. A I publications have increased dramatically in China and India in \nrecent years ( Figure 4.2). Since 2019, China has published more AI research than the United States or \nthe European Union. India has also recently made significant advances, with its number of AI research \npublications more than doubling since 2015. If these trends continue, India can be expected to catch up to \nthe leading three players in AI research (OECD.AI, 2023[91]).  \n \n4 Implementing the five \nrecommendations to governments \n48    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 4.1. Select policies that implement OECD AI Principle 2.1 on investing in AI R&D \n \nFigure 4.2. AI research publications, 2000-2021, top publishing countries  \n \nNote: This figure shows the quantity of AI research publications for a sample of top countries for 2000-2021. OpenAlex publications are scholarly \ndocuments such as journal articles, books, and dissertations.  \nSource: (OECD.AI, 2023[91]), visualisations powered by JSI using data from OpenAlex. \nMerely assessing the quantity of AI publications does not offer a comprehensive view of publication quality \nand impact. Analysing the frequency  of citations a publication receives can serve as proxy indicator for \nquality, as a higher number of citations tends to indicate  a “higher impact”. As of 2019, Chinese \npublications have been cited more frequently, with more AI research papers in the top 1% of the most-\ncited papers compared to those from both the United States and the European Union (Figure 4.2). While \nthis trend indicates that China is currently ahead  in terms of AI research publications, a different p icture \nemerges when considering the proportion of high -impact publications relative to the total number of AI \npublications. Since 2000, the United Kingdom has led in the proportion of high -quality AI publications, \nfollowed by Canada, China, and the United States (OECD.AI, 2023[91]).  \n \n \n• France's \"National AI Research Programme\"\n• United Kingdom's \"AI Sector Deal\"\nAI R&D policies, plans, and \nprogrammes\n• Chile's \"National AI Research Centre\"\n• Canada's \"National AI Institutes (Amii, Vector \nInstitute, Mila)\nAI research institutes and \ncentres of excellence\n• Japan's \"AI R&D Network\"\n• United States' \"AI Researchers Portal\"\nAI research networks and \nportals\n• European Union's \"Horizon Europe 2021-2027\"\n• Australia's \"AI Solutions to Build a Stronger \nAustralia\"\nFunds to support AI R&D \ndiffusion\n\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   49 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 4.3. High-impact AI research publications, 2000-2021, top publishing countries \n \nNote: This figure shows the quantity and impact of AI research publications for a sample of top countries for 2000-2021. OpenAlex publications \nare scholarly documents such as journal articles, books, and dissertations. Publication quality ranking is calculated by dividing  the number of \ncitations by the average citations in the subdiscipline, discounted by the number of years since the publication was  published, plus one (as the \nnumber of years impacts the number of citations). Publications are classified as “high-impact” if their score falls in the highest quartile. \nSource: (OECD.AI, 2023[91]), visualisations powered by JSI using data from OpenAlex. \nPublic funding to AI R&D \nThe allocation of public budgets to AI R&D varies in scale across countries. In many instances, the \ndistribution of budgets per year to AI R&D and other aspects of the strategy is not explicitly mentione d. \nThe OECD has assessed government spending on AI-related R&D up to 2019 through key terms matching \nof national (and EU) databases (Yamashita and et al., 2021 [92]). However, there is  no comprehensive \nmethod for tracking and comparing AI R&D funding across countries and agencies. While there are no \nofficial or comparable estimates of public investment in non-defence AI R&D, several budgetary elements \nare provided below.  \nIn the United States, the funding requested for non -defence AI R&D by the Networking and Information \nTechnology Research and Development (NITRD) Program and the National AI Initiative Office (NAAIO) \nwas USD 1.8 billion in 2023 (National Council of Science and Technology, 2022[93]), making it the highest \nspending since 2019  (NITRD, 2023[99]). The U.S. National AI Initiative Act of 2020 provides a co-ordinated \nprogramme across the Federal government to accelerate A I research and application for the country’s \neconomic prosperity and national security  (OECD.AI, 2023[8]). Most recently, the NSF and the Office of \nScience and Technology Policy (OSTP) proposed to establish a National AI Research Resource (NAIRR) \nwith an estimated funding of USD 2.6 billion over the next six years (OECD.AI, 2023[8]). To connect AI \nresearchers to Federal resources that can support their research, the Networking and Information \nTechnology R&D Coordination Office, in partnership with Federal departments and agencies , created the \nAI Researchers Portal in 2022. The Porta l serves as  an online AI research platform that includes \ninformation about navigating federal research funding processes, data and computing resources, an AI \nresearch programme repository, and an AI R&D testbed inventory (NAIIO, 2022[95]). \nThe EU allocated EUR 1 billion per year for AI , including for R&D, within the Horizon Europe and Digital \nEurope programmes (European Commission, 2023[97]). This follows a Coordinated Plan on AI released in \n2018, in which all European Union Member States emphasise the importance of coordinating AI R&D \naction to maximise the impact of investments at both EU and national levels. The plan, which was reviewed \nin 2021, includes the development of “shared agendas for industry -academia collaborative AI R&D and \n\n50    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \ninnovation” (OECD.AI, 2023 [8]). Horizon Europe is the EU ’s main research and innovation funding \nprogramme. Within this programme, several calls support  research and innovation focusing on the \ndevelopment and deployment of AI , the increase of private and public investment, and the promotion of \ntrustworthy AI  (European Commission, 2020[98]). \nChina’s 2018 public AI R&D spending, including basic research through the National Natural Science \nFoundation of China and applied research through the National Key R&D Programmes, has been \nestimated to range from USD 1.7 billion USD to USD 5.7 billion (Center for Security and Emerging \nTechnology, 2019[99]). Although estimates for more recent years are lacking, it is likely that the budget for \nAI R&D in China is closer to the upper bound estimate and to have increased in recent years, i n line with \nthe country’s ambitions set in official strategic documents. The 2017 New Generation AI Plan sets the \nobjective of positioning China as the world’s primary innovation centre by 2030 , while China’s 14th Five-\nYear-Plan lists AI among the top priorities for scientific research. \nAI R&D-focused policies, plans, and programmes \nFrance launched it s National AI Research Programme in 2018 as part of  its National AI Strategy , the \ncoordination of which has been entrusted to the French National Institute for Research in Digital Science \nand Technology (INRIA) (Inria, 2023[100]) . From 2018 to 2022, EUR  445 million, or nearly 30% of the \nfunding allocated to the strategy, w ere dedicated to research, compared to EUR  134 million, or 8.7%, in \nthe second phase. Spain’s AI Strategy in RDI outlines specific priorities within the new Spanish Strategy \nfor Science, Technology, and Innovation for the period 2021 to 2028. These priorities will be implemented \nthrough defined initiatives and activities funded via the Science, Technology , and Innovation Strategic \nPlans. The goal is to mobilize  synergies among various  levels of public administration and through \ncooperation between the public and private sectors. (OECD.AI, 2023[8]).  Portugal’s R&D programme on \ndata science and AI in Public Administration , which was undertaken between 2018 an d 2020, aimed at  \ndeepening the processing of public data and stimulat ing the production of new knowledge relevant to \ncitizens based on the use of advanced techniques of AI and data science . Its main objectives were to \npromote deriving scientific knowledge from large amounts of data available in public administration, and to \nassist decision making processes and the definition of public policies in areas such as health, employment, \neducation, sustainable development, and road prevention. \nNational AI research institutes and centres \nSome national AI strategies call for the establishment of AI research institutes and centres of excellence \nsupporting AI R&D efforts. Countries have created national AI research institutes and centres to strengthen \ntheir AI research capabilities and to create interdisciplinary research communities. \nAs part of the Australian AI Action Plan, the Government of Australia has allocated AUD 124 million to \nestablish its National AI Centre to further develop Australia ’s AI and digital ecosystem (CSIRO, 2022[101]). \nCanada’s Pan-Canadian AI Strategy, launched in 2017 and renewed in 2021,  has a strong emphasis on \nresearch, talent, and commercialization . Through the strategy, Canada is investing  in three National AI \nInstitutes: Amii (Edmonton), the Vector Institute (Toronto), and Mila (Montréal). Funding by the strategy \nincludes CAD 60 million over five years (2021 to 2026) to help the institutes drive commercialization and \nadoption of AI, and CAD 208 million over ten years (2021 to 2031) for the Canadian Institute for Advanced \nResearch (CIFAR) and the institutes to attract, retain and develop academic research talent in AI, and \nadvance AI research (OECD.AI, 2023[8]).  \nIn February 2023, Korea opened the Research Data Centre of AI Innovation Hub, the country’s top-tier AI \nresearch network for AI research and workforce development. It consists of a computing lab with 35 \nPetaFlops, i.e., machines that allow up to 100 researchers to work on large -scale AI projects \nsimultaneously.  \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   51 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nBrazil’s Ministry of Science, Technology and Innovation (MCTI), the Sao Paolo Research Foundation and \nCGI.br are creating up to eight Applied Research Centres in AI (AIGO, 2022[4]). They will collectively invest \nBRL 1 million per year in each of the new Applied Research Centres for a period of up to ten years. Partner \nfirms will invest a matching amount, taking the total to BRL 2 0 million per Applied Research Centre. The \nfocus areas of the first centres will be Health (CIIA -Saúde and CEREIA), Agriculture (BIOS), Industry \n(IPT/SP, CIMATEC), and Smart Cities (IARA).  \nEgypt established an Applied Innovation Center (AIC) within the Mi nistry of Communications and \nInformation Technology to adopt AI in various fields and to identify innovative solutions to the challenges \nfaced by the Egyptian society. The AIC has already developed an application that uses AI in diagnosing \ndiabetic retinopathy and is currently in the process of developing an initiative aimed at the early detection \nof diabetic retinopathy for one million citizens. The AIC has also developed an Automatic Speech \nRecognition system for Arabic, which is being used in tribunals to transcribe judges’ sentences (OECD.AI, \n2023[8]). Peru has established a National Centr e for Innovation and AI  aimed at  accelerating the \ndevelopment and adoption of AI in the country. The centre has a mandate to carry out R&D in AI and to \ncoordinate its activities with national and international academ ia as well as with the private and public \nsector. \nThe United States’ National AI Institutes represent a cornerstone Federal Government commitment to \nfostering long-term, fundamental research in AI while also delivering significantly on each of the other eight \nobjectives in that strategy.  The programme represents a multisector effort led by the National Science \nFoundation (NSF), in partnership with the Simons Foundation (SF), the National Institute of Standards and \nTechnology (NIST), Department of Defense (DOD) Office of the Under Secretary of Defense for Research \nand Engineering (OUSD (R&E)),  Capital One Financial Corporation (Capital One),  and Intel Corporation \n(Intel) (National Science Foundation, 2023[102]).  \nReinforcing collaborative networks of experts and researchers  \nMany countries also seek to promote collaboration between experts and researchers in AI and ML through \nthe establishment of national and/or international networks. \nIn 2021, the German government created a Network of National Centres of Excellence for AI Research in \n2021 (ML2R, 2021 [103]). The Network includes the following six Institutes: (1) Berlin Institute for the \nFoundations of Learning and Data; (2) German Research  Centre for AI; (3) Munich Centre for Machine \nLearning; (4) Competence Centre Machine Learning Rhine-Ruhr; (5) Tübingen AI Centre (TUE.AI Centre); \nand (6) Competence Centre for Scalable Data Services and Solutions (ScaDS.AI) Dresden/Leipzig. The \nNetwork’s objectives are to increase synergies on AI Research through the exchange of competencies \nand research results, to implement joint activities, and to increase the national and international visibility \nof German AI research. The European Lighthouse on Secure and Safe AI (2022) – a joint initiative of the \nEU and the UK governments funded by Horizon Europe – is a virtual centre of excellence that brings \ntogether leading European experts in AI and machine learning  (OECD.AI, 2023 [8]). Brazil’s Ministry of \nScience, Technology and Innovation (MCTI) and the Brazilian Company for Industrial Research and \nInnovations (EMBRAPII) have established the MCTI / EMBRAPII Network of Innovation in AI to encourage \nAI use in the production process of the national industry. EMBRAPII Units support industry projects with \ninfrastructure and qualified professionals. Thus far , the Network has provided support for  286 projects \ninvolving 257 companies and successfully concluded 197 of these projects (AIGO, 2022[4]). \nFunds to support AI diffusion to businesses and the public sector \nCountries have also established specific funds to support AI R&D and the translation of theory to practice. \nSome of these funds are specific to AI, while others encompass AI among other new technologies such \nas blockchain and the Internet of Things. Furthermore, some of the funds focus on AI R&D to promote \ngreater adoption in the public sector, while others encourage greater adoption in business and industry. \n52    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nAs part of Denmark’s national AI strategy, the Danish government launched the Danish National Uptake \nFund for New Technologies (2019) to boost the use of AI and di ssemination of digital welfare solutions  \n(Agency for Digital Government, 2023 [104]). From 2020 to 2022, the National Uptake Fund has funded a \ntotal of 40 projects, which provide experience with the use of AI in the public sector a nd knowledge of \nchallenges when using the technology. The aim of these projects is to provide concrete experience with \nthe use of AI in the areas of welfare services, climate impact, and public administration . Türkiye’s AI \nEcosystem Call, launched in 2023 by the Ministry of Industry and the Technology & Scientific and \nTechnological Research Council, acts as a support model for facilitating collaboration between companies \nseeking AI solutions and relevant stakeholders across sectors. The model encourages companies to form \na consortium comprising a technology provider, a university research laboratory/centre or a public research \ncentre/institute with AI expertise, and the Scientific and Technological Research Council of Türkiye’s AI \nInstitute. Companies in need of AI solutions for sector-specific problems can actively seek out partners to \njoin their consortium. To expedite and optimize the process of addressing these needs, the AI Institute will \nprovide support by pairing experienced technology providers with u niversity research laboratories and \npublic research centres or institutes. The United Kingdom’s AI Sector Deal (2018-2027) is a GBP 1 billion \nsupport package from the UK government and industry to realise the potential of AI in the UK. It sets out \nactions to promote the adoption and the use of AI in the UK and delivers on the recommendations of the \nindependent AI review “Growing the AI industry in the UK” (UK Government, 2019[105]). The AI Sector Deal \naims to attract and retain both domestic and global AI talent , deliver major upgrades to digital and data \ninfrastructure, ensure that the UK has an enabling business environment , and contribute to communities’ \nprosperity by spreading the benefits of AI across the country.  \nFostering a digital ecosystem for AI (Principle 2.2) \n“Governments should foster the development of, and a ccess to, a digital ecosystem for trustworthy AI. Such \nan ecosystem includes in particular digital technologies and infrastructure, and mechanisms for sharing AI \nknowledge, as appropriate. In this regard, governments should consider promoting mechanisms, such as data \ntrusts, to support the safe, fair, legal and ethical sharing of data.” \nEmbracing AI-enabled transformation depends on the availability of data, infrastructure, and software to \ntrain and use AI models at scale. It is thus critical for countries to ensure they have sufficient AI compute \ncapacity to meet their needs to captur e AI’s full economic potential. Fostering a digital ecosystem for AI \nhence represents a crucial component of countries’ efforts to advance in their AI adoption. Countries have \nimplemented several policies to address this principle, including  linking data access and sharing policies \nwith AI policies, strengthening efforts to increase computing capacity and access to infrastructure, and \ninvesting in NLP technologies (Figure 4.4). \nFigure 4.4. Select policies that implement OECD AI Principle 2.2 on fostering a digital ecosystem \nfor AI \n \n• Norway's \"Resource Centre for Sharing of Data\"\n• France's \"Health Data Hub\"Data access and sharing\n• United States' \"National AI Research Resource Task \nForce\"\n• European Union's \"European High Performance \nComputing Joint Undertaking\"\nAccess to AI technologies and \ncomputing capacity\n• Several European Countries' \"Common Language \nResources and Technology Infrastructure (CLARIN \nResearch Infrastructure Consortium)\"\n• Denmark's \"Danish Gigaword Project\"\nInvestments in Natural \nLanguage Processing\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   53 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nLinking policies on data access and sharing with AI policies  \nAI requires a large amount of data to recognize patterns, learn, and make accurate predictions or decisions. \nLinking data policies to AI policies is important because it helps keep data ethical, protected, and safe from \nprivacy issues, while at the same time ensuring that AI can do its job effectively.  Several countries have \ntherefore begun to link their data access and sharing policies with AI policies.  \nThe Government of Colombia, in partnership with the World Economic Forum, created a data marketplace \nand infrastructure called the Moonshot project (OECD, 2022[93]). The project was developed as part of the \nNational Policy for Digital Transformation and AI – and specifically, its principle of easily accessible data \ninfrastructure allowing for the design and implementation of AI systems. As part of the Forum’s Data for \nCommon Purpose Initiative, the Colombia Moonshot project seeks to implement a data marketplace in the \ncountry based on the principles of auditability, equity, ethics, inclusion, transparency, and social and \nenvironmental responsibility.  The Czech Republic ’s National Strategy of Open Access t o Scientific \nInformation and Data (2017-2020) includes open access to publications and data requirements for publicly \nfunded research projects . Its objective is to define and implement a clear strategy for open access to \nscientific information for projects financed by public funds (OECD.AI, 2023[8]). \nThe French government created a data sharing commons (data.gouv.fr, 2023[106]), i.e., an open community \nplatform that aims to centralise and structure open data in France. The portal includes a selection of public \nand private data with an appropriate license to allow their use for research and innovation purpose s. \nTogether with the Ibero-American Data Protection Network, in 2019, Mexico launched specific guidelines \nfor compliance with the principles and rights that govern the protection of personal data in AI projects. As \nmentioned above (see p. 42) , Mexico’s Federal Institute for Access to Public Information and Data \nProtection (INAI) issued the “Recommendations for the Processing of Personal Data derived from the use \nof AI” in 2022. The recommendation aims at safeguarding the fulfilment of the data protection principles \nand duties when applying AI in different sectors such as education, public security, or health.  \nSweden’s Strategy for Enhanced Data Accessibility (2021) was formulated with the understanding that it \nis essential  to bolster Sweden ’s international competitiveness by enhancing  data sharing capabilities, \naccelerating data market development, and maintaining a flexible policy framework that can adapt to rapid \ntechnological advancements (OECD.AI, 2023 [8]). Peru’s National Network of Open Access Digital \nRepositories of Science, Technology, and Innovation (2013) is a national network for open access \nrepositories connecting 49 institutions and providing access to approximately 50 000 publications and \ndatasets (OECD, 2022 [93]). The Network’s objective is to promote collaboration among its members, \nthereby fostering open access, use and preservation of information and knowledge in science, technology \nand innovation. India’s Biological Data Storage, Access, and Sharing Policy (2019) establishes a \nframework and principles for data sharing that safeguard the rights of individuals and populations, ensuring \nno harm is caused to them. Its objective is to define guidelines for sharing data generate d by scientists in \nIndia using modern biotechnological tools and methods (OECD.AI, 2023[8]). \nAccess to AI technologies and computing capacity \nBoth researchers and industry need access to AI technologies and computing capabilities to develop \ninnovative solutions, solve complex problems, and gain competitive advantage s in the global economy. \nMany governments have therefore implemented policies providing these groups with access to their \ngovernment-owned AI technologies and computing capabilities. \nCanada’s Pan-Canadian AI Strategy, launched in 2017 and renewed in 2021, leverages Canada’s National \nAI Institutes—Amii (Edmonton), the Vector Institute (Toronto), and Mila (Montréal) —and supports the \nacquisition of HPC capacity dedicated for AI researchers. Canada ’s Digital Research Infrastructure \nStrategy, launched in 2019, fu nds the Digital Research Alliance of Canada, a new national not -for-profit \norganisation to advance and invest in national digital research infrastructure ( DRI) activities. Computing \n54    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nfacilities at University of Victoria, Simon Fraser University, University of Waterloo, University of Toronto, \nand McGill University were expanded as part of the Advanced Research Computing Expansion Program \nin 2019. (Digital Research Alliance of Canada, 2020[107]).  \nProviding the needed computation i nfrastructure for R&D in the academia, private and public sectors is \none of the key actions of Israel’s National AI Program (2022). Following a request for information for the \nestablishment of an HPC/compute infrastructure in 2021 (Israel Innovation Authority, 2021[108]), the country \nhas launched an advanced HPC R&D technology lab for the use of start -ups and academia in August \n2023. Israel’s Innovation Authority plans to invest up to NIS 30 million (USD 7.9 million) in its establishment \n(Israel Defense, 2023 [109]). In December 2022, Korea announced the K -Cloud Project Implementation \nStrategy, investing a total of approximately USD 650 million from 2023 to 2030 in the development of ultra-\nfast, low-power domestic AI chips. The goal is to apply them to data centres, thereby raising domestic \ncloud competitiveness while contributing to carbon neutrality and pro viding citizens with advanced AI \nservices. In 2022, Portugal, in collaboration with Google, launched “Calls for High Performance Computing \nR&D Projects: AI on the cloud”. The collaboration agreement resulted in the availability of USD 2 million in \nGoogle C loud platform credits over a period of two years for AI R&D projects, with 80% of resources \ndedicated to the ethics in AI and NLP.  \nIn 2022, the United Kingdom launched a review of its digital research infrastructure needs to support the \ndevelopment and use of AI, examining the provision of compute, data access, and talent, which will inform \nits ongoing national AI strategy (The Alan Turing Institute, 2022[110]). The review report, which was released \nin March 2023, called for a strategic vision and integrated compute ecosystem, significant investment in \npublic AI compute infrastructure, and the empowerment of the compute community through skills \nprogrammes and the attraction of leading AI talent (UK Government, 2023[111]).  In 2022, the United States \nDepartment of Energy launched the Frontier supercomputer as one of the world’s most powerful HPCs for \nAI applications (US Department of Energy, 2019 [112]). The National Science Foundation (NSF) invest s \nsignificantly in next -generation AI R&D supercomputers, such as Frontera, deployed in June 2019 \n(National Science Foundation, 2019 [113]), and provides programs for access to AI compute through the \nNational AI Research Institutes (National Science Foundation, 2022 [114]). The United States National AI \nInitiative Act of 2020 plans to make world-class computing resources and datasets available to researchers \nacross the country through the forthcoming United States National AI Research Resource (NAIRR). In \nJanuary 2023, an implementation plan was presented, proposing the democratisation of AI R&D through \nfunding a widely accessible AI compute infrastru cture with a budget of USD  2.6 billion over an initial six -\nyear period (US Government, 2023[115]).  \nThe Government of India established its MeitY Quantum Computing Applications Lab (2021) in \ncollaboration with Amazon Web Servi ces (MEIT, 2023[116]) to provide scientific, academic, and developer \ncommunities with access to a quantum computing development environment in the cloud .  \nInvestments in language technologies \nNatural Language Processing (NLP) refers to computer programs and tools that automate  natural \nlanguage functions by analysing, producing, modifying, or responding to human texts and speech. NLP is \na subset of AI that uses language as an input, produces language as an output, or both. Chatbots, machine \ntranslation systems, and virtual assis tants that recognise speech are all examples of NLP applications \n(OECD, 2023 [123]). Increasingly powerful NLP such as Large Language Models (LLMs) also raises \nsignificant policy challenges related to trustworthiness, including p rivacy, digital security, misinformation \nand disinformation, inclusion, as well as financial and environmental cost ( Figure 4.5). Limited access to \ndigitally readable text for most languages, which is essential for training models, could limit the benefits of \nsuch technology extending to various groups, including those using minority languages. To address this \nissue, several countries have launched initiatives to promote NLP in their national languages (Figure 4.5).  \n \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   55 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 4.5. Language Models: definition, risks, opportunities, policy considerations and national \ninitiatives \n \nSource: (OECD, 2023[123]), “AI language models: Technological, socio-economic and policy considerations”, https://doi.org/10.1787/13d38f92-\nen. \nFostering an enabling policy environment for AI (Principle 2.3) \na) Governments should promote a policy environment that supports an agile transition from the research an d \ndevelopment stage to the deployment and operation stage for trustworthy AI systems. To this effect, they \nshould consider using experimentation to provide a controlled environment in which AI systems can be tested, \nand scaled-up, as appropriate. \nb) Govern ments should review and adapt, as appropriate, their policy and regulatory frameworks and \nassessment mechanisms as they apply to AI systems to encourage innovation and competition for trustworthy \nAI. \n\n56    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 4.6. Select policies implementing OECD AI Principle 2.3 on Fostering an enabling policy \nenvironment for AI \n \nAs discussed in Chapter 2, countries are encouraging trustworthy AI use and innovation by reviewing and \nadapting existing policies and legislations, as well as by adopting AI-specific regulatory frameworks. They \nare also supporting an agile transition from R&D to commercialisation or deployment of AI by providing \ncontrolled environments for experimentation and testing of AI systems  \nFigure 4.6). \nAI skills, jobs and labour market transformation (Principle 2.4) \na) Governments should work closely with stakeholders to prepare for the transformation of the world of work \nand of society. They should empower people to effectively use and interact with AI systems across the breadth \nof applications, including by equipping them with the necessary skills. \nb) Governments should take steps, including through social dialogue, to ensure a fair transition for workers as \nAI is deployed, such as through training programmes along the working life, support for those affected by \ndisplacement, and access to new opportunities in the labour market. \nc) Governments should also work closely with stakeholder s to promote the responsible use of AI at work, to \nenhance the safety of workers and the quality of jobs, to foster entrepreneurship and productivity, and aim to \nensure that the benefits from AI are broadly and fairly shared. \nAI is already changing the nat ure of many aspects of life as it diffuses across sectors, particularly within \nthe context of labour, employment, and the workplace. Countries recognise that both managing a fair \ntransition of the labour market and leading in research, development , and adoption of AI requires policies \nfor AI skills development in tandem with talent attraction. Governments have mainly put in place initiatives \nto prepare the workforce with the skills required for AI through formal education programmes, training and \nlifelong learning initiatives. They also launched initiatives to attract and retain AI talent. Initiatives to monitor \nthe impact of AI in the labour market and to accompany transitions in the labour market appear limited to \ndate (Figure 4.7). \n• European Union's proposed \"AI Act\"\n• United States' \"AI Bill of Rights\"\nAI-specific regulatory \nframeworks\n• Japan's \"Social Principles of Human-Centric AI\"\n• Korea's \"Human-Centred National Guidelines for AI \nEthics\"\nGovernance norms, \nprinciples, and standards\n• Norway's \"Data Protection Authority Regulatory \nSandbox\"\n• Singapore's \"FinTech Regulatory Sandbox\"\nControlled environments for \nAI experimentation\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   57 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 4.7. Select policies that implement OECD AI Principle 2.4 on AI skills, jobs and labour \nmarket transformation \n \nFormal education programmes for STEM, AI and AI-related fields \nMany countries have invested in expanding formal education program mes. They have, for example, \ndesigned action plans for stronger digital, AI -focused curricula, developed new AI -centric degree \nprogrammes, and launched AI-specific scholarships. \nKorea, for example, announced the Comprehensive Strategy for Digital Workforce Development in August \n2022, aimed at supporting the expansion of digital education opportunities and digital capacity building  of \nall citizens and nurturing a total 1 million digital talent s by 2026. Moreover, i n February 2023, Korea \ndeveloped three types of AI ethics textbooks for students in elementary and secondary schools and three \ntypes of teacher manual for teaching AI ethics.  \nSome countries launched AI PhD programmes to support emerging researchers in AI and machine \nlearning. Australia has dedicated AUD 1.4 million to an industry-co-funded PhD scholarship programme \n– the Next Generation AI Graduates Programme  (2021-2027) – to attract and train the next generation of \nspecialist workers in AI by collaborating with industry on research projects and internships. Italy’s National \nPhD Programme in AI (PhD -AI.it) (2021) consists of five federated PhD courses that bring together 61  \nuniversities and research institutions  (OECD.AI, 2023[8]). The five courses share a common basis in the \nfoundations and developments of AI, while each course specialises in a different sector of AI application \n(Health and life sciences, Agrifood and environment, Security and cybersecurity, Industry 4.0, and Society) \n(OECD.AI, 2023[8]). Likewise, Israel has been supporting Master’s, PhD and Post -doc students in AI with \nscholarships in 2022 and 202 3. The United Kingdom  has been promoting 2 500 Master’s conversion \ncourses at 28 Higher Education Institutes across the country for applicants from near - and non-STEM \nbackgrounds, providing 1 000 scholarships for students from underrepresented groups, including women, \nblack, disabled and lower socio -economic backgrounds , to encourage greater diversity in AI careers. \nIndustry-funded AI Master’s programmes (2019) have also been established to broaden access to AI -\nrelated education and training and help galvanise future sources of talent (OECD.AI, 2023[8]). The Institute \nof Coding matches partners in industry interested in sponsoring places on Industrial Masters in AI (IMAI) \nwith higher education institutes able to provide additional  places on Master ’s programmes which meet \nindustry needs (OECD.AI, 2023[8]). \nUnder its Digital Europe Programme, the European Union funds actions to boost advanced digital skills \nin Europe, including in AI. With a total budget o f EUR 580 million for digital skills over seven years, the \n• Australia's \"Next Generation AI Graduates Program\"\n• Kenya's \"Digital Literacy Programme\"Formal education programmes\n• Japan's \"Practical Guidebook on Providing Data for Employee \nDevelopment in AI and Data Science\"\n• Singapore's \"AI for Industry\"\nTraining and lifelong learning\n• United States' \"AI Training for the Acquisition Workforce Bill\"\n• India's \"AWS Young Builders Challenge\"\nInitiatives to retain and attract AI \ntalent\n• Singapore's \"Guide to Job Redesign in the Age of AI\"\n• United States' \"American Workforce Policy Advisory Board\"\nMonitoring the impact of AI on \nthe labour market\n58    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nprogramme aims at enhancing co-operation between EU Member States and stakeholders in digital skills \nand jobs through specialised education programmes in key digital areas such as AI, blockchain, robotics, \nquantum and HPC. It also funds  short-term training courses tailored to the needs of businesses, with an \nemphasis on SMEs in Europe, as well as jobseekers and citizens looking to reskill. Since 2021, th e \nprogramme also supports the establishment of four new Master programmes on AI that focus on human \ncentric AI, AI ethics, AI for public sector and AI in healthcare through the Connecting Europe Facility (CEF) \nTelecom programme (OECD.AI, 2023[8]). \nCroatia’s European Social Fund Plus (ESF+) Programme 2021-2027 provides support for the application \nof digital techn ologies in education . The Programme includes curriculum development and digital \ntransformation of educational processes  to strengthen workforce skills and  to reinforce quality and \naccessible education related to the labour market (OECD.AI, 2023 [8]). In Egypt, developing human \ncapacity for AI is a key pillar of the national AI strategy (National Council for Artificial Intelligence, 2019[118]). \nThe Information Technology Institute developed and is implementing the AI Capacity Building Framework, \ntargeting different population groups, including young people, citizens, AI engineers and workers. In India, \nAmazon established the AWS Young Builders Challenge (2021) in partnership with the Ministry of \nEducation, the Atal Innovation Mission (AIM) , Niti Aayog, and the Central Board of Secondary Education  \n(OECD.AI, 2023 [8]). The Challenge seeks to enable school students in Ind ia to develop an early \nunderstanding and adoption of cloud computing and AI, inspire design and computational thinking, and \nhelp the rising generation develop a scientific calibre at a young age (AWS India, 2021[119]). \nTraining and lifelong learning AI and related programmes \nAlongside the increased focus on AI in formal education programs, governments have launched initiatives \nto raise the level of AI skills in the population through vocational training and lifelong learning.  \nChile’s Digital Talent programme (2019-2022) was established to connect companies, training institutions, \nand the government to develop new capacities in people, aligned with the demands of the digital economy, \nand generate more opportunities to access quality jobs  (OECD.AI, 2023[8]). Japan’s Practical Guidebook \non Data Provision for Fostering Human Resources of Experts  in AI and Data Science (METI, 2021[120]) \nfocuses on employee development in AI and data science. It summarises issues in four categories based \non increasing benefit and risk reduction for businesses  (OECD.AI, 2023[8]). AI Competence for  Sweden \n(2018) is a national initiative on AI education and competence that involves the collaboration between ten \nSwedish universities  (OECD.AI, 2023 [8]) to develop courses for professionals who can contribute to \nSweden’s development in the area  of AI (OECD.AI, 2023[8]). Furthermore, Sweden’s Digital Excellence \n(2019) represents a government assignment to the Swedish Agency for Economic and Regional Growth \nand the Swedish Higher Education Authority with the goal to enhance competencies of digital excellence \nin society and, more specifically, in the labour market. Singapore has three programmes to enhance skills \nof professionals. First, AI for Industry (AI4I) (2018) is a training programme designed to equip participants \nwith basic AI and data competency skills. It brings together the country’s AI research institutions, start-ups, \nand companies developing AI products . Second, the AI Apprenticeship Programme (AIAP)  consists of a \nfull-time 9-month programme to train local Singaporean AI talent in technology and in the skillsets needed \nto work with AI. Third, the Chartered AI Engineer (CAIE) designation (2020) is a professional qualification \nprogramme by the AI Professionals Association (AIP) to recognise and awar d credentials to working \nprofessionals in AI-related engineering roles (OECD.AI, 2023[8]).  \nInitiatives to retain and attract AI talent \nCanada was one of the early adopters of a skills focused strategy, supporting the attraction  and retention \nof leading academic talent in its Pan -Canadian AI Strategy first launched in 2017, and renewed in 2021, \nprogress reports indicated that CIFAR and Canada’s three National AI institutes had named 120 leading \nAI researchers, trained over 1,200 graduate students and post-doctoral fellows, and that Canada had been \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   59 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nranked 6th in the Global AI Talent Report (OECD.AI, 2023[121]). Germany, whose technical universities \nalready generate high numbers of homegrown AI talent, has made human capacity a key part of its National \nAI Strategy. The strategy aims to train 100 new professorships in AI as part of a federal STEM action plan \nconducted through nationally funded STEM clusters . It will open new centres of excellence and establish \na program me for international Master’s and PhD students through the German Academic Exchange \nService (OECD.AI, 2023[121]). In the United States, the AI Training for the Acquisi tion Workforce Bill (Bill \nS-2551) (2021) requires the Director of the Office of Management and Budget to provide an AI training \nprogramme for the acquisition workforce and other purposes (OECD.AI, 2023[8]). China has introduced its \nAI Innovation Action Plan for higher education to “significantly enhance China’s cadre of AI talent and its \nuniversity AI curricula by 2030” by optimising college infrastructure, delivering first -class AI research, and \noperating leading AI innovation c entres in universities (OECD.AI, 2023[121]). Attracting and developing AI \ntalent is also a key objective of China’s National New Generation AI Plan , which prioritises talent \ndevelopment, education, and skills acquisition (OECD.AI, 2023[121]). \nMonitoring the impact of AI on the labour market \nIn order to better prepare for changes, a few countries have launched efforts to measure the impact of AI \non the labour market. \nThe Office of the White House established the American Workforce Policy Advisory Board (2019) as part \nof a national initiative to help bridge the widening skills, driven in part by increased automation and the \ngrowing demand for high-tech skills. (OECD.AI, 2023[8]). The Board issues advice and recommendations \nto the National Council for the American Worker to encourage the private sector and educational \ninstitutions to combat the skills crisis through demand -driven education, training, and re -training. \nSingapore’s Guide to Job Redesign in the Age of AI  (2020) is a document that helps organisations and \nemployees understand how existing job roles can be redesigned to harness the potential of AI and increase \nthe value of their work (OECD.AI, 2023[8]).  \nInternational and multi-stakeholder co-operation on AI (Principle 2.5) \na) Governments, including developing countries and with stakeholders, should actively co-operate to advance \nthese principles and to progress on responsible stewardship of trustworthy AI.  \nb) Governments should work together in the OECD and other global and regional fora to foster the sharing of \nAI knowledge, as appropriate. They should encourage international, cross-sectoral and open multi-stakeholder \ninitiatives to garner long-term expertise on AI. \nc) Governments should promote the development of multi -stakeholder, consensus -driven global technical \nstandards for interoperable and trustworthy AI. \nd) Governments should also encourage the development, and their own use, of internationally comparable \nmetrics to measure AI research, development and deployment, and gather the evidence base to assess \nprogress in the implementation of these principles. \nCountries are increasingly engaged in international co -operation to promote the beneficial use of AI while \naddressing its challenges. This is happening through several types of initiatives, including: i) international \nresearch collaborations on AI, ii) trade agreements including language on AI, and  iii) co-operation for AI \ncapacity building in developing countries (Figure 4.8).  \nIntergovernmental organisations  (IGOs) play a key role in helping policy makers develop common \nvisions and solutions. Many of these organisations, with complementary mandates and memberships, are \nactively involved in AI initiatives and projects. The OECD and seven other treaty-based IGOs launched the \ncoalition GlobalPolicy.AI ( \n60    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 4.9), which includes an online platform to align their work and share information on the AI initiatives \nand projects undertaken by the respective  organisations (Globalpolicy.AI, 2023 [122]). Through this \ncollaboration, partner IGOs help policy  makers navigate various international initiatives, stay informed \nabout each other's AI policy activities , strive  to ensure interoperability between their work, and work \ntogether to advance trustworthy AI in areas where their mandates intersect.  \nFigure 4.8. Select policies implementing the OECD AI Principle 2.5 on international co-operation \n \nInternational AI research collaboration \nThe Global Partnership on AI  (GPAI) is an international and multi -stakeholder initiative jointly founded \nby Canada and France during their respective 2018 and 2019 G7 presidencies. GPAI was launched in \nJune 2020 to undertake cutting-edge research and pilot projects on AI priorities to advance the responsible \ndevelopment and use of AI. As of 2023, it counts 29 members (GPAI, 2020[123]). GPAI is led by a ministerial-\nlevel Council and a Steering Committee and is supported by a Secretariat hosted by the OECD, as well as \nby two Centres of Expertise: the International Centre of Expertise in Montreal for the Advancement of \nArtificial Intellig ence (ICEMAI) and the French National Institute for Research in Digital Science and \nTechnology (INRIA). GPAI brings together leading AI experts from industry, government, civil society, and \nacademia to collaborate across four current working groups on the themes of: i) responsible AI (including \na subgroup on AI and pandemic response), ii) data governance, iii) the future of work, and iv) innovation \nand commercialisation. The Montreal Centre of Expertise supports the first two working groups, while the \nParis Centre of Expertise supports the latter two working groups. \nCountries are promoting cross -border research collaboration on AI at the regional level. In Europe, \nMinisters responsible for digital development from Denmark, Estonia, Finland, the Faroe Islands , Iceland, \nLatvia, Lithuania, Norway, Sweden, and the Åland Islands released a Declaration on AI in the Nordic -\nBaltic Region (OECD.AI, 2023[8]). The Working Group on AI of the African Union (AU)  aims to unify \nviews, develop a single African AI strategy , and to create a joint capacity -building framework across the \ncontinent (OECD.AI, 2023 [8]). In the Japan-Singapore Economic Partnership Agreement (JSEPA) \nframework, signed in 2002, Japan and Singapore plan to build upon the two Memoranda of Cooperation \ncovering areas like AI, cybersecurity , and digital government transformation (OECD.AI, 2023 [8]). The \nQuadrilateral Security Dialogue (QUAD) is an informal strategic forum of the United States of America, \nIndia, Australia and Japan. In September 2021, it released the QUAD Principles on Technology Design, \nDevelopment, Governance, and Use  (White House, 2021 [124]). In the document, the signatories declare \ntheir commitment to facilitating the exchange of researchers and movement of highly skilled personnel, to \n• European Union's \"Horizon Europe\"\n• Several like-minded countries' \"Global Partnership on AI (GPAI)\"\nInternational AI research \ncollaboration\n• Canada's \"Montreal Declaration for Responsible AI\"\n• Korea's \"AI Ethics Policy Forum\"\nInternational and multi-\nstakeholder co-operation on AI\n• Chile, New Zealand and Singapore's \"Digital Economy \nPartnership Agreement (DEPA)\"\n• Australia and Singapore's \"Singapore-Australia Digital Economy \nAgreement (SADEA)\"\nTrade agreements that include AI\n• Germany's \"Artificial Intelligence for All - FAIR Forward\"\n• World Bank's \"Harnessing AI for Development\"\nCo-operation for AI capacity \nbuilding in developing countries\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   61 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nenhance science and technology collaboration, and to develop shared research and development \nagendas, joint projects and joint capacity building. \nInternational and multi-stakeholder co-operation on AI \nInternational research collaboration in AI has the potential to accelerate scientific progress by combining \nknowledge, resources, and expertise from different countries and cultures. It can help address and resolve \nethical, social, and political challenges associated with the global implementation of AI technologies on a \nbroader, international scale. Some countries have therefore launched initiatives to foster international and \nmulti-stakeholder co -operation on AI. In Canada, the Montreal Declaration for Responsible AI is an \ninitiative led by the Montreal AI Ethics Institute that seeks to establish a global consensus on the ethical \nand trustworthy use of AI. The declara tion has been signed by about 2 400 individuals and organisations \nfrom around the world (OECD.AI, 2023 [8]). In Korea, the AI Ethics Policy Forum involves a multi -\nstakeholder approach to spread the AI Ethics System. It establishe s a foundation for AI reliability \ntechnology and consists of three committees: (1) Proliferation of AI ethics system; (2) Laying the foundation \nfor trustworthy AI; and (3) Strengthen AI literary and ethics education (OECD.AI, 2023[8]). \nThe private sector is also coming together with other stakeholders. Partnership on AI is a collaboration \nbetween major tech companies like Amazon, Google , and Microsoft, as well as the civil society and non -\nprofit organisations such as the American  Civil Liberties Union (ACLU) and the Electronic Frontier \nFoundation (EFF), the academic community, and media organisations. The collaboration includes several \nmembers from Asia , including Baidu Research and the University of Tokyo. The partnership aims to  \npromote AI technologies that are beneficial to society, ethical, transparent, and trustworthy (PAI, 2023[125]). \nTrade agreements that include AI \nAI has an undisputable potential to drive innovation, help firms create new value  from data, and reduce \ntrade costs. Trade can also be an important mechanism through which countries and firms access the \ninputs needed to build AI systems (Ferencz, López González and Oliván García, 2022[126]). \nThe growing interest in AI systems increases the focus on the intersection of trade policy and AI, resulting \nin the enhanced presence of AI in the current trade policy deliberations. Some countries are leveraging \ntrade agreements to enhance co-operation on AI. In 2020, Chile, New Zealand and Singapore signed the \nDigital Economy Partnership Agreement (DEPA), which aims to promote the safe and responsible use \nof AI technologies (DEPA, 2020[127]). In 2023, Korea has agreed to join DEPA as of 2030. Australia and \nSingapore, building on their pre -existing trade agreement, signed the Singapore-Australia Digital \nEconomy Agreement (SADEA) in the same year, where parties agreed to advance their co-operation on \nAI (SADEA, 2020 [128]). Furthermore, the Korea-Singapore Digital Partnership Agreement (KSDPA)  \nentered into force in January 2023. In Article 14.28 of this digital economy agreement, both sides \nrecognised the importance of developing ethical governance fram eworks for the trusted, safe and \nresponsible use of AI technologies, and agreed to cooperate through promoting dialogue and sharing \nexperiences on regulations, policies, and initiatives relating to the use and adoption of AI technologies.  \nCo-operation for AI capacity building in developing countries \nTo ensure that the potential of AI is not only harnessed in the Global North, but also in countries of the \nGlobal South, some countries, international organi sations, and companies have co-operated around AI \ncapacity building in developing countries.  \nGermany’s “AI for All – FAIR Forward” (2019-2023) is a policy initiative launched by the Federal Ministry \nfor Economic Co -operation and Development focused on the open and sustainable development and \napplication of AI. It aims to strengthen local technical know -how on AI in Africa and Asia, improve access \nto training data and AI technologies for local innovation, and to develop policy frameworks for ethical AI, \n62    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \ndata protection, and privacy. To date, FAIR Forward is active in Ghana, Rwanda, Kenya, South Africa, \nUganda, Indonesia , and India . The partnering countries are pursuing objectives in co -operation that \ninclude: (1) building, expanding , and transferring knowledg e to AI; (2) improving access to training data \nand AI technology; and (3) developing political frameworks for ethical AI and improved data protection \n(OECD.AI, 2023[8]).  \nThe Asian and Pacific Training Centre for Information and  Communication Technology for Development \n(APCICT) was inaugurated in Korea in 2006 as a regional institute of the Economic and Social Commission \nfor Asia and the Pacific (ESCAP). Its main goal is to build ICT capacity and narrow the digital divide in the \nregion. The Centre provides developing countries in the Asia -Pacific region with ICT training, knowledge \nsharing and policy consultations. Among other things, the Centr e also provides an “Academy of ICT \nEssentials for Government Leaders (ACADEMY)” course. Under ACADEMY, the Frontier ICT for \nSustainable Development for Digital Leaders programme, which focuses on AI, big data and blockchain, \nwas developed and launched in September 2022. As of July 2023, a total of three sessions have been \nheld with 142 people finishing the programme. In June 2023, the Ethics of AI programme was developed, \nwith the goal of being unveiled in Indonesia in August 2023. Furthermore, APCICT courses are \nrequirements for central government official training programmes in 16 countries,  including Kazakhstan, \nthe Philippines, Cambodia, Bhutan , and Indonesia. The courses have been translated into 17 languages \nand distributed to 40 countries. \nThe United Nations Development Programme (UNDP)  has developed iVerify, an open -source, \nautomated fact-checking tool that can be used to help identify false information and prevent and mitigate \nits spread. iVerify is now listed as a Digital Public Good and has been deployed in Zambia, Kenya, and \nHonduras, with plans for deployment  in Liberia, to tackle m isinformation during elections. The UNDP is \nalso developing AI Readiness Assessment to help governments understand the current state of AI \nadoption through multi-stakeholder engagement, including with marginalized groups. Furthermore, UNDP \nsupports countries such as Moldova, Senegal, Mauritania, and Kenya in developing  Data Governance \nframeworks that foster responsible and inclusive use of data in policymaking (OECD.AI, 2023[8]). \nThe “Harnessing AI for Development” initiative, prepared with the support of the  Digital Development \nPartnership, is an ongoing work within the  World Bank’s Digital Development Global Practice . It aims to \nunderstand the role of governments in fostering AI development and adoption in developi ng country \ncontexts. The work highlights how governments are designing policy and regulatory frameworks around AI \nto support their unique development needs and make progress toward s reaching the 17 SDGs (World \nBank, 2023[129]). \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   63 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nFigure 4.9. Globalpolicy.AI: a platform for intergovernmental co-operation on AI  \n \n\n64    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \n \n  \n\nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   65 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nReferences \n \nAgency for Digital Government (2023), Signature Projects Under the Danish National Uptake \nFund for New Technologies, https://en.digst.dk/policy/new-technologies/national-uptake-\nfund-for-new-technologies/. \n[110] \nAI Standards Hub (2023), About the AI Standards Hub, https://aistandardshub.org/the-ai-\nstandards-hub/ (accessed on 13 March 2023). \n[58] \nAIGO (2023), Presentation at the third meeting of the OECD Working Party on Artificial \nIntelligence Governance (AIGO). \n[28] \nAIGO (2022), Presentation at second meeting of the OECD Working Party on Artificial \nIntelligence Governance (AIGO). \n[5] \nAitken et al. (2023), Common Regulatory Capacity for AI, \nhttps://doi.org/10.5281/zenodo.6838946 (accessed on 29 March 2023). \n[53] \nArtificial Intelligence and Data Science Committee (2020), [Final report of the committee on AI \nand data science], \nhttps://innovationisrael.org.il/sites/default/files/%D7%93%D7%95%D7%97%20%D7%A1%\nD7%95%D7%A4%D7%99%20%D7%A1%D7%99%D7%9B%D7%95%D7%9D%20%D7%\n95%D7%95%D7%A2%D7%93%D7%AA%20%D7%AA%D7%9C%D7%9D%20%D7%9C\n%D7%AA%D7%9B%D7%A0%D7%99%D7%AA%20%D7%9E%D7%95%D7%A4%20%D\n7%9C%D. \n[3] \nAtlantic Council (2021), Ukraine’s roadmap to an artificial intelligence future, \nhttps://www.atlanticcouncil.org/blogs/ukrainealert/ukraines-roadmap-to-an-artificial-\nintelligence-future/. \n[6] \nAustrian Government (2021), Strategie der Bundesregierung für Künstliche Intelligenz, \nhttps://www.bmk.gv.at/themen/innovation/publikationen/ikt/ai/strategie-\nbundesregierung.html (accessed on 28 March 2023). \n[71] \nAWS India (2021), Students present innovative ideas at AWS Young Builders Challenge 2021, \nhttps://www.aboutamazon.in/news/aws/students-present-innovative-ideas-at-aws-young-\nbuilders-challenge-2021 (accessed on 29 March 2023). \n[125] \nBen-Israel, I., E. Matania, E and L. Friedman (2020), The National Initiative for Secured \nIntelligent Systems to Empower the National Security and Techno-Scientific Resilience: A \nNational Strategy for Israel. Special Report to the Prime Minister, \nhttps://icrc.tau.ac.il/sites/cyberstudies-\nenglish.tau.ac.il/files/media_server/cyber%20center/The%20National%20Initiative_eng%2\n[2] \n66    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \n02021_digital.pdf. \nBertuzzi, L. (2023), EU lawmakers set to settle on OECD definition for Artificial Intelligence, \nhttps://www.euractiv.com/section/artificial-intelligence/news/eu-lawmakers-set-to-settle-on-\noecd-definition-for-artificial-intelligence/. \n[40] \nCAIDP (2021), Artificial Intelligence and Democratic Values Index, \nhttps://www.caidp.org/reports/aidv-2021/ (accessed on 29 March 2023). \n[43] \nCanadian Government (2023), Algorithmic Impact Assessment tool, \nhttps://www.canada.ca/en/government/system/digital-government/digital-government-\ninnovations/responsible-use-ai/algorithmic-impact-assessment.html (accessed on \n28 March 2023). \n[92] \nCanadian Government (2023), The Artificial Intelligence and Data Act (AIDA) – Companion \ndocument, https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-\nintelligence-and-data-act-aida-companion-document (accessed on 29 March 2023). \n[16] \nCanadian Parliament (2022), An Act to enact the Consumer Privacy Protection Act, the \nPersonal Information and Data Protection Tribunal Act and the Artificial Intelligence and \nData Act and to make consequential and related amendments to other Acts , \nhttps://www.parl.ca/legisinfo/en/bill/44-1/c-27 (accessed on 28 March 2023). \n[13] \nCenter for Security and Emerging Technology (2021), Ethical Norms for New Generation \nArtificial Intelligence Released, https://cset.georgetown.edu/publication/ethical-norms-for-\nnew-generation-artificial-intelligence-released/. \n[136] \nCenter for Security and Emerging Technology (2019), Chinese Public AI R&D Spending: \nProvisional Findings, https://cset.georgetown.edu/wp-content/uploads/Chinese-Public-AI-\nRD-Spending-Provisional-Findings-1.pdf. \n[105] \nChavez, K., J. Bahr and T. Vartanian (2022), AI has made its way to the workplace. So how \nhave laws kept pace?, https://oecd.ai/en/wonk/workplace-regulation-2022. \n[90] \nChinese Government (2022), Opinion on Strengthening the Ethics and Governance in Science \nand Technology, http://www.gov.cn/zhengce/2022-03/20/content_5680105.htm (accessed \non 29 March 2023). \n[46] \nChinese Government (2021), Data Security Law, \nhttp://www.npc.gov.cn/englishnpc/c23934/202112/1abd8829788946ecab270e469b13c39c.\nshtml (accessed on 29 March 2023). \n[51] \nCivic Coding (2018), Über uns, https://www.civic-coding.de/ueber-uns (accessed on \n28 March 2023). \n[70] \nCouncil of Europe (2023), Consolidated Working Draft of the Framework Convention on \nArtificial Intelligence, Human Rights, Democracy and the Rule of Law, \nhttps://rm.coe.int/cai-2023-18-consolidated-working-draft-framework-\nconvention/1680abde66. \n[42] \nCouncil of Europe (2020), Recommendation of the Committee of Ministers to member States \non the human rights impacts of algorithmic systems, \nhttps://search.coe.int/cm/pages/result_details.aspx?objectid=09000016809e1154 \n(accessed on 28 March 2023). \n[78] \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   67 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nCourt des Comptes (2023), La stratégie nationale de recherche en intelligence artificielle, \nCourt des comptes, https://www.ccomptes.fr/fr/publications/la-strategie-nationale-de-\nrecherche-en-intelligence-artificielle. \n[12] \nCSIRO (2022), National Artificial Intelligence Centre, https://www.csiro.au/en/work-with-\nus/industries/technology/national-ai-centre. \n[107] \nData & Trust Alliance (2022), Data & Trust Alliance, https://dataandtrustalliance.org/. [50] \ndata.gouv.fr (2023), Plateforme ouverte des données publiques françaises, \nhttps://www.data.gouv.fr/fr/. \n[112] \nDatatilsynet (2021), Sandbox for responsible artificial intelligence, \nhttps://www.datatilsynet.no/en/regulations-and-tools/sandbox-for-artificial-intelligence/. \n[63] \nDatatilsynet (2020), Sandbox for responsible artificial intelligence, \nhttps://www.datatilsynet.no/en/regulations-and-tools/sandbox-for-artificial-\nintelligence/#:~:text=The%20main%20mission%20for%20the%20Data%20Protection%20A\nuthority%27s,full%20openness%20about%20the%20assessments%20that%20are%20ma\nde. (accessed on 29 March 2023). \n[62] \nDemocratic Society (2022), Scottish Artificial Intelligence (AI) Co-Creation Public Engagement \nWorkshops, https://www.demsoc.org/projects/scottish-artificial-intelligence-ai-co-creation-\npublic-engagement-workshops (accessed on 28 March 2023). \n[73] \nDEPA (2020), Digital Economy Partnership Agreement, https://www.mti.gov.sg/-\n/media/MTI/Microsites/DEAs/Digital-Economy-Partnership-Agreement/Text-of-the-\nDEPA.pdf. \n[133] \nDigital Research Alliance of Canada (2020), Canadian Digital Research Infrastructure Needs \nAssessment, https://alliancecan.ca/en/initiatives/canadian-digital-research-infrastructure-\nneeds-assessment. \n[113] \nDutch Government (2021), Fundamental Rights and Algorithms Impact Assessment (FRAIA), \nhttps://www.government.nl/documents/reports/2021/07/31/impact-assessment-\nfundamental-rights-and-algorithms (accessed on 28 March 2023). \n[77] \nDutch Ministry of Internal Affairs (2021), Non-discrimination by design, \nhttps://www.tilburguniversity.edu/sites/default/files/download/04%20handbook%20non -\ndiscrimination%20by%20design%28ENG%29.pdf. \n[84] \nEuropean Commission (2023), A European approach to artificial intelligence, https://digital-\nstrategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence. \n[103] \nEuropean Commission (2022), AI Liability Directive, https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/?uri=CELEX:52022PC0496 (accessed on 29 March 2023). \n[138] \nEuropean Commission (2022), Proposal Product Liability Directive, https://eur-\nlex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2022%3A495%3AFIN (accessed on \n29 March 2023). \n[137] \nEuropean Commission (2020), Horizon Europe, European Commision, https://research-and-\ninnovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-\ncalls/horizon-europe_en. \n[104] \n68    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nEuropean Commission (2021b), Coordinated Plan on Artificial Intelligence 2021 Review, \nhttps://digital-strategy.ec.europa.eu/en/library/coordinated-plan-artificial-intelligence-2021-\nreview. \n[38] \nEuropean Commission (2021a), Proposal for a Regulation of the European Parliament and of \nthe Council laying down Harmonised Rules on Artificial Intelligence and amending certain \nUnion Legislative Acts (Artificial Intelligence Act), https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/?uri=CELEX:52021PC0206 (accessed on 28 March 2023). \n[37] \nEuropean Council (2022), Artificial Intelligence Act: Council calls for promoting safe AI that \nrespects fundamental rights, https://www.consilium.europa.eu/en/press/press-\nreleases/2022/12/06/artificial-intelligence-act-council-calls-for-promoting-safe-ai-that-\nrespects-fundamental-rights/ (accessed on 29 March 2023). \n[39] \nEuropean Parliament (2023), DRAFT Compromise Amendments on the Draft Report - \nProposal for a regulation of the European Parliament and of the Council on harmonised \nrules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union \nLegislative Acts, \nhttps://www.europarl.europa.eu/meetdocs/2014_2019/plmrep/COMMITTEES/CJ40/DV/202\n3/05-11/ConsolidatedCA_IMCOLIBE_AI_ACT_EN.pdf. \n[41] \nEuropean Union (2022), Digital Services Act (DSA) Package, https://digital-\nstrategy.ec.europa.eu/en/policies/digital-services-act-package (accessed on \n28 March 2023). \n[87] \nEuropean Union (2018), General Data Protection Regulation (GDPR), https://eur-\nlex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32016R0679 (accessed on \n28 March 2023). \n[91] \nExecutive Office of the President (2020), Promoting the Use of Trustworthy Artificial \nIntelligence in the Federal Government, \nhttps://www.federalregister.gov/documents/2020/12/08/2020-27065/promoting-the-use-of-\ntrustworthy-artificial-intelligence-in-the-federal-government. \n[35] \nFasken (2022), The Regulation of Artificial Intelligence in Canada and Abroad: Comparing the \nProposed AIDA and EU AI Act, https://www.fasken.com/en/knowledge/2022/10/18-the-\nregulation-of-artificial-intelligence-in-canada-and-abroad (accessed on 29 March 2023). \n[14] \nFerencz, J., J. López González and I. Oliván García (2022), “Artificial Intelligence and \ninternational trade: Some preliminary implications”, OECD Trade Policy Papers, Vol. No. \n260, https://doi.org/10.1787/13212d3e-en. \n[132] \nFond Darstellende Künste (2021), AUTONOM - Symposium zu Künstlicher Intelligenz in den \nDarstellenden Künsten, https://www.fonds-daku.de/events-und-diskurs/archiv/autonom-\nsymposium/ (accessed on 28 March 2023). \n[75] \nForum IA Quebec (2023), Artificial intelligence will change the world, \nhttps://forumia.quebec/en/ (accessed on 28 March 2023). \n[69] \nFrench Government (2021), Aide pour la maîtrise et la diffusion numérique dans le cadre de « \nIA Booster », https://www.economie.gouv.fr/plan-de-relance/mesures/aide-maitrise-\ndiffusion-numerique-ia-booster (accessed on 28 March 2023). \n[67] \nGerman Government (2022), Regulatory Sandboxes – Testing Environments for Innovation [60] \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   69 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nand Regulation, https://www.bmwk.de/Redaktion/EN/Dossier/regulatory-sandboxes.html \n(accessed on 29 March 2023). \nGerman Government (n.d.), Unsere Förderinitiative “KI-Leuchttürme für Umwelt, Klima, Natur \nund Ressourcen”, https://www.bmuv.de/themen/nachhaltigkeit-\ndigitalisierung/digitalisierung/unsere-foerderinitiative-ki-leuchttuerme (accessed on \n28 March 2023). \n[74] \nGlobalpolicy.AI (2023), https://globalpolicy.ai/en/, https://globalpolicy.ai/en/. [128] \nGovernment of Canada (2023), Advisory Council on Artificial Intelligence, https://ised-\nisde.canada.ca/site/advisory-council-artificial-intelligence/en. \n[10] \nGovernment of Canada (2023), Artificial Intelligence and Data Act - Companion Document, \nhttps://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-\nact-aida-companion-document. \n[89] \nGovernment of Canada (2023), Public Awareness Working Group, https://ised-\nisde.canada.ca/site/advisory-council-artificial-intelligence/en/public-awareness-working-\ngroup (accessed on 28 March 2023). \n[72] \nGPAI (2020), Global Partnersip on Artificial Intelligence, https://www.gpai.ai/. [129] \nHabuka, H. (2023), Japan’s Approach to AI Regulation and Its Impact on the 2023 G7 \nPresidency, Center for Strategic and International Studies (CSIS), \nhttps://www.csis.org/analysis/japans-approach-ai-regulation-and-its-impact-2023-g7-\npresidency. \n[86] \nHolistic AI (2023), Making Sense of China’s AI Regulations, \nhttps://www.holisticai.com/blog/china-ai-regulation. \n[47] \nIbero-American Data Protection Network (2019), General Recommendations for the \nProcessing of Personal Data in Artificial Intelligence, \nhttps://www.redipd.org/sites/default/files/2020-02/guide-general-recommendations-\nprocessing-personal-data-ai.pdf. \n[80] \nIbero-American Data Protection Network (2019), Specific Guidelines for Compliance with the \nPrinciples and Rights that Govern the Protection of Personal Data in Artificial Intelligence \nProjects, https://www.redipd.org/sites/default/files/2020-02/guide-specific-guidelines-ai-\nprojects.pdf. \n[81] \nINAI (2022), Recomendaciones para et tratamiento de datos pesonales derivado del uso de la \nInteligencia Artificial, https://home.inai.org.mx/wp-\ncontent/documentos/DocumentosSectorPublico/RecomendacionesPDP-IA.pdf. \n[79] \nIndian Government (2023), IndiaAI, https://indiaai.gov.in/ (accessed on 28 March 2023). [7] \nInria (2023), Programme national de recherche en intelligence artificielle, \nhttps://www.inria.fr/fr/programme-national-recherche-intelligence-artificielle. \n[106] \nISO (2022), ISO/IEC 23053:2022 Framework for Artificial Intelligence (AI) Systems Using \nMachine Learning (ML), https://www.iso.org/standard/74438.html. \n[56] \nISO (2009), ISO 31000 Risk management, https://www.iso.org/iso-31000-risk- [57] \n70    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nmanagement.html. \nIsrael Defense (2023), Next Silicon will establish a national R&D laboratory for acceleration \ndevelopment of technologies for supercomputing, \nhttps://www.israeldefense.co.il/node/59291 (accessed on 21 September 2023). \n[115] \nIsrael Innovation Authority (2021), [National Infrastructure for supercomputing (HPC) - \nRequest for Information], https://www.law.co.il/media/computer-\nlaw/innovation_authority_doc1.pdf. \n[114] \nIsraeli government (2023), [New interagency team looking at the use of AI in the financial \nsector], https://www.gov.il/he/departments/news/ai-team. \n[18] \nJapanese Government (2022), AI Strategy 2022, \nhttps://www8.cao.go.jp/cstp/ai/aistratagy2022en.pdf. \n[4] \nJapanese Government (2019), Social Principles of Human-Centric AI, \nhttps://www8.cao.go.jp/cstp/stmain/aisocialprinciples.pdf (accessed on 29 March 2023). \n[19] \nJapanese government (2023), Summary of AI issues, \nhttps://www8.cao.go.jp/cstp/ai/ronten_youshi_yaku.pdf. \n[25] \nKI Bundesverband (2019), KI Gütesiegel, https://ki-verband.de/wp-\ncontent/uploads/2019/02/KIBV_Guetesiegel.pdf (accessed on 28 March 2023). \n[83] \nLandry et al. (2022), Bill C-27 – Canada’s proposed Artificial Intelligence and Data Act, \nhttps://www.stewartmckelvey.com/thought-leadership/bill-c-27-canadas-proposed-artificial-\nintelligence-and-data-act/ (accessed on 29 March 2023). \n[15] \nLaskai, L. and G. Webster (2019), Translation: Chinese Expert Group Offers ’Governance \nPrinciples’ for ’Responsible AI’, https://www.newamerica.org/cybersecurity-\ninitiative/digichina/blog/translation-chinese-expert-group-offers-governance-principles-\nresponsible-ai/. \n[11] \nMEIT (2023), MeitY Quantum Computing Applications Lab, \nhttps://quantumcomputing.negd.in/. \n[122] \nMETI (2021), Practical Guidebook on Data Provision for Fostering Human Resources of \nExperts in AI and Data Science, https://www.meti.go.jp/english/press/2021/0301_003.html. \n[126] \nMIC (2023), G7 Hiroshima AI Process, G7 Digital & Tech Ministers’ Statement (September 7, \n2023), https://www.soumu.go.jp/main_content/000900470.pdf (accessed on \n7 September 2023). \n[23] \nMIC (2023), Results of the G7 Digital and Tech Ministers’ Meeting in Takasaki, Gunma. , \nhttps://www.soumu.go.jp/joho_kokusai/g7digital-tech-2023/en/topics/topics_20230430.htm. \n[21] \nMinistry of Economy, Trade and Industry (2022), Governance Guidelines for Implementation \nof AI Principles (Ver. 1.1), \nhttps://www.meti.go.jp/shingikai/mono_info_service/ai_shakai_jisso/pdf/20220128_2.pdf . \n[20] \nMinistry of Foreign Affairs of Japan (2023), G7 Hiroshima Leaders’ Communiqué, \nhttps://www.mofa.go.jp/ms/g7hs_s/page1e_000690.html (accessed on 6 July 2023). \n[22] \nMinistry of Innovation, Science and Technology and Ministry of Justice, the Office of legal [17] \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   71 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \ncounsel and legislative affairs (2022), Principles for Policy, Regulation and Ethics in AI: \ntechnical paper (annex to policy paper), \nhttps://www.gov.il/BlobFolder/rfp/061122/he/professional-letter.pdf. \nML2R (2021), Network of German AI Centres strengthens Scientific Exchange, \nhttps://www.ml2r.de/en/network-of-german-ai-centres-strengthens-scientific-exchange/. \n[109] \nNAIIO (2022), AI Researchers Portal, https://www.ai.gov/ai-researchers-portal/. [102] \nNational Artificial Intelligence Initiative Office (2023), ABOUT, \nhttps://www.ai.gov/about/#NAIIO_-_National_Artificial_Intelligence_Initiative_Office. \n[9] \nNational Council for Artificial Intelligence (2019), Egypt National Artificial Intelligence Strategy, \nhttps://mcit.gov.eg/Upcont/Documents/Publications_672021000_Egypt-National-AI-\nStrategy-English.pdf. \n[124] \nNational Council of Science and Technology (2022), The Networking & Information \nTechnology R&D Program and the National Artificial Intelligence Initiative Office \nSupplement to the President’s FY 2023 Budget, https://www.nitrd.gov/pubs/FY2023-\nNITRD-NAIIO-Supplement.pdf. \n[100] \nNational Institute of Standards and Technologies - US Department of Commerce (2023), AI \nRisk Management Framework, https://www.nist.gov/itl/ai-risk-management-framework \n(accessed on 28 March 2023). \n[33] \nNational Science Foundation (2023), National Artificial Intelligence Research Institutes, \nhttps://new.nsf.gov/funding/opportunities/national-artificial-intelligence-research. \n[108] \nNational Science Foundation (2022), National Artificial Intelligence Research Institutes, \nhttps://beta.nsf.gov/funding/opportunities/national-artificial-intelligence-research-institutes. \n[120] \nNational Science Foundation (2019), NSF-funded leadership-class computing center boosts \nU.S. science with largest academic supercomputer in the world, \nhttps://www.nsf.gov/news/news_summ.jsp?cntn_id=299134. \n[119] \nNIST (2023), Artificial Intelligence Risk Management Framework (AI RMF 1.0), \nhttps://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf. \n[55] \nNITRD (2023), ARTIFICIAL INTELLIGENCE R&D INVESTMENTS, FISCAL YEAR 2018 – \nFISCAL YEAR 2023, https://www.nitrd.gov/apps/itdashboard/ai-rd-investments/#Chart-2-\nFederal-Budget-for-Nondefense-AI-RD-FYs-2018-2023 (accessed on  2023). \n[101] \nOECD (2023), “Advancing accountability in AI: Governing and managing risks throughout the \nlifecycle for trustworthy AI”, OECD Digital Economy Papers, No. 349, \nhttps://doi.org/10.1787/2448f04b-en. \n[66] \nOECD (2023), “AI language models: Technological, socio-economic and policy \nconsiderations”, Digital Economy Papers, No. 352, https://doi.org/10.1787/13d38f92-en. \n[123] \nOECD (2023), Catalogue of AI Tools & Metrics, https://oecd.ai/en/catalogue/overview. [96] \nOECD (2023), “Emerging privacy-enhancing technologies: Current regulatory and policy \napproaches”, OECD Digital Economy Papers, No. 351, https://doi.org/10.1787/bf121be4-\nen. \n[82] \n72    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nOECD (2023), G7 Hiroshima Process on Generative Artificial Intelligence (AI): Towards a G7 \nCommon Understanding on Generative AI, OECD Publishing, \nhttps://doi.org/10.1787/bf3c0c60-en. \n[24] \nOECD (2023), “Regulatory sandboxes in Artificial Intelligence”, OECD Digital Economy \nPapers, No. 356, https://doi.org/10.1787/8f80a0e6-en. \n[59] \nOECD (2022), “OECD Framework for the Classification of AI systems”, OECD Digital \nEconomy Papers, No. 323, https://doi.org/10.1787/cb6d9eca-en. \n[34] \nOECD (2022), The Strategic and Responsible Use of Artificial Intelligence in the Public Sector \nof Latin America and the Caribbean, OECD Publishing, Paris, \nhttps://doi.org/10.1787/22190414. \n[93] \nOECD (2021), OECD Business and Finance Outlook 2021 : AI in Business and Finance , \nhttps://www.oecd.org/finance/oecd-business-and-finance-outlook-26172577.htm. \n[61] \nOECD (2021), “State of Implementation of the OECD AI Principles: Insights from National AI \nPolicies”, OECD Digital Economy Papers, No. 311, https://doi.org/10.1787/1cd40c44-en. \n[1] \nOECD (2019), OECD Recommendation of the Council on Artificial Intelligence, \nOECD/LEGAL/0449, https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449. \n[65] \nOECD.AI (2023), Database of national AI policies & strategies, \nhttps://oecd.ai/en/dashboards/overview (accessed on 5 October 2023). \n[8] \nOECD.AI (2023), Live data: AI jobs and skills, https://oecd.ai/en/data?selectedArea=ai-jobs-\nand-skills&selectedVisualization=relative-international-ai-skill-demand (accessed on \n13 March 2023). \n[127] \nOECD.AI (2023), Live data: AI research, https://oecd.ai/en/data?selectedArea=ai-\nresearch&selectedVisualization=ai-publications-by-country-over-time and \nhttps://oecd.ai/en/data?selectedArea=ai-research&selectedVisualization=ai-publication-\ntime-series-by-institution (accessed on 13 March 2023). \n[98] \nOECD.AI (2023), Live data: investments in AI and data, \nhttps://oecd.ai/en/data?selectedArea=investments-in-ai-and-data (accessed on \n13 March 2023). \n[48] \nPAI (2023), Partnership on Artificial Intelligence, https://partnershiponai.org/. [131] \nProteccion Data (2022), AESIA, https://protecciondata.es/agencia-espanola-supervision-\ninteligencia-artificial-aesia/. \n[97] \nRoberts et al. (2021), “Achieving a ‘Good AI Society’: Comparing the Aims and Progress of the \nEU and the US”, Science and Engineering Ethics, \nhttps://link.springer.com/article/10.1007/s11948-021-00340-7. \n[44] \nSADEA (2020), Memorandum of Understanding between the Government of the Republic of \nSingapore and the Government of Australia on Cooperation on Artificial Intelligence , \nhttps://www.mti.gov.sg/-/media/MTI/Microsites/DEAs/Singapore-Australia-Digital-Economy-\nAgreement/MOUs/MOU-on-Cooperation-onArtificial-Intelligence.pdf. \n[134] \nSalvi del Pero, A., P. Wyckoff and A. Vourc’h (2022), Using Artificial Intelligence in the \nworkplace: What are the main ethical risks?, OECD Publishing, \n[88] \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   73 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nhttps://doi.org/10.1787/840a2d9f-en. \nShaughnessy et al. (2023), Lessons From the World’s Two Experiments in AI Governance, \nhttps://carnegieendowment.org/2023/02/14/lessons-from-world-s-two-experiments-in-ai-\ngovernance-pub-89035 (accessed on 9 March 2023). \n[45] \nSingapore Government (2023), Overview of Regulatory Sandbox, \nhttps://www.mas.gov.sg/development/fintech/regulatory-sandbox (accessed on \n29 March 2023). \n[64] \nStanford University (2021), The AI Index Report, https://aiindex.stanford.edu/ai-index-report-\n2021/ (accessed on 28 March 2023). \n[76] \nThe Alan Turing Institute (2023), Women in Data Science and AI, \nhttps://www.turing.ac.uk/research/research-projects/women-data-science-and-ai-new \n(accessed on 28 March 2023). \n[68] \nThe Alan Turing Institute (2022), UK AI Research Infrastructure Requirements Review, \nhttps://www.turing.ac.uk/work-turing/uk-ai-research-infrastructure-requirements-review. \n[116] \nThe White House (2023), FACT SHEET: Biden-⁠Harris Administration Secures Voluntary \nCommitments from Leading Artificial Intelligence Companies to Manage the Risks Posed \nby AI, https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-\nsheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-\nintelligence-companies-to-manage-the-risks-posed-by-ai/. \n[36] \nTurkish Government (2019), National Data Dictionary, https://cbddo.gov.tr/en/project-\ndescription/4605/national-data-dictionary- (accessed on 28 March 2023). \n[94] \nUK Government (2023), AI regulation: a pro-innovation approach, \nhttps://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach \n(accessed on 29 March 2023). \n[27] \nUK Government (2023), Independent Review of The Future of Compute: Final report and \nrecommendations, https://www.gov.uk/government/publications/future-of-compute-\nreview/the-future-of-compute-report-of-the-review-of-independent-panel-of-experts. \n[117] \nUK Government (2022), Establishing a pro-innovation approach to regulating AI, \nhttps://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-\nregulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement \n(accessed on 2023 March 2023). \n[26] \nUK Government (2022), National AI Strategy, \nhttps://www.gov.uk/government/publications/national-ai-strategy (accessed on \n29 March 2023). \n[29] \nUK Government (2020), Review into bias in algorithmic decision-making, \nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_d\nata/file/957259/Review_into_bias_in_algorithmic_decision-making.pdf (accessed on \n28 March 2023). \n[85] \nUK Government (2019), Policy paper: AI Sector Deal, \nhttps://www.gov.uk/government/publications/artificial-intelligence-sector-deal/ai-sector-\ndeal#further-information. \n[111] \n74    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nUK Parliament (2023), The Data Protection and Digital Information (No. 2) Bill 2022-2023, \nhttps://commonslibrary.parliament.uk/research-briefings/cbp-9746/ (accessed on \n28 March 2023). \n[30] \nUS Congress (2022), Algorithmic Accountability Act (AAA), \nhttps://www.congress.gov/bill/117th-congress/house-bill/6580 (accessed on \n28 March 2023). \n[31] \nUS Department of Energy (2019), U.S. Department of Energy and Cray to Deliver Record-\nSetting Frontier Supercomputer at ORNL, https://www.energy.gov/articles/us-department-\nenergy-and-cray-deliver-record-setting-frontier-supercomputer-ornl. \n[118] \nUS Government (2023), United States National AI Research Resource (NAIRR), \nhttps://www.ai.gov/nairrtf/ (accessed on 29 March 2023). \n[121] \nUS Government (2022), Blueprint for an AI Bill of Rights, https://www.whitehouse.gov/ostp/ai-\nbill-of-rights/ (accessed on 29 March 2023). \n[32] \nUS Government (2021), Automated Vehicles Comprehensive Plan, \nhttps://www.transportation.gov/av/avcp (accessed on 29 March 2023). \n[49] \nUS Presidential Office (2019), Maintaining American Leadership in Artificial Intelligence, \nhttps://www.federalregister.gov/documents/2019/02/14/2019-02544/maintaining-american-\nleadership-in-artificial-intelligence (accessed on 28 March 2023). \n[95] \nWhite House (2021), Quad Principles on Technology Design, Development, Governance, and \nUse, https://www.whitehouse.gov/briefing-room/statements-releases/2021/09/24/quad-\nprinciples-on-technology-design-development-governance-and-\nuse/#:~:text=Build%20trust%2C%20integrity%2C%20and%20resilience,integrity%2C%20a\nnd%20resilience%20foster%20innovation. \n[130] \nWorld Bank (2023), Digital Development, \nhttps://www.worldbank.org/en/topic/digitaldevelopment/overview (accessed on \n29 March 2023). \n[135] \nYamashita, I. and et al. (2021), “Measuring the AI content of government-funded R&D \nprojects: A proof of concept for the OECD Fundstat initiative”, OECD Science, Technology \nand Industry Working Papers, Vol. No. 2021/09, https://doi.org/10. \n[99] \nYang (2023), China just set up a new bureau to mine data for economic growth, \nhttps://www.technologyreview.com/2023/03/15/1069814/china-new-bureau-data-economiy/ \n(accessed on 29 March 2023). \n[54] \nZhang (2021), Provisions on the Administration of Algorithm Recommendation of Internet \nInformation Services in China, https://www.lexology.com/library/detail.aspx?g=08867c3e-\n7ded-43d2-af2b-\n8f09878ef7a6#:~:text=Provisions%20on%20the%20Administration%20of%20Algorithm%2\n0Recommendation%20of,information%20services%20...%203%20V.%20Legal%20liability\n%20 (accessed on 29 March 2023). \n[52] \n \n \n \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   75 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nAnnex A.  \nTable A.1. AI definitions, risk classifications, scope and oversight in select AI-\nspecific regulations \n \nCountry/ \nInstitution \nLegislation \nand \nregulation \nAI Definition Risk classification Scope & Accountability Oversight  \nCanada Proposed AI \nand Data Act \n(AIDA) \n“a technological system that, \nautonomously or partly \nautonomously, processes data \nrelated to human activities \nthrough the use of a genetic \nalgorithm, a neural network, \nmachine learning or another \ntechnique in order to generate \ncontent or make decisions, \nrecommendations or \npredictions” \n• Classification \naccording to high -\nimpact system. The \ncriteria applicable to \nthe AIDA definition \nis to be established \nin regulations. \n• High-impact AI \nsystems could result \nin two types of \nadverse impacts: (1) \nindividual and \ncollective harms; (2) \nbiased outputs.  \n• Persons – including \ntrusts, joint ventures, \npartnerships and \nother legal entities, as \nwell as governmental \ninstitutions (except for \nthose defined section \n3 of the Privacy Act, \nas well as specific \nfederal security \ninstitutions and \nprovincial ones as \nprescribed by \nregulation) – under \nthe following \ncircumstances: “a \nperson is responsible \nfor an artificial \nintelligence system, \nincluding a high -\nimpact system, if, in \nthe course of \ninternational or \ninterprovincial trade \nand commerce, they \ndesign, develop or \nmake available for use \nthe artificial \nintelligence system or \nmanage its operation” \n– Art. 5(2).  \n• When dealing with \nhigh-impact AI \nsystems, obligations \nof such persons \ninclude: to keep \ngeneral records \nthroughout the \nlifecycle of thes e \nsystems; monitor \ncompliance with \nmitigation measures \nand their \neffectiveness; notify \nthe competent \nauthority; and make a \ndescription of the \nsystem publicly \navailable. \n• Albeit not expressly \n• Minister ( “the \nmember of the \nQueen’s Privy \nCouncil for Canada \ndesignated under \nsection 31 or, if no \nmember is so \ndesignated, the \nMinister of Industry” \n– Art. 5). The \nMinister may order \nthe person \nresponsible for high-\nimpact systems to \nprovide records, as \nwell as to conduct an \naudit and provide the \nresulting report \n(Arts. 13 to 15). It \nmay also require, \namong others, the \nimplementation of \nmeasures as well as \nthe discontinuation \nof the use or \navailability of a high-\nimpact AI system. \n• “In addition, the \nAIDA would create a \nnew statutory role for \nan AI and Data \nCommissioner, who \nwould support the \nMinister in carrying \nout” its \nresponsibilities. \n76    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nprohibiting the use of \nAI systems that may \nlead to unacceptable \nrisk, AIDA lists a \nnumber of actions as \nper se criminal \noffenses, e.g., the \npossession or use of \npersonal information \nunlawfully obtained. \nCouncil of \nEurope \nProposed \nRevised Zero \nDraft \n[Framework] \nConvention on \nArtificial \nIntelligence, \nHuman \nRights, \nDemocracy \nand the Rule \nof Law \n“an algorithmic system or a \ncombination of such systems \nthat, as defined herein and in \nthe domestic  law of each \nParty, uses computational \nmethods derived from \nstatistics or other \nmathematical techniques to \ncarry out functions that are \ncommonly associated with, or \nwould otherwise require, \nhuman intelligence and that \neither assists or replaces the \njudgment of human decision -\nmakers in carrying out those \nfunctions. Such functions \ninclude, but are not limited to, \nprediction, planning, \nclassification, pattern \nrecognition, organization, \nperception, \nspeech/sound/image \nrecognition, text/sound/image \ngeneration, la nguage \ntranslation, communication, \nlearning, representation, and \nproblem-solving” \nNo classification of risks \nbased on severity levels, \nas it leaves for each \ncountry to determine its \npriorities in terms of \nclassification system. \nInstead, it addresses \noverall “risks and adverse \nimpact resulting from the \napplication of an artificial \nintelligence system in \nrelation to the enjoyment \nof human rights, the \nfunctioning of democracy \nand the observance of rule \nof law” \n• Public and private \nactors indistin ctively, \nwhich design, develop \nand apply AI systems, \nthroughout the \nlifecycle of the latter, \nwhere it “involves \nissues related to \nhuman rights, the \nfunctioning of \ndemocracy and the \nobservance of rule of \nlaw”.  \n• The Convention is not \nintended to be applied \nto the design, \ndevelopment, and \napplication of AI \nsystems “used for \npurposes related to \nnational defense”. \n• Each Party shall \nensure that \nadequate oversight \nmechanisms are set \nin place within its \njurisdiction and in \naccordance with its \ndomestic law \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   77 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nEuropean \nUnion \nProposed AI \nAct \nProposed definitions by: \n• European Commission: \n“artificial intelligence \nsystem’ (AI system) \nmeans software that is \ndeveloped with one or \nmore of the techniques \nand approaches listed in \nAnnex I and can, for a \ngiven set of human -\ndefined objectives, \ngenerate outputs such \nas content, predictions, \nrecommendations, or \ndecisions influencing the \nenvironments they \ninteract with” \n• European Council: \n“artificial intelligence \nsystem’ (AI system) \nmeans a system that is \ndesigned to operate with \nelements of autonomy \nand that, based on \nmachine and/or human -\nprovided data and \ninputs, infers how to \nachieve a given set of \nobjectives using \nmachine learning and/or \nlogic- and knowledge \nbased approaches, and \nproduces system -\ngenerated outputs such \nas cont ent (generative \nAI systems), predictions, \nrecommendations or \ndecisions, influencing \nthe environments with \nwhich the AI system \ninteracts” \n• European Parliament: \n“artificial intelligence \nsystem’ (AI system) \nmeans a machine -\nbased system that is \ndesigned to operate with \nvarying levels of \nautonomy and that can, \nfor explicit or implicit \nobjectives, generate \noutputs such as \npredictions, \nrecommendations, or \ndecisions, that influence \nphysical or virtual \nenvironments” \nFour levels of risk: (1) \nUnacceptable risk \n(banned); (2) High risk \n(strictly regulated); (3) \nLimited risk (transparency \nobligations); (4) Minimal or \nno risk (no intervention). \n• Applicable to natural \npersons, legal \npersons and public \nauthorities. \nSpecifically, to \nproviders and users/ \ndeployers of AI \nsystems.  \n• A specific set of \nobligations is \nestablished for \nproviders of high -risk \nAI systems (including \nmanufacturers, \nimporters, \ndistributors), which \nincludes undertaking \nrelevant conformity \nassessment \nprocedures and taking \nthe necessary \ncorrective actions, \namong others.  \n• Users/deployers of \nhigh-risk AI systems \nare also subject to \nobligations, such as to \nmake due use, \nmonitor and maintain \ndocumentation \navailable.  \n• AI systems developed \nor used exclusively for \nmilitary purposes are \noutside the scope of \nthe Regulation. \n \nEuropean Artificial \nIntelligence Board (or \nOffice) to be established \nto: foster co -operation \nacross national as well as \nthe European \nCommission; coordinate \nand provide guidance to \nauthorities; and ensuring \nconsistency in the \napplication of the EU AI \nAct. \nUnited \nKingdom  \nWhite Paper \n“A pro -\ninnovation \napproach to AI \nRegulation” \n(March 2023) \nProducts and services that are \n‘adaptable’ and ‘autonomous’  \n• No predefined risk \ncategories or risk \nclassification, but \nrather illustrations of \nAI risks, such as \nrisks t o human \nrights (e.g., \ndeepfake \npornographic video \ndamaging \nreputation, \n• Needs to be further \nclarified by central \ngovernment or a \ncentral body.  \n• Areas outside of the \nscope of the \nregulatory framework \ninclude some of  “the \nwider societal and \nglobal challenges that \nmay relate to the \n• No oversight nor \ncross-sectoral body \nwill be created at a \nfirst moment. \nGovernment (not \nspecified which part \nof it) will monitor the \neffectiveness of the \napplication of \nvalues-based \nprinciples by \n78    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nrelationships and \ndignity), safety (e.g., \nphysical and/or \nmental harm), \nfairness (e.g., \nbiases), privacy and \nagency, societal \nwell-being (e.g., \ndisinformation that \ncould undermine \naccess to reliable \ninformation and trust \nin democratic \ninstitutions and \nprocesses) and \nsecurity (e.g., cyber \nattacks).  \n• The regulatory \nframework is \ncontext-specific, \nfocusing on the \noutcomes AI is likely \nto generate in \nparticular \napplications rather \nthan assigning rules \nor risk levels to \nentire sectors or \ntechnologies. \ndevelopment or use of \nAI”, such as “issues \nrelating to access to \ndata, compute \ncapability, and \nsustainability, as well \nas the balancing o the \nrights of content \nproducers and AI \ndevelopers”. \nregulators (which \nare based on the \nOECD.AI respective \nprinciples). \nHorizontal \ncollaboration across \nexisting regulators \nwill prevail for the \napplication of an AI \nregulatory \nframework on a \nvoluntary basis at a \nfirst moment (non -\nstatutory), according \nto their own \ndiscretion. \n• In a second moment, \nthe government will \nassess the need to \nintroduce a statutory \nduty mandating \nregulators to \nimplement further \nmeasures to support \nenforcement. \nUnited \nStates \nBlueprint for \nan AI Bill of \nRights \n“An ‘automated system’ is any \nsystem, software, or process \nthat uses computation as \nwhole or part of a system to \ndetermine outcomes, make or \naid decisions, inform policy \nimplementation, collect data or \nobservations, or otherwise \ninteract with individuals and/or \ncommunities. Automated \nsystems include, but are not \nlimited to,  systems derived \nfrom machine learning, \nstatistics, or other data \nprocessing or artificial \nintelligence techniques, and \nexclude passive computing \ninfrastructure. “Passive \ncomputing infrastructure” is \nany intermediary technology \nthat does not influence or \ndetermine the outcome of \ndecision, make or aid in \ndecisions, inform policy \nimplementation, or collect data \nor observations, including web \nhosting, domain registration, \nnetworking, caching, data \nstorage, or cybersecurity. \nThroughout this framework, \nautomated systems that are \nconsidered in scope are only \nthose that have the potential to \nmeaningfully impact \nindividuals’ or communities’ \nrights, opportunities, or \naccess” \nNo predefined risk \ncategories; applicability is \nto be judged based on \npotential for impact on  \nrights, opportunities, and \naccess to critical needs.  \n \nNon-binding proposals; it is \nleft to each authority \n(whether federal or local) or \nAI actor who is applying the \nprinciples to establish \naccountability in relation to \nspecific matters they seek to \nprovide guidance or \nregulate. \nThe establishment of a \nhorizontal authority for \noversight, coordination and \nmonitoring has not been \nforeseen. \nNIST R isk \nManagement \nFramework \nAn AI system is “an \nengineered or machine -based \nsystem that can, for a given set \nof objectives, generate outputs \nsuch as predictions, \nNo predefined risk \nthreshold categories. The \nNIST RMF refers to risks \nto people ( individuals, \ngroups, organi sations, \nThe NIST RMF makes \nreference to AI designers, \ndevelopers, deployers and \nusers. \n \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   79 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nrecommendations, or \ndecisions influencing real or \nvirtual environments. AI \nsystems are designed to \noperate with varying levels of \nautonomy”. \ncommunities, society, the \nenvironment, and the \nplanet.), organisations and \nsystems/ecosystems (e.g., \nenvironment), but it does \nnot seek to establish \nthreshold categories nor \nmetrics related to those \nrisks. These are rather left \nfor each organi sation that \nvoluntarily choose to apply \nthe NIST RMF to \ndetermine using the \nguidance, suggested \nactions. and information \nreferences provided in the \nAI RMF Playbook. \nNote: This table is a sample of emerging AI -specific initiatives from select jurisdictions at the time of writing in May 2023. Elements may have \nchanged since and thus it should be taken for illustrative purposes only.  \n80    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nAnnex B.  \nFurther aspects of selected AI-specific regulations \n \nTerritorial scope \n1. Some regulations expressly place obligations irrespective of the physical location or \npresence of the persons they intend to reach. The proposed Canadian AIDA is explicit about its \nextraterritorial reach. It creates responsibilities to persons that design, develop or make \navailable AI systems in the course of international or interprovincial trade. The proposed EU AI \nAct will be applicable to use rs, manufacturers and providers inside or outside of the Union. \nDistributors, importers and representatives placing AI products or putting into service AI \nsystems in the Single Market will have to abide to it (European Commission, 2021a[33]).  \n2. China has mainly focused its regulatory efforts on implementation within its territory. \nThe country’s Internet Information Service Algorithmic Recommendation Provisions (Zhang, \n2021[48]) specifies on Article 2 that it is applicable within mainland China. The Ethical Norms for \nNew Generation Artificial Intelligence (Center for Security and Emerging Technology, 2021[130]) \nobserve the same territorial scope.  \nProhibitions \n3. Canada’s proposed AIDA sets from the outset, as one of its purposes, the prohibition \nof certain conducts in relation to AI systems that may result in serious harm. However, the text \ndoes not clarify what the prohibitions consist of. The Companion to AIDA complet es this task, \nclarifying what is prohibited under AIDA and creating three new prohibited conducts under \ncriminal law. In a nutshell, the prohibitions include (Canadian Government, 2023[15]): \n• human rights discrimination (that i s, biased output of an AI system leading to \nunjustifiable and adverse impact) as prohibited in the Canadian Human Rights Act;  \n• reckless and malicious uses of AI that cause serious harms (to Canadians and their \ninterests).  \n4. To address these uses, AIDA creates three new criminal law provisions: \n• “Knowingly possessing or using unlawfully obtained personal information to design, \ndevelop, use or make available for use an AI system”; \n• “Making an AI system available for use, knowing, or being reckless as to whether, it is \nlikely to cause serious harm or substantial damage to property, where its use actually \ncauses such harm or damage”; or \n• “Making an AI system available for use with the intent to defraud the public and or cause \nsubstantial economic loss to an individual, where its use actually causes that loss”. \n• other applicable provisions of the Canadian Criminal Code. \nTHE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON   81 \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \n5. Art. 5 of the proposed EU AI Act establishes bans to the use of certain AI systems \n(European Commission, 2021a [33]). The exa ct scope is still subject to negotiations before the \nEuropean Union institutions and has been recently expanded in the Parliament. Currently, \nprohibitions include AI systems that: \n• deploy subliminal techniques beyond a person’s consciousness for certain obj ectives \nor effects; \n• exploits vulnerabilities of a specific group of persons, due to age, disability or specific \nsocial or economic situation; \n• evaluates or classifies natural persons (social scoring) leading to certain negative \neffects; or \n• carry out ‘real-time’ remote biometric identification in public spaces, subject to certain \nexceptions set forth in Art. 5(d). \nNon-contractual civil liability \n6. The EU opted to address non-contractual civil liability arising from harm caused by AI \nsystems in legislation separ ate from the AI Act.  Accordingly, in September 2022 it published \ntwo Directive proposals.  \n7. The proposed revision of the Product Liability Directive (European Commission, \n2022[131]) intends to update the existing 1985 Product Liability Act, modernizing the rules for the \ndigital economy. It does it by a) clarifying liability for defective products in situations when there \nis substantial modifications, taking into account the circular economy; b) including provisions on \nliability for technological products and their updates, such as software and digital services; c) \nincluding manufactures outside of the EU in the scope of the Directive, as well as importers of \ndefective products; and d) requiring manufactures to disclose evidence and alleviates the \nburden of proof to those harmed in certain situations, such as complex cases involving AI \nsystems (European Commission, 2022[131]).  \n8. The second Directive proposal under analysis is the newly proposed AI Liab ility \nDirective (European Commission, 2022[132]). It provides for damages caused by safety violations \nor breaches of privacy, as well as for compensation for discrimination in recruitment processes \ninvolving AI systems. In addition, it eases the burden of proof for those affected by AI systems \nto establish causality between the fault and the resulting harm in given circumstances, as well \nas makes it easier for injured parties to obtain evidence in cases involving high -risk AI \n(European Commission, 2022[132]). \nSanctions \n9. In Canada, the proposed AIDA establishes two types of penalties for non -compliance \nwith the regulation: administrative monetary penalties and prosecution of regulatory offences. \nIn additional, criminal offenses are also punishable under AIDA (Canadian Parliament, 2022[12]).   \n• The regulator would apply administrative monetary penalties to any type of violations, \nfor the purpose of encouraging compliance with the obligations under AIDA. The \ngovernment would have to carry out consultations and create new regulations to \nestablish these penalties. \n• Regulatory offenses would be applicable to serious cases of non -compliance with \nobligations. Here, guilt must be demonstrated. The authority responsible for examining \nwhether a regulatory offense is in the public interest is the Public Prosecution Service \n82    THE STATE OF IMPLEMENTATION OF THE OECD AI PRINCIPLES FOUR YEARS ON \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n  \nof Canada, which retains full autonomy in deciding whether to proceed with a \nprosecution. \n• The Public Prosecution Service of Canada would also be in charge of assessing the \npublic interest and prosecuting criminal offenses based on reckless and malicious uses \nof AI that cause serious harms. \n10. The proposed EU AI Act establishes fines up to 6% of annual worldwide annual turnover \nfor placing a prohibited AI system on the market; up to 4% of worldwide annual  turnover for \nviolating requirements for high-risk AI systems; and up to 2% of the worldwide annual turnover \nfor submitting false or incomplete information to authorities (European Commission, 2021a[33]).",
    "metadata": {
      "filename": "oecd_ai_principles.pdf",
      "source": "uploads"
    }
  },
  {
    "id": "74551c6b-faf3-4e01-b8db-47ede560de55",
    "text": "Classical Waterfall Model The classical waterfall model is a sequential software development process\nwhere progress flows in one direction: requirements → design → implementation → testing →\ndeployment → maintenance. Key Characteristics: - Linear, phase-based approach -\nDocumentation-heavy - No feedback loops between phases - Best suited for well-understood, stable\nrequirements Common Uses: - Military systems - Manufacturing - Large enterprise systems with fixed\nrequirements",
    "metadata": {
      "filename": "sample_test_document.pdf",
      "source": "uploads"
    }
  }
]